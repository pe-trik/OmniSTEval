Hallo, das ist Elena und ich werde unsere Arbeit präsentieren , indem wir uns in spanischen und angepreßten Modellen und Modellen vorstellen. (Stimme) Wir werden also darüber reden, was lexikalisches Borgen ist, die Aufgabe, die wir vorgeschlagen haben, die Datensätze, die wir veröffentlicht haben und einige Modelle, die wir erforscht haben. Aber was ist mit der Lexik und was ist mit der Mathematik? Leksikale Verwandlung ist in der Regel die Verwandlung von Worten aus einer Sprache in eine andere Sprache, in Spanisch verwenden Sie Wörter aus dem Englischen und Sie haben einige Beispiele wie Pods in English, All these English words are sometimes used in Spanish, Lexikale Verwandlung ist ein Sprachverwandlungsmodell. #um, die eigentlich einfach nur in einer Sprache verwendet werden, die von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen. Es gibt jedoch einige Unterschiede zwischen dem Text und dem Sch reibverhalten, die man auf dem Text verarbeiten kann. Das Schreibverhalten ist etwas, das durch die Definition von Linguistik und der Sprachverarbeitung nicht integriert ist. Es ist auch etwas, das durch das Sprachverhalten integriert ist. Das ist ja ein sehr gutes Beispiel dafür, dass die Bor rowings mit der Grammatik der Empfängersprache übereinstimmen und dass die Borrowings in der Empfängersprache integriert werden können. Warum ist das Leihen also ein interessantes Phänomen? Aus der Sicht der Linguistik ist das Leihen eine Manifestation, wie Sprachen sich verändern und wie sie interagieren. Und auch L eksikale Lern ern sind Beispiele für neue Lernformen, die in der spanischen Sprache eingeführt wurden. als neue Wörter . In Bezug auf NLP #ah Borrowings sind eine Ausgabe von Wörtern aus der Sprache und in der Tat automatisch auswählen, wie die Textverarbeitung für NLP-T ests wie Sprachverknüpfungen oder Maschinendurchsetzungen verwendet werden kann. Es hat ein wachsendes Interesse an der Wirkung von Englisch auf andere Sprachen, besonders Ich habe mir vorgenommen, dass ich mich mit dem Englischen auskennen kann, was manchmal als "Anglicisations" bezeichnet wird. Und du hast einige Beispiele für #ah-Work auf der automatischen Ausarbeitung von #ah-Bewerbungen in einigen dieser Sprachen. Die Aufgabe, die wir vorschlagen, ist es zu detektieren, nicht analysierte lexikalische Bogen in spanischen Nachrichten, was bedeutet, dass wir interessiert sind, in der Ausführung von #ah-Werben aus Die anderen Sprachen, die in spanischen Zeitungen verwendet wurden, waren nicht integriert oder in die Sprache integriert , also nicht in spanisch integriert. Hier haben Sie ein Beispiel für diese Sprache. Sie sehen die drei Spannungen, die eigentlich englisch sind, wie English Work, Animal Printing und Packing Work. Das ist die Art von Spannungen, die wir in der Erforschung und Erforschung interessieren. Es gab vorherige Worte. auf der Anglistik der Anglistik, die von der C.R.F. Model für die Anglistik der spanischen Nachrichten in der spanischen Version erstellt wurde . Dieser Model hat einen Punkt von sechsundsechzig, aber es gibt einige Einschränkungen in der Modellierung und in der Modellierung. Es ist auch so, dass es in der Zeit, in der wir uns in der Zeit befinden, in der wir uns in der Zeit befinden, in der wir uns in der Zeit befinden. die Präsentation der Modellierung der Modellierung kann eigentlich generell zu den vorherigen Bawings. #ah, so wir wollen es, einige dieser #ah, Limitation in der Taske zu verarbeiten. Also zu Beginn mit der wir eine neue Datensatz ung erzeugt haben. Die neue Datensatzung ist Anmerkung: mit den Textverarbeit ungen und dem Test wird erzeugt, dass es so schwierig ist, dass es möglich ist, dass es so wenig wie möglich in den Themen zwischen dem Training und dem Test zu finden ist, und dass die Tests aus den Quellen und den Quellen kommen. Wir sehen nicht in der Trainings-Serie, dass Sie nicht in der Zeit überlaufen können. Es ist auch sehr schwierig, Ihnen einige Zahlen zu geben, wenn Sie die Trainings-Serie mit sechs Boards haben. Ich habe tausend Tok en, die Teststelle enthält zwanzig Barings. Ich habe tausend Token. Die Test stelle enthält so viele aus dem Vokabular gewählte Wörter wie möglich. Tatsächlich waren 92 % der Borrowings in der Teststelle OOB, also waren sie nicht während des Trainings gesehen. #um der Korpus ist eigentlich eine Sammlung von Texten, die aus verschiedenen Sprach arten der spanischen Zeitungen stammen und #um ist von Hand verwendet, um einen englischen Text zu verwenden, der für die meisten Sprachveröffentlichungen in Spanien und dann für andere Sprachen verwendet wird. Wir verwenden Konformform ate und wir verwenden BIO-Encoder. So dass wir können, dass wir ein Single-Token-Browning, wie z.B. ein Multi-Token-Browning oder ein Machine Learning-Browning machen können. Diese Zahlen der Corpus können Sie sagen, dass es drei hundert undsiebzigtausend Tonnen sind und Sie haben die Nummer von dass wir als Englisch und die Sp rachsprachigen als andere Wörter und wie viele von ihnen wir haben. Und hier haben wir ein paar Beispiele von der Datensatz -Satzung, wie Sie sehen können, Wir haben in der ersten Example wir haben die Bauing. Das ist ein Multi-Wort-Borgen. Und wir haben es mit der BIO- Anmeldung verwendet, also die BIO-Anmeldung wird immer verwendet. Wo ist in Spanisch so nicht für wo für wo wir nicht barren? Und hier in diesem zweiten Beispiel haben wir Penching und Crash. die auch als "Labour" aus Englisch bezeichnet werden. So, als wir das Datensatz hatten, haben wir mehrere Modelle für die Aufgabe der Entdeckung und Entdeckung dieser lexikalischen Ent deckungen erforscht. Das erste, was wir versuchen, ist die Conditioning-Modell-Modell. Das ist das Modell , das in der vorherigen Arbeit verwendet wurde. Und wir benutzen die gleichen Hand werk-Features von der Arbeit, wie man sie von der Arbeit aus sieht. Diese Features sind die Features, die in der Welt der Apotheke sind. Es ist eine Quote, die Mark so etwas wie: sind die Art von Funktionen, die man in einer benannten Editurenerkennungs-Task erwarten würde. Das sind die Ergebnisse, die wir bekommen haben. Ich habe fünfzig fünf von F one Score erhalten, indem ich die mit der C.R.F. #ahm mit Handcrafted Feature, die unterschiedlich ist. Vergleichen Sie das mit dem berichteten F1 Score von 86 - das Ergebnis wurde mit dem gleichen CRF-Modell er mittelt, aber mit unterschiedlichen Daten, auch für die Sprachverarbeitung. dass die Daten, die wir erstellt haben, viel schwieriger sind als die Wir brauchen , um mehr spezielle Modelle für diese Aufgabe zu erforschen. Also wir... Wir haben das Transformator-Modell getestet, wir benutzen das Beto, das ein monolinguales Modell für Spanisch und auch für Sprach modelle ist. Wir benutzen beide Modelle, die wir durch die Transformations-Bild ung nutzen. Das ist so, dass wir sagen können, dass wir besser als Beto funktionieren. Die Entwicklung ist auf der Testseite und auf der Testseite und über alle Metriken. Das haben wir. Und ich habe die Idee, die zu vergleichen, die C.R.F. Model von Ten and Eighty Two. Das ist der C.R.F. von der Obtainer Fifty Five, der Obtainer Fifty Five von One Score, der Multilingual Bird Obtainer Eighty. die zwei, die eine große Differenz sind. So, dass wir diese Ergebnisse haben, fragen wir uns selbst eine andere Frage, die lautet: Was ist gut für uns? #uh Find ein Bios DMCRF-Modell, mit verschiedenen Typen von Embeddings. Es gibt verschiedene Arten von linguistischen Informationen. und so, dass die Ergebnisse von Transformationsmodellen erhalten werden. Also wir müssen es so machen, wir ran some #ah Preliminary experiments we we. #ah, die ranne die TMCR-Modell-Mode. Wir haben die Bibliothek und die Experimenten mit verschiedenen Arten von Bäumen, wie z.B. Bäumen, aber auch mit Fassbändern und so weiter. Was wir herausfanden, war, dass Transformator-basierte Embeddings besser funktionieren als nicht-kontextuelle Embeddings , dass die Kombination aus englischen und spanischen Embeddings aus Multi-Lingual-Bedings und B.P. Embeddings produziert wird. Besser als ein und ein und ein und ein und ein. Das ist besser, wie ich das hier denke. Best performing weighs so viel. Modelle werden von der MCRF Modell. #ah, wenn man ein Stück weiß, ist es so. Beton und Betonbeton und B.P. und der andere Beton und B.P. und auch B.P. und auch B.P. und B.P. Das ist das letzte, das ist der höchste Punkt auf der Teststelle, der höchste Punkt auf der Entwicklung wird von der Entwicklung durch die eine mit den B.P. Ich denke, dass wir mit dem besten Ergebnis mit einer Mütze von siebenundsechzig auf der Entwicklung und zwei auf der Testseite kommen. Das ist eine Verbesserung, die sich mit diesen Ergebnissen vergleichen lässt. Wir haben uns endlich gefragt, ob wir eine andere Frage stellen können, die lautet: Kann die translationsfähige Ausdehnung von der Sprachidentifizierung und der Sprachverwechslung so aussehen, dass wir das gleiche Modell haben, das wir haben, aber wir verwenden nicht die verwendeten Transformations-Behälter. und aber auch in den Bäudungen. Wir verwenden Code Switching und Betten. Was sind Code Switching und Betten? die in der Transform ation eingeb aut wurden, die für die Sprachidenti fizierung in der spanischen englischen Sprach abteilung der Sprachvermittlung verwendet wurden , die auf der Sprachvermittlung in der spanischen Sprachabteilung der Sprachvermittlung angesiedelt ist. Fett unsere Biol ast-TMCRF mit Switch -Bedding und Optional-Bedding und B .P.B.B.ing und so weiter. Das beste Ergebnis ist vierundzwanzig Punkt, der höchste Punkt, den wir alle versuchen, auf der Teststelle zu testen. Das beste Ergebnis ist, dass wir einen Punkt auf der Entwicklung erzielen, der siebenund zwanzig ist, und das beste Ergebnis ist, dass wir es durch die BLS-Ref #ah. mit unadaptierten Einbettungen. Also einige Schlussfolgerungen aus unserer Arbeit haben wir Wir haben ein neues Datensatz von Spanish News Wired produziert. Das ist mit dem nicht-verknüpften, nicht-verknüpften Barwins, der ist mehr als der Barwins und wir haben vier Arten von Modellen für die Erforschung von Barwins in der Erforschung von Analysen. Wir haben mit Wick Point für alle Modelle zusammengetragen. #um, wie ich hier auch Frequent negative Folgen beinhalten auch Abwehrmaßnahmen, die in englischer und spanischer Sprache vorhanden sind. #um auch interessant die B P P-B ewertungen zu verbessern ein Punkt und zu verbessern die #um zu verbessern, was #um ist interessant zu finden, dass wir es auf der Zukunft arbeiten können und das alles haben wir alles, danke fürs Hören.
Mein Name ist Antoine. Ich bin Doktorand an der Universität von Massachusetts Amherst. Ich präsentiere unsere Zeitung. Kenya Berth, Morphologie, wie? Kenya Rwanda Language Model. Das ist ein sehr gutes Beispiel. Heute werde ich über die Motivation für diese Forschung sprechen. Dann präsentiere ich Kenyabereits Modellarchitektur im Detail. Ich werde dann über unsere experimentellen Ergebnisse reden. Dann schließen wir mit einigen Schlüssen. Wir wissen alle, dass die jüngste Fortschritte bei der Verarbeitung natürlicher Sprachen durch die Verwendung von prätrainierten Sprachmodellen wie BERT ermöglicht wurden. Aber es gibt eine Reihe von Limitationen. aufgrund der komplexen Morphologie, die von den meisten morphologisch reichen Sprachen ausgedrückt wird? Die Ubiquitas byte. Die al gorithmischen Tokenisierungsalgorithmen, die verwendet werden, können nicht die exakten Unter wortlexikaleinheiten extrahieren, die für die Effektivität der Morpheme benötigt werden. Repräsentation Zum Beispiel haben wir hier drei kinya-rwanda-Wörter. Das hat mehr Morphine in ihm. aber die B P Algorithmen können sie nicht extrahieren. Das ist, weil es morphologische Regeln gibt. produziert verschiedene Oberflächenformen, die die genauen lexikalischen Informationen verbergen. und B P E, die sich auf der Oberfläche der Formeln befindet, hat keinen Zugang zu diesem lexikalischen Modell. Die zweite Herausforderung ist, dass auch wenn man Zugang zu einem morphologischen Analyzer hat, die BPE-Token mit Morphemen zu ersetzen, nicht ausreicht, um die morphologische Kompositionalität auszudrücken. Ein dritter Gap in der Forschung ist, dass Neue , prät rained Language-Modelle sind meistens auf High-Resource-Sprachen evaluiert. und wir müssen ihre Anwendbarkeit auf Rohstoffe bewerten. und diverse Languages as well. Deshalb präsentieren wir Kenya Bird. die eine einfache, aber wirksame Anpassung der B ERT-Architektur ist, die morphologisch reiche Sprachen effektiver handhaben soll. Wir evaluieren Kenya B ells in Kenia Rwanda, eine Low -Resource morphologisch-reiche Sprache, die von mehr als zwölf Millionen Menschen in East und Central Africa gesprochen wird. Das ist der Eingang zu dem Modell. ist auch eine Satz oder ein Dokument. Hier haben wir zum Beispiel John , du warst ja ein sehr treuer Mensch, das bedeutet, wir waren überrascht, John da zu finden. Wie Sie sehen können, enthalten die kenianischen Wörter mehrere Morph eme, die unterschiedliche Informationen enthalten. Deshalb. In unserem Modell geben wir diesen Satz oder ein Dokument einem morphologischen Analyzer vor. Die dann Morpheme generiert, die in jedem der Wörter enthalten sind. Die Morpheme sind normalerweise made of a stem and zero or more affixes. Die Aff ixe können Z ehnter, Aspekt, Subjekt oder Objekt in Verbs und mehr oft in Bezug zu den Band-Namen angeben. Klasse für Subjekte und Objekte. Der morphologische Analyzer produziert auch ein Part-of-Speech-Tag für jedes der Wörter. Nach diesem Schritt Wir machen Embeddings für die Spee the for the part of speech tags. Imbedings für die Affixes. Und das ist für den Stamm. Das sind die mehr fragilisierbaren. Das ist die Morphologie-Ebene. Imbedings Wir übertragen diese Embeddings dann durch einen Morphologie-Enkoder, der ein kleiner Transformator-Enkoder ist , der unabhängig voneinander auf jede Welt angewendet wird. Die Aus gaben sind die Vektoren, die mit der morphologischen Information an jeder Seite kontextualisiert sind. Jetzt führen wir Kompositionen durch. wo die morphologischen Embeddings, die einem Teil der Sprache und dem Stamm entsprechen, zusammenkorrelieren. Wir werden sie weiter mit einem anderen Stamm in der Satzstufe kombinieren. dann bilden wir einen Input zu dem Main-Satz- oder Dokumentencoder. Die End-Ausgabe sind kontextualisierte Embeddings, die für Downstream-NLP-Aufgaben verwendet werden können. für einen morphologischen Analysator. Wir verwenden endliche , zweistufige Morphologie-Prinzipien mit einer benutzerdefinierten Implementierung, die der kenia-rwandischen Sprache zugeschrieben ist. Wir modellieren die Morphologie aller kinya-rwandischen Wörter, einschließlich der Verbal, Nomen, Demonstrative und Possessive Pronomen, Numerals und andere. Wir benutzen unüberwachte part of Speech tagging Algorithm. Ein First-Told-Up-Faktor-Modell wird verwendet, um Morphologie-Wahrscheinlichkeiten zu berücksichtigen. #ah, eigentlich die Wahrscheinlichkeit, die ist von der morphologischen Analyzer. Wir werden auch berücksichtigen, Die Part of Speech tags pre misses, wie auch die syntactic agreements, die vorhanden sind. in den Input-Wörtern. Der Part of Speech Tagger verwendet eine bidirektionale Inferenz. die die häufigsten VTB- Algorithmen für das Dekodieren verbessert. Ein paar Bemerkungen hier für Positionalschlüsselung. Der Morphologie-Enkoder verwendet keine Positionalschlüsselung , da jeder der Morpheme einen eigenen Slot im morphologischen Modell hat, daher ist die Position alschlüsselung inhärent, wenn die Morpheme gegeben werden. Zweite Der Satzencoder verwendet die sogenannten untiefen relativen Positionalschlüsselungen, die kürzlich in der I con Conference veröffentlicht wurden. Diese Positionalschlüsselungen ent ziehen die positionalen Korrelationen von Token-zu-Token-Aufmerksamkeit-Computation. Ähnlich wie bei Bert verwenden wir ein maskiertes Sprachmodell für das Vor-Training-Ziel. Wir müssen sowohl den Stamm als auch die Affixe vorhersagen , die mit den Wörtern assoziiert sind. während des Pretraining Fünfzehn Prozent aller Wörter werden für die Vorhersage in Betracht gezogen. von denen 80 % maskiert sind, 10 % mit zufälligen Worten geschickt und 10 % unverändert bleiben. Für eine Affix-Vorhersage Wir haben ein Multi-Label-Klassifikationsproblem. für this, we either group together affixes into a fixed number of sets and predict the set as a class label. Die andere Option ist, die affixed Probabilitätsvektoren zu vorhersagen. Wir evaluieren both of these approaches in our experiments. Wir haben Kenya Beret auf etwa 2,5 Gigabyte von Kinyarwanda Text und compare it to three baseline models. Einer ist ein multilinguales Modell, das als Excel bezeichnet wird. M R. Das ist ein Trained on a large text corpora. Das ist aus mehreren Sprachen gemacht. Die anderen beiden Bas eline sind auf demselben Text aus Kenia und Rwanda vorbereitet, wobei entweder ein BitPay-Encoder-Algorithmus oder oder morph ologische Analysen verwenden, ohne die Transformator-Architektur zu verwenden. Alle Modelle sind konfiguriert. in der B ese Architektur, die zwischen hundert und hundert und zehn Millionen Parameter mit Kenya Rwanda mit Kenya Bite verwendet, die geringste Anzahl von Parameter. Alle Modelle , außer der mehrsprach ige, sind für 32000 Gradient-Updates vorbereitet. mit der Batch-Size von zwei tausend fünfhundert und sechzig Sequenzen. in jeder Batch. Wir bewerten die prätrainierten Modelle auf drei Set of tasks, one is the group benchmark, die für die Auswertung der Effektivität von Pre-trained Language Models verwendet wurde. Wir haben unsere Blue Benchmark erhalten. Daten , indem wir die ursprünglichen Benchmark-Daten in Kenia, Rwanda, using Google Translate, übersetzen. Die zweite Aufgabe ist Kenya Rwanda Named Identity Recognition Benchmark. Das ist ein hochwertiger Datensatz, der von trainierten Native Speakers annotiert wurde. Der dritte... ist die News Kategorierung task. wo wir Newsartikel von mehreren Websites ziehen und ihre Kategor isierungstags korrigieren, die von den Autoren unterschrieben wurden , und dann im Wesentlichen versuchen, das Gleiche zu vorhersagen. Das sind die gleichen Kategorien. Und jetzt gehen wir zu den Ergebnissen. für die Gluebenchmark Wir finden, dass Kenya Belt konsequent Baseline-Modelle übertrifft. Hier sehen wir die durchschnitt liche Leistung für ten fine tuning runs. Wir führen auch eine User-E valuation der Übersetzungen durch Google Translate durch. Es ist nicht so, dass die Nutzer etwa sechstausend Beispiele bewertet haben, indem sie Scores auf einer Skala von 1 bis 4 zugewiesen haben. die Qualität der Transaktionen. Das Ergebnis ist, dass viele Translationen laut sind. Aber alle Mod elle mussten mit dem selben Translation Noise coppiert werden, und die relative Performance zwischen den Modellen ist immer noch wichtig zu bemerken. für die Namensdentizitätserkennung. Wir finden auch, dass König Berth mit der Bestleistung , mit der Best leistung der Affinity Distribution Regulation Variant, sind diese Ergebnisse auch Durchschnittswerte von zehn Fin tuning runs. für die News-Kategorifizierung Wir finden gemischte Ergebnisse , vorherige War-Kontext-Klassifikationen für Kenia und Rwanda haben ergeben, dass die simple Keyword-Detektion ist mostly enough for solving this specific task, therefore there is Leise von using Pretraying Language Models. auf dieser besonderen Aufgabe der Kategoriisierung von Nachrichten. Wir haben auch eine Ablationstheorie durchgeführt, um zu sehen, ob es alternative Strukturen gibt, die die Leistung verbessern. für die Group Benchmark finden wir, dass wir verwenden Affix-Sets. Konsequenz volles Performance besser, wenn wir die Probabilität der Erstellung der Bestleistung der Besten Performance der Besten Recognition. Oder so. Durch das Aussehen der Lose Curves für die Feintuning finden wir, dass Knyabert eine bessere Konvergenz hat. in den meisten Fällen. So zu dem Schluss, dass diese Arbeit die Wirksamkeit der expliziten Verwendung morphologischer Informationen in prä- trainierten Sprachmodellen demonstriert hat. Ermöglicht die morphologische Erfassung von C. die morphologische Kompositionalität erfasst, was ein wichtiger Aspekt von morphologisch reichen Sprachen ist. Diese Ergebnisse sollten weitere Forschungsinitiativen fördern. In Morphologie haben wir Language Pre-Traded Language Models.
Hallo, mein Name ist Miho Petrushka und es ist mir eine Freude, Ihnen das Papier mit dem Titel Sparsifying Transformers Models with Trainable Representation Pooling vorzustellen. Ich hab's nicht so gut geschafft. Lass mich mit den Problemen beginnen, unsere Arbeitsziele. Unsere Methode funktioniert gut für Fälle, in denen lange Eingänge berücksichtigt werden. Es ist für das Task Quarter gedacht, es ist ein Input von über 2000 Token und die Ziele sind kürzer. Dann haben sie Inputs geliefert. Das hat einige spezifische Anwendungen in NLP. Zum Beispiel kann man sich vorstellen, dass es aufgrund eines langen Dokuments eine Notwendigkeit gibt, das zu zusammenfassen und zu klassifizieren. Antworten Sie auf die Frage, Extrak-Informationen. Ich habe ein paar Schlüsselphrasen. Lass mich an den Vanilla-Transformer erinnern, der eine Frage der Aufmerksamkeit und Komplexität hat, die vom Quadrat des Eingangsläufers abhängt. In der Vanilla-Transformer Mit voller Aufmerksamkeit auf die Konnektivität, die Beziehungen von jedem Token To every other token have to be calculated. Die Komplexität der Aufmerksamkeit hängt von der Anzahl der Layer ab. Sequenzlänge an Eine weitere Sequenz. und die Dimensionalität von Repräsentationen. In ähnlicher Weise scharft der Dekatur Aufmerksamkeit auf dieses Bild auf der rechten Seite. Der einzige Unterschied hier ist, dass die Target Tokens die Input Tokens betreffen. In diesem Fall. Das ist auch in dieser Formel zu sehen. Die blaue Punktzahl stellt Beziehungen dar, die berechnet werden müssen. Im Falle der vollen Aufmerksamkeit müssen wir alle Beziehungen innerhalb der Eingangssequenz berechnen. Jetzt sehen wir, was passiert, wenn wir einen Block-Wise-Enkoder haben, der durch die Begrenzung der Token-Konnektivität funktioniert. So dass sie nur andere Token in der Nähe sehen können. Der Text wird in Stücken gelesen, was die Anzahl der Berechnungen auf der Encoder-Seite drastisch reduzieren kann. aber es verbessert nicht die Decoders. Cross Attention , da jeder Input-Token sowieso an den Decoder übergeben wird. Diese Methode wird oft als Fusion Indikator bezeichnet. Die Verbesserung hier kann als Veränderung interpretiert werden. Einer der Abhängigkeiten von n zu einer anderen Konstante m. die Blockgröße repräsentiert. Unsere wichtigste Beobachtung ist, dass die meisten Tokens irrelevant sind. für eine Vielzahl von Aufgaben und kann fast vollständig vernachlässigt werden. Das ist auf der Schieberliste veranschaulicht. wo nur Teile der Eingänge relevant sind. zu the desired output. zum Beispiel Man kann einen Artikel einmal lesen. die wichtigsten Teile mit einem High lighter zu markieren und dann eine Zusammenfassung basierend auf diesen Teilen aus der mittleren Stufe zu erstellen. Die Kosten für die Auszeichnung und Entscheidung, ob die aktuellen Token für die Herstellung der Zusammenfassung unerlässlich sind, sind also billig und hängen nur von der Repräsentation der Token ab. Das Pulling der hervorgehobenen Tokens ist möglich. Danke an unseren Top K Operator. Und die Kosten sind unerheblich. Die Kosten für die Zusammenfassung aus einem kurzfristigen Input. Es ist auch viel niedriger als im Van ille-Modell, wenn der gesamte Eingang berücksichtigt wird. Aber hier ist die Frage: wie man wichtige Tokens auswählt und wieder verbreitet. Gradienten zu dieser Selektion. Das ist das Problem, das wir lösen, den trainierbaren Selektionsmechanismus vorzuschlagen. Einer, der es ermöglicht, dass Gradienten während des Trainings zurückverbreitet werden, so dass das Netzwerk lernen kann, die wichtigsten Token zu wählen. Genauer gesagt: Geben Sie mir einige Embeddings und Schnürsenker aus einer einfachen linearen Schicht gewonnen Die Aufgabe ist es, die höchsten Punkte zurückzugeben. Zuerst ist die Sequenz permutirt. und Paars werden so vorbereitet, dass der höhere Scoring-Vektor mit dem niedrigeren Scoring genommen wird. Als nächstes werden die Gewichte mit Boosted Softmax über die Scores berechnet. Nach jeder Turnierrunde Neue Vektoren und die Scores sind als eine lineare Kombination aus diesen Parts zusammengesetzt. Und da haben wir das gewählt. Kurz gesagt, wir kombinieren sie linear. durch Performing Softmax Over The Das ist natürlich... Und während wir kombinieren, Zwei Tokens, ein bisschen Noise. die man produzieren kann. produziert, aber es erlaubt auch, die Gradienten zu propagieren, um alle Eingaben zu embonieren. Kurz gesagt, ein trainierbares Topk. Wir haben einen Antrag gemacht. Es basiert auf der Durchführung eines Turniers wie Soft Selection. Und aus einer anderen Perspektive. Die Repräsentationspoling folgt der Encoder-Schicht. Zuerst wird jede Repräsentation bewertet und dann nur die mit den höchsten Punkten. zu den nächsten Layer. Die Kodierung kann wie in der Standard-Transformer-Architektur auf der vollständigen Eingabe durchgeführt werden. Es ist jedoch möglich, Texte in Blöcken von Pixel zu verarbeiten. fixed length and globally select the best representation. Hier ist ein Beispiel für das Repräsentations-Polling , das nach dem Encoder eingeführt wurde. Das beeinflusst direkt die Kosten für die enge Aufmerksamkeit, Nicht auf der Input-Link-End. Aber die Konstante "K". Ich repräsentiere das Pool Play. Das ist konstant informiert, wie viele Repräsentanten ausgewählt wurden. und dann übergeben an den Dekoder. Produzieren eine Zusammenfassung aus einem kürzeren Text ist &nbsp; bedeutend billiger als vorherige Lösung. Da die Sequenzlänge durch einen großen Faktor verkürzt werden kann, Zum Beispiel haben wir erfolgreich K verwendet. von 16 oder sogar 60 Mal. oder sogar 64 mal kleiner als der Wert von n in unserem Experiment. Bitte nicht. dass die positive Wirkung von Blockwise-Enkodierung und Selbstbetrachtung ist sustained. Ich habe immer gedacht, dass die Rechnungskosten der Aufmerksamkeit vom Quadrat der Eingangslänge abhängen. Das ist ein Kind. Die früheren Eingänge während des Kodierungsprozesses können die Kosten deutlich senken. Für das pyramidische Modell schränken wir die Größe der Repräsentation auf der Ausgabe von jeder gewählten Schicht, was zu einer exponentiellen Reduktion die Kosten für die Berechnung, während die Kodierung voranschreitet. Wie Sie sehen können, die Gesamtrechnung skosten eines vollständigen Encoders sind weniger als doppelt so hoch wie die Kosten für die Vollgroß erste Schicht. Wenn das Pulling früher eingeführt wurde, Die Summe aller lila Quadrate ist also begrenzt auf eine Konstante. nicht abhängig von der Anzahl der Layer L. Was auf die Konstanz zu sagen die durch die Platzierung der Pulling-Layer innerhalb des Netzwerks beeinflusst werden können. Unsere Verbesserungen wurden auf achttausend Token langen Eingängen verglichen. Und die Figur zeigt, dass beim Pulling die beste Skalierbarkeit für die Netzwerkstiefe erreicht wird. hier kann man noten Das ist billiger , als einen zwei-Schicht-Vanille-Transformer mit so langem Eingang zu trainieren. Nicht zu erwähnen, wie leicht Vanilla Transformers Geh aus dem Gedächtnis für so einen langen Input. Der **. Qualitative Vergleichung unserer Die andere Baseline ist die Perform on the long document summary task, die die Gesamtheit eines Artikels aus Archive oder Papmat erzeugt. Wie man sehen kann. Blockwise, das ist unsere Baseline. performs on the level of the recent state of the art models. Während die Pyramidion die Leistung dieser wettbewerbsfähigen Baseline beibehält oder verbessert, Gleichzeitig. Unser Modell ist 80. Per zent schneller zu trainieren und über 450 Prozent schneller bei der In ferenz im Vergleich zur Blockweite-Basislinie. Beide Modelle haben eine viel niedrigere Parameterzahl und wurden von Anfang an an den gewählten Aufgaben ausgebildet. Vorherige Ansätze um zu erreichen. Eine ähnliche Leistung musste mehr Parameter verwenden. Und die Leverage- Pretrained-Foundation, die Fundamentals. und zusätzliche Sprachen für das Training. Ziel: Ähnliche Leistung zu erzielen. Oh, ja. Wir laden Sie ein, unser vollständiges Papier zu lesen und unseren GitHub-Code zu verwenden. Danke, dass du es gesehen hast.
Hallo, das ist Yoweri Joe von der Harvard University. Ich bin sehr froh, ihre Arbeit zur Online-Semantik-Parsing für Latenzreduktion in taskorientiertem Dialog vorzustellen. Das ist ein Joint Work mit Jason, Michael, Anthony und Sam von Microsoft Cementing Machines. in task-oriented dialog Ein Benutzer interagiert mit einem System, das Anfragen von Benutzervertretern, normalerweise im Sprechen, bearbeitet. von der Endung der Benutzeransage bis zur Systemantwort Es gibt oft unmerkliche Verzögerungen. Unter der Hood wird das Benutzer-Audience in ein ausführbares Programm übersetzt. und dann ausgeführt , damit das System richtig reagieren kann. Hier ist das Programm als semant ischer Graph dargestellt, der die Berechnung skizziert. wo eine Note eine Funktion inruft und seine Kinder die Argumente sind Die großen N odes markieren augenblickliche Operationen , aber die anderen sind langsam auszuführen. bemerken, dass, unlike the simple example here we show, Diese Programme können oft kompliziertere Graphen sein. Jenseits der Baumstrukturen. in diesem talk wir stellten die frage können wir generieren das Programm und die Ausführung, bevor der Benutzer die Änderung fertigstellt, damit das System eine schnellere Reaktion erzielen kann. Das ist eine Online- Vorhersage und Entscheidung Problem. Es gibt viele andere in diesem Raum. Beispiele hierfür sind die gleichzeitige Übersetzung Wo ein Live-Interpreter eine Sprache in Echtzeit in eine andere übersetzt. Smart Text -Autokompilierung, um die Benutzerabsicht zu ermitteln. und über den Pool, wo die Fahrer geschickt werden, wo sie gebraucht werden könnten, basierend auf der vorhergesagten Nachfrage. Alle diese Szen arien haben eine Gemeinsamkeit: Es ist vorteilhaft, Entscheidungen zu treffen, bevor man andere Einträge sieht. In unserem Fall werden wir mit Online-Semantik parsen. was erwartet werden könnte, dass es schwierig ist, da wir wissen, was der Benutzer sagen könnte, und es ist auch untersucht , ohne formelle Bewertungsmetrik. Zuerst schauen wir uns an, wie ein normales System funktioniert. Es ist offline betrieben, indem es nur am Ende der Benutzeranweisung zum Programm parst. Hier wird der Kriptograph vorhergesagt, nachdem er alle Informationen gesehen hat. Im Gegensatz dazu schlagen wir ein Online-System vor, das bei jedem Alternativ-Präfix parsert. Zum Beispiel: Jedes Mal, wenn wir einen neuen Token sehen, prognostizieren wir einen neuen Graph. nicht gesagt da könnte er irres an der position von an der pool party mit barack obama Wir haben einen Grafik mit den rechten Noten. auf die Person und das Ereignis-Subjekt, aber das ist die falsche Zeitinformation. Dieser Prozess geht weiter. bis wir die vollen Benutzeranweisungen erhalten haben. Wie würde sich das auswirken? Die Exekution ist in der Offline-Systeme Wir haben am Ende den Programmgrafen. So dass das System an diesem Punkt mit der Ausführung beginnen kann. Denken Sie daran, dass die großen Knoten schnell funktionieren , also betrachten wir nur die Ausführungszeitlinie der farbigen langsamen Funktionen. Er stens können diese zwei Personenfunktionen parallel ausgeführt werden. und sie haben keine Abhängigkeit von anderen Funktionen. Als nächstes kann das No-C rit-Event dann ausgeführt werden, nachdem Ergebnisse von niedrigeren Noten erhalten wurden, und dann die Topfunktion yellt, so dass das ganze Programm fertig ist. Der Ausführungsprozess ist auf die Programmabhängigkeitsstruktur beschränkt. wo manche Operationen nicht parallelisiert werden können , was eine bemerkenswerte Verzögerung verursacht. in unserem Online-System, wo wir vorhersagen, wie wir gehen. Die Ausführung des Programms kann früher beginnen. Hier ist das Präfix nach Obama. Wir können mit Sicherheit vorhersagen, dass die F inder-Person-Funktion in einem Programm sein sollte , aber der Rest kann Fehler enthalten, da sie ausgegraben sind. Die Ausführung des Knoten punktes kann sofort in diesem Schritt gestartet werden. Dann , mit mehr Tok en, prognostizieren wir einen völlig neuen Graph , aber ein Teil davon wird bereits ausgeführt , also müssen wir nur den Rest der Nodes, von denen wir uns auch sicher sind, berücksichtigen. Hier kann eine andere gute Person parallel hingerichtet werden. Wiederum, wir können falsche Vorhersagen haben. mit mehr Text. Wir haben mehr Fähigkeit, es richtig zu machen , wie z.B. die Ereigniszeit hier, wo am auch richtig erwartet wird. Dann können wir den Rest ausführen , nach der Programmabhängigkeitsstruktur. Durch die Überlappung der Ausführungszeitlinie mit der Zeitlinie der Änderungen sparen wir eine große Zeit. Wir haben also die Aufgabe vorgeschlagen, Online-Sementizparsing zu machen. Eine zugrunde liegende Annahme ist, dass die Ausführungszeit die Modell vorhersagezeit dominiert, so dass wir nur Zeit gewinnen konnten, indem wir früher vorhersagen. Eine andere Annahme ist, dass die Vorhersage und die Ausführung den Hintergrund haben, der nicht sichtbar ist für Benutzer es ist nicht notwendig, eine Konsistenz zu halten Wir analysieren die Geschichte, also analysieren wir nach jedem Token von Anfang bis Ende. In besonderem Fall schlagen wir einen Zwei-Stufen-Ansatz vor. ein vorgeschlagener Schritt, der einen Graphen mit vollständiger Struktur vorhersagt. Und ein Select- Schritt, der die Noten auswählt, die es wert sind, in dieser Zeit ausgeführt zu werden. Wir haben zwei Varianten der vorgeschlagenen Methode. Der erste Ansatz kombiniert eine Sprachmodellvollendung mit vollständiger Ausdrucksfähigkeit zur Graph-Parsing. In der Regel ist das Präfix abt Obama erst durch ein Feintun-Bart-Sprachmodell abgeschlossen und dann in ein Programm mit vollem Offline-Parser übersetzt. Der zweite Ansatz sagt das Programm direkt von YouTube aus voraus. Benutzer -Other-Prefix. Das wird erreicht, indem ein einzelner Online-Parser angepasst wird, um den Gold graph von jedem Präfix zu übersetzen. Dies erleichtert dem Modell, die richtige Ansicht zu lernen. Ein bisschen detaillierter. Wie generieren wir diese Grafiken? Wir formulieren das Problem, indem wir eine serielle Version des Graphen generieren. Jede Note oder Kante wird durch eine Aktion dargestellt. Hier beginnen wir mit dem ersten Knoten. Die Nummer unten zeichnet die absolute Indexierung der Aktionsgeschichte auf. Dann haben wir die zweite Note. Als nächstes ist die Kante zwischen ihnen. Sie enthält den Zeiger zum Index der vorherigen Knoten. Und das Randlabel : Zero hier bedeutet, die neueste Knotenstelle zu verbinden. mit einem Nullpunkt, der durch die Nullaktion erzeugt wird. und Next Note, Next Edge. Dieser Prozess geht weiter , bis wir den vollen Graph generieren. Das zugrunde liegende Modell basiert auf einem Transformator mit einem selbstgesteuerten Mechanismus, ähnlich wie ein früherer übergangsbasierter Parser. Nach der Erstellung eines vollständigen Graphen Wir haben die Wahrscheinlichkeiten der Aktionsstufe, die verschiedenen Teilen des Graphen entsprechen. mit leichten, zuversichtlichen Subgraphen basierend auf der Schwellenheuristik, die ausgeführt werden soll. Später werden wir den Schwellenwert variieren, um unterschiedliche Kompromisse zwischen der Latenzreduktion und den Ausführungskosten zu erzielen. Eine formelle Bewertung der Online-Methoden Wir schlagen eine endgültige Latenzreduktion vor. Oh, FLR-Mänschchen. Hier ist ein Zusammenfassung , wie ein Offline-System die Ausführungszeitlinie beendet. in Online-Systemen Die Ausführung überschneidet sich mit der Zeitlinie der Abwechslung, also endet sie früher. FLR ist definiert als die Reduzierungszeit im Vergleich zum Offline-System, die am Ende der Ausführung markiert wird. Wir führen Experimente an zwei großen, konservierten, zementierten Parsing-Datensätzen durch. S. M. Kaflo und Ch. D. S. T. graph-based parser. Wenn man offline arbeitet, kann man die neuesten Leistungen bei der Parsing erzielen. Auf beiden Datensätzen erreicht das Outline Complete Modell auch Nontrivial. bluegane vergleicht mit der einfachen bislinie der note completion Jetzt schauen wir uns die Vorhersage und Genauigkeit unseres Grafikparser-Präfixes an. Wir testen die Übereinstimmung von Grafen zwei Pools zwischen der Generation und der Gergraph-Invalidationsdaten. in Y-Achsen , für jede Präfixlänge in X-Achsen , repräsentiert durch Prozentsätze. Jede dieser Kurven repräsentiert ein anderes Modell. mit dem Unterschied in den Trainingsdaten. Die Bottom Curve ist der Offline-Parser. und wir mischen in den Präfixdaten unterschiedliche Linien. um das Modell zu einem Online-Parser zu transformieren. Zum Beispiel das Legend-Präfix 80 % plus. Das bedeutet, dass das Modell mit den Präfixdaten gekennzeichnet ist. mit einer Länge von Perfekten, die größer ist als 80 % der Länge der vollständigen Ausdrucksform. Die obere linke Ecke ist ein gewünschter Bereich. wie wir sehen können , der Offline-Parser in Black Curve ist nicht gut auf den Präfixdaten, als wir mehr Präfixe in Training mischen, die Kurve ist liftend oben und links , besser auf alle die Präfixlänge. Allerdings ist die voll ordnungsgemäße Parsingleistung in der oberen rechten Punkt nicht beeinflusst. basierend auf diesen starken Ergebnissen Wie viel Latenz reduzieren wir? Wir messen die Zeit an der Anzahl der Quelltokens und simulieren verschiedene Funktionsausführungszeiten. Die Kur ven zeigen den Kompromiss zwischen der FLR-Metrik und den Ausführungskosten, gemessen an der Anzahl der übermäßigen Funktionskosten, die nicht korrekt sind. Das wird erreicht, indem man die Subgraph-Selektionsschwelle variiert. Eine höhere Schwelle wählt weniger Funktionen und Fehler aus , aber erhält eine kleinere FLR. Während die untere Schwelle aggressiver Programme auswählt und ausführt. Wir vergleichen die beiden Ansätze, die wir vorschlagen, in der Basislinie. Das tut nichts, als direkt den Offline-Parser anzuwenden. für den Online-Gebrauch. Regen ist das beste FLR und kostet Trade-off. Wir sehen , dass beide Methoden die Basislinie mit großer Marge schlagen und dass sie auf GDST ähnlicher funktionieren. Wenn die einzelne Funktion schneller ausgeführt wird, gibt es tendenziell mehr Run-Ausführungen und weniger Latenzreduktionsraum. wenn die Ausführung der einzelnen Funktionen langsamer ist Es gibt mehr Raum für FLR-Bevorderung. Unsere beiden Ansätze erzielen bessere Leistungen in verschiedenen Kostregionen. Insgesamt haben wir 30 bis 63 % relative Latenzreduktion erreicht. abhängig von der Ausführungszeit. und erlaubt Kosten. Schließlich haben wir einen Abbruch der durchschnittlichen Latenz reduktion in Tokens für jede Art der Funktionsknoten. Die erlaubte Ursache sind drei falsche Hinrichtungen. wie wir sehen können, sind sie gegen alle der borde Es gibt auch einige Funktionen, auf denen wir gewinnen. Impress ive Latenzreduktion, wo die rote Stange viel länger ist , wie z.B. bei Manager und Empfänger. Das sind Low-Level-Funktionen , die nicht viel Abhängigkeit haben. und andere. In der Schlussfolgerung Wir schlagen Online- Sem ester-Parsing als neue Aufgabe vor, die mit einer strengen Latenzreduktionsmetrik erforscht werden muss. mit einem starken grafischen Zementparser Wir haben eine relativ gute Latenzreduktion erreicht. entweder durch eine Pipeline -Ansatz mit L-Line-Erfüllung und einem vollen Parser oder direkt durch einen gelernten Parser auf den Präfixen. Unser Ansatz kann ein allgemeiner Rahmen sein und auf andere ausführbare semantische Repräsentationen in verschiedenen Bereichen angewendet werden. Zukunftswerke könnten eine schlauer Vorhersage- und Ausführungsintegrationsmethode erforschen. Danke für das Hören.
Hallo, ich werde unsere Arbeit über die Erzeugung von Retrieval-Augmented- Kontrar-Factuals für Fragen-Antwort-Aufgaben diskutieren. Das ist eine Arbeit, die ich während meines Praktikums bei Google Research gemacht habe, wo ich von Matthew Lam und Ian Tenney betreut wurde. Um die Aufgabe zu motivieren, las se mich mit der Definition eines Vertrags beginnen. In diesem Werk definieren wir ein Counterfactual als eine Pertribution des Inputs. Text, der sich in irgendeiner bedeutungsvollen, kontrollierten Weise von dem Originaltext unterscheidet und uns erlaubt, über die Veränderungen im Ausgang oder das Tasklabel zu reden. Zum Beispiel, wenn man die Worte faszinierend zu faszinierend ändert. Und das expected to mind ändert das Gefühl für diese Filmrevision. Ähnlich fügte er den Qualifikator Womens zu der Frage hinzu. Changes the answer to the question in the example below. Ändert die Antwort auf die Frage im Beispiel unten. Menschen sind im Vergleich zu NLP-Modellen, die auf der Aufgabe ausgebildet sind, typischerweise robust gegen solche Störungen. Warum ist das ? Der Datensatz kann mit Systemax samplt werden. Das ist eine schlichtere Entscheidung, die von der Gegenwirkung durch die Verletzung der Grenzen durchführt wird. wie in diesem Studiendeklassifikationsproblem gezeigt. Priorwerk hat festgestellt, dass das Hinzufügen von Konterfaktusbeispielen zu den Tradingdaten kann das Modell robust zu solchen Perdurationen machen. Also, wenn Fälschungsregeln wertvoll sind. Wie können wir sie generieren? Diese Aufgabe ist besonders schwer für NLP. Weil hier sind drei Beispiele aus drei verschiedenen NLP-Aufgaben. Wie Sie sehen können, gibt es Beispiele, die die Entscheidungsschwelle verletzen. Zwischen den Ergebnissen muss man sehr sorgfältig gearbeitet werden, indem man einige Attribute des Textes, die hier unterstrichen sind, perterpiniert. Das könnte man mit menschlicher Anmerkung machen, aber das ist teuer und voreingenommen. Einige frühere Arbeiten konzentrierten sich auf die Verwendung von Syntax bäumen oder der semantischen Rollenlabelung. Aber die Menge der durch diese Technik erzeugten Störungen ist durch den semantischen Rahmen begrenzt. In jüngeren Arbeiten wurden Massensprachmodelle verwendet, um Füllen Sie Masken-Partsionen des Textes aus. Aber es kann schwierig sein, zu finden, welche Teile des Textes zu stören sind. Es gibt mehr Challenges zu Generating Contracts für Fragen beantworten, speziell. Diese Aufgabe erfordert Hintergrundwissen. Zum Beispiel, um die ursprüngliche Frage zu stellen : Ist Indiana Jones Temple of Doom ein Prequel? Wir müssen uns der anderen Filme in der Franchise bewusst sein. um eine Frage zu bekommen , wie ist Indiana Jones Raiders of the Lost Ark? Darüber hinaus können zufällige Störungen zu Fragen führen, die mit den verfügbaren Beweisen nicht beantwortet werden können. oder falsche Prämissen haben. Darüber hinaus können einige Fragen perturbationen zu einer signifikanten semantischen Drift vom ursprünglichen Input führen. Zum Beispiel ist diese Frage indiana jones praktizierender kindesklaver in temple of doom Wir schlagen eine sehr einfache und dennoch effektive Technik vor , die Retrieve-Generate-Filter oder RGF genannt wird. um gegen gegen wirkende Perturbationen von Fragen zu bekämpfen und auch alle anderen erwähnten Herausforderungen zu bekämpfen. Die Kernintuition hinter RTF ist , dass die notwendige Hintergrundinformation, die benötigt wird, um Störungen zu erzeugen, in den nahen Missen des Fragestellmodells vorhanden sein kann. Zum Beispiel produziert das State-of-the-Art-Modell Realm die folgenden Top-K-Antworten auf die Frage, wer der Kapitän des Richmond Football Clubs ist. Nun, es hat den ursprünglichen Referenzpassage -Anwalt Trent Kotkin als Topmost-Choice zurückgewonnen, es hat auch zusätzliche Passagen und Antworten zurückgewonnen, die verwendet werden können, um For instance, es enthält zwei weitere Antworten, die den Kapitänen der Reserve- und der Frauenmannschaft des gleichen Clubs entsprechen, und das kann zu interessanten Edits führen. Zusammenfassend: RGF erstmals holt Top-K-relevanteste Antworten und Kontexte ab, die nicht mit der Referenzantwort und dem Kontext übereinstimmen. Nach diesem Schritt , dem Frage-Generation-Modell, werden die Bedingungen für diese alternativen Antworten, eine Frage zu generieren, die ihnen entspricht, festgelegt. Und schließlich können wir die generierten Fragen filtern, die auf Minimalität basieren oder auf der Art der semantischen Pertribution basieren, die wir einführen möchten. Über jeden Schritt geht man im Detail. Für das Retrieval verwenden wir ein Retrieve- und Read-Modell wie Realm, das die Originalfrage als Input nimmt und ein großes Korpus wie Wikipedia. Es besteht aus zwei Modulen. Das Retriever-Modul führt eine Ähnlichkeitsüberprüfung über einen dichten Index von Passagen durch, um die topk-relevantesten Passagen der Frage zu finden, und ein Reader-Modul, das Extrakt aus jedem Passage. als eine potenzielle Antwort. Realm retrieves the gold. Passage and Answer in most cases. In diesem Werk sind wir jedoch mehr interessiert in die Antworten und Kontexte, die es weiter hinunter die Linie retrieves. Im nächsten Schritt Wir verwenden diese alternativen Antworten und Kontexte, um neue Fragen zu erzeugen , die diesen Alternativen entsprechen. Das ist das Modell der Generation Question. Pre-Trained Text-to-Text-Transformer, das ist auf die NQ-Daten ausgelegt. um eine Frage für eine Antwort zu generieren, die im Kontext markiert ist. Während der Inferenz liefern wir das Fragestellungsmodell, die alternative Antwort und Kontexte, die wir in der vorherigen Folge zurückgewonnen haben. Zum Beispiel für das Query. Wer ist der Kapitän des Richmond Football Clubs? Das Frauenteam des Clubs, kapitän von Jess Kennedy, Und das Generation-Modell der Frage: Der Question, der Kapitän des Richmond Football Clubs, der erste weibliche Team des Jahres. die eine spezifische somatische Perturbation hat. In ähnlicher Weise bekommen wir auch Qu eries wie: Wer ist Kapitän Richmond s VFL-Reserv eteam oder wer hat Graham negiert im Grand Final letztes Jahr? Schließlich filtern wir eine Teilmenge der generierten Anfragen auf der Grundlage einiger gewünschter Merkmale aus. Wie bereits erwähnt, möchten wir sicherstellen, dass die neue Frage dem Original semantisch noch nahe steht. Für eine Filtertechnik, die keine zusätzliche Aufsicht erfordert, behalten wir einfach neue Fragen dass sie eine kleine, auf Token-Ebene angelegte Distanz von der ursprünglichen Frage haben. Zum Beispiel: Wir entfernen die Frage , wer Graham in der Grand Final letztes Jahr negierte, weil es eine lange Zeit dauert. Längere Distanz von der ursprünglichen Frage. In unseren Experimenten demonstrieren wir, dass diese einfache Heuristik kann zu Augment and Q-Training-Daten verwendet werden. Wir experimentieren auch mit einer Filterstrategie, die auf der Art der semantischen Perturbation basiert. Zu diesem Zweck verwenden wir ein allgemeines Queriedekompositionen-Framework, das QED genannt wird. QED identifiziert zwei Teile der Frage , ein Predikat und eine Referenz. Referenzen sind nun Phrasen in der Frage, die Entitäten im Kontext entsprechen. Ein Predikat ist im Grunde der verbleibende Teil der Frage. Zum Beispiel sind wir in der Lage, die Query zu dekomposieren. Das war die erste Frauenmannschaft von Captain Richmond. In zwei Referenzen : Richmond Football Club, Frauenmannschaft Und das Prädikat: "Wer hat Kapitän X?" Ein Modell, das auf Referenzpredikaten-Anmerkungen für Und Q gibt uns diese Frage der Dekomposition. Das dekomposieren der ursprünglichen und der generierten Fragen basierend auf Q&amp;E. Er erlaubt uns, unsere Generatoren zu kategorisieren. Wir haben zwei Gruppen von Fragen erhalten. Das ist der Referenzwechsel, während wir die Prädikate behalten. Und die, die einen Prädikatwechsel durchlaufen, und optionale Referenzen für For instance. Wer Captain Richmonds VFL-Reserve-Team ist eine Referenzänderung. Während wer trägt Nummer neun für Der Club ist ein Prädikatwechsel. Wir bewerten jetzt die Wirksamkeit von RGF-Perturbationen , wenn sie auf Trainingsdaten ergänzt werden. um die Wirksamkeit der gegenwahrscheinlichen Erweiterung besonders effektiv zu bewerten. Wir experimentieren mit zwei starken Daten-Augmentations-Basislinien. Die erste Baseline , die als Random Answer and Question Generation bezeichnet wird, Das hat keine Beziehung zu der ursprünglichen Frage. Das heißt, Passagen und Antworten sind einfach zufällig aus Wikipedia samplt. Diese Baseline fügt im Grunde mehr Daten hinzu, die wie NQ aussehen. mit der zweiten Wäsche, Gold, Antwort und Frage, Generation. Wir werden speziell den Retrieval-Teil unserer Methode ablösen. Hier werden nur alternative Antworten aus demselben Abschnitt ausgewählt, der die goldene Antwort enthielt. Wie gehen die Basislinien und RJF? # Augmentation Auf Read-Comprehension, wo das Modell Zugang zu Frage und Kontext hat. Wir experimentieren mit sechs Out-of-Domain-Datensätzen und präsentieren hier die Ergebnisse. Wo die Training-Daten in Augmentation verdoppelt werden. Wir finden das beide. Daten-Augmentation-Basislinien sind nicht in der Lage, die Domain-Generalisierung zu verbessern. Ein Ensemble von sechs Modellen, trainiert auf den Originaldaten. Es scheint die konkurrenzfähigste Basis zu sein. Verglichen mit der Basislinie finden wir, dass RGF-Counterfactuals die in der Lage sind, die Performance außerhalb der Domain zu verbessern, während sie die Performance in der Domain beibehalten. Das suggeriert, dass die Reasoning Gaps des Modells Die Quanti fakt-Augmentation ist effektiver als das Hinzufügen von Daten aus der Training-Distribution. Darüber hinaus sind wir auch sehr zufrieden mit der neuen Technologie. Wir finden , dass wir mit Retrieval zu samplenden alternativen Ausgängen oder Antworten ist wichtig für eine effektive CDA. Wir experimentieren auch mit Open-Domain-Q A-Einstellungen, bei denen das Modell nur die Frage sieht. Und noch einmal, wir bewerten vier Out-of-Domain-Datensätze. Wir finden , dass Baseline-Modelle nicht so effektiv sind für die Aus-der-Domain-Vergänzung. Die Datenvergrößerung mit R GF zeigt mehr signifikante Verbesserungen. Wir haben sogar in der Indomäne-NQ-Datensatz verbessert. Wir haben die Hypothese aufgestellt, dass die kontrafaktischen Daten Augmentation hilft dem Modell. In Learning Better Query Encoding für sehr ähnliche Queries. Schließlich bewerten wir auch die Fähigkeit des Modells, die Konsistenz in der lokalen Nachbarschaft der ursprünglichen Frage zu verbessern. Die Konsistenz misst den Anteil der von dem Modell korrekt beantworteten Fragen, bei denen sowohl die ursprüngliche als auch die kontrafaktuelle Frage richtig beantwortet sind. Das hilft uns, die Robustheit des Modells zu messen. zu kleinen Perturbationen in der Nachbarschaft der Original-Input Wir experimentieren mit fünf Datensätzen , die Paare von Fragen enthalten, die sich semantisch nahe stehen. Abgesehen von den drei Datensätzen EQA, Ambiguous und CoreF kontrastieren sie, die bereits verfügbar sind. Wir bewerten auch auf RGF -Counterfactuals, die mit Original- und Q-Fragen gepaart sind, basierend auf dem, ob sie eine Prädikatsänderung oder einen Referenzwechsel durchlaufen haben. Diese Untersätze wurden im Haus annottiert, um Lärm zu eliminieren. und werden als Ressource zur Verfügung gestellt. Alle Bas islinien sind nicht in der Lage, die Konsistenz mit dem Ensemble-Modell&nbsp;&nbsp; Improving Consistency via Small Margin Allerdings hat RDF-Konterfaktualvergrößerung beeindruckende Gewinne und Konsistenz. Sowohl auf vorherigen Datensätzen als auch auf den beiden Subsätzen werden wir kuratiert. für Referenz - und Predikate-Motivationen. Beachten Sie, dass die erweiterte Argie der Daten nicht durch Motivationstypen verzerrt ist, nur die Evaluierungssätze sind. In der Tat wurde eine qualitative Inspektion der Arten von Counterfeits erstellt. Zeigen Sie, dass die generierten Fragen mehrere verschiedene Störungen enthalten. Zum Beispiel diese ursprüngliche Frage über die Bevölkerung von Walnut Grove, Minnesota, ist auf unterschiedliche Dimensionen wie Stadt, Staat, Land und Land verwirrt. und verschiedene Prädikate wie Standort, Poverty, Anzahl der Schulen. R GF-Perturbationen sind kontextspezifisch. Zum Beispiel für diese andere Frage über den Wimmenden. Das Einzel-Turnier ist eine lange, Typ of Game, Typ of Tournament oder das Game-Outcome. Wir lösen die Aufgabe, die Counterfactual-Daten zu berücksichtigen. #Ah Augmentation and Perturbations for Informationen suchen, Queries suchen und Wir haben uns mit den einzigartigen Herausforderungen der Reversal of the Generation-Ansatz befasst. Übergen erieren mit Nearsight-Misses des Modells und filtern nach Störungsart oder Minimalität. Wir finden , dass diese Technik keine zusätzliche Aufsicht erfordert und die Beispiele sind für Augmentation gekennzeichnet. Augmentation verbessert die Ausdehnung der Domäne und die Nachbarschaftskonsistenz. Und wir finden , dass RGF-Kontrast-Fakturiere semantisch divers sind, ohne zu introduzieren. Buyers during Augmentation. (Bei Augmentation) Danke fürs Gespräch.
