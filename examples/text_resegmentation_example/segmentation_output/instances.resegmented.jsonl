{"index": 0, "docid": 0, "segid": 0, "prediction": "Hallo, das ist Elena und ich werde unsere Arbeit präsentieren , indem wir uns in spanischen und angepreßten Modellen und Modellen vorstellen. (Stimme)", "reference": "Hallo, hier ist Elena und ich stelle nun unsere Arbeit vor: Die Erkennung nicht-assimilierter Entlehnungen im Spanischen: Ein annotierter Korpus und Ansätze zur Modellierung."}
{"index": 1, "docid": 0, "segid": 1, "prediction": "Wir werden also darüber reden, was lexikalisches Borgen ist, die Aufgabe, die vorgeschlagen haben, die wir veröffentlicht haben und einige Modelle,", "reference": "Wir werden uns also damit beschäftigen, was die lexikalische Entlehnung ist, die von uns vorgeschlagene Aufgabe, den veröffentlichten Datensatz und einige untersuchte Modelle."}
{"index": 2, "docid": 0, "segid": 2, "prediction": "Aber was ist mit der Lexik und was ist mit der Mathematik?", "reference": "Doch zunächst einmal: Was ist die lexikalische Entlehnung und warum ist sie als NLP-Aufgabe so wichtig?"}
{"index": 3, "docid": 0, "segid": 3, "prediction": "Leksikale Verwandlung ist in der Regel die Verwandlung von Worten aus einer Sprache in eine andere Sprache,", "reference": "Die lexikalische Entlehnung ist im Grunde die Übernahme von Wörtern aus einer Sprache in eine andere Sprache."}
{"index": 4, "docid": 0, "segid": 4, "prediction": "in Spanisch verwenden Sie Wörter aus dem Englischen", "reference": "Zum Beispiel verwenden wir im Spanischen Wörter, die aus dem Englischen stammen."}
{"index": 5, "docid": 0, "segid": 5, "prediction": "und Sie haben einige Beispiele wie Pods in English, All these English words are sometimes in Spanish, Lexikale Verwandlung ist ein Sprachverwandlungsmodell.", "reference": "Und hier ein paar Beispiele: Wörter wie Podcast, App und Online-Crowdfunding sind englische Wörter, die wir manchmal im Spanischen verwenden."}
{"index": 6, "docid": 0, "segid": 6, "prediction": "#um, die eigentlich einfach nur in einer Sprache verwendet werden, die von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen anderen Sprachen und von anderen", "reference": "Die lexikalische Entlehnung ist eine Art der sprachlichen Entlehnung, die im Grunde genommen die Reproduktion von Mustern einer Sprache in einer anderen Sprache bedeutet."}
{"index": 7, "docid": 0, "segid": 7, "prediction": "Sprachen und von anderen Sprachen und von anderen Sprachen und von Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen und von anderen Sprachen.", "reference": "Manchmal wurde die Entlehnung mit dem Code-Switching verglichen und als ein Kontinuum beschrieben. Code-Switching wird von Zweisprachigen praktiziert, wenn sie zwei Sprachen gleichzeitig verwenden."}
{"index": 8, "docid": 0, "segid": 8, "prediction": "Es gibt jedoch einige Unterschiede zwischen dem Text und dem Sch", "reference": "Es gibt jedoch einige Unterschiede zwischen lexikalischer Entlehnung und Code-Switching."}
{"index": 9, "docid": 0, "segid": 9, "prediction": "reibverhalten, die man auf dem Text verarbeiten kann.", "reference": "Wir werden uns auf die lexikalische Entlehnung konzentrieren."}
{"index": 10, "docid": 0, "segid": 10, "prediction": "Das Schreibverhalten ist etwas, das durch die Definition von Linguistik und der Sprachverarbeitung nicht integriert ist. Es ist auch etwas, das durch das Sprachverhalten integriert ist.", "reference": "Zweisprachige Personen praktizieren das sogenannte Code-Switching. Per Definition sind die Code-Switches nicht Teil der verwendeten Sprachen, während die lexikalische Entlehnung auch von einsprachigen Personen verwendet wird."}
{"index": 11, "docid": 0, "segid": 11, "prediction": "Das die Bor rowings mit der Grammatik der Empfängersprache übereinstimmen", "reference": "Die Entlehnungen werden der Grammatik der Empfängersprache angepasst."}
{"index": 12, "docid": 0, "segid": 12, "prediction": "und dass die Borrowings in der Empfängersprache integriert werden können.", "reference": "Entlehnungen können Schritt für Schritt in die Empfängersprache integriert werden."}
{"index": 13, "docid": 0, "segid": 13, "prediction": "Warum ist das Leihen also ein interessantes Phänomen?", "reference": "Warum ist Entlehnen so ein interessantes Phänomen?"}
{"index": 14, "docid": 0, "segid": 14, "prediction": "Aus der Sicht der Linguistik ist das Leihen eine Manifestation, wie Sprachen sich verändern und wie sie interagieren.", "reference": "Aus Sicht der Linguistik ist die Entlehnung eine Manifestation dessen, wie sich Sprachen verändern und wie sie interagieren."}
{"index": 15, "docid": 0, "segid": 15, "prediction": "Und auch L eksikale Lern ern sind Beispiele für neue", "reference": "Auch lexikalische Entlehnungen sind eine Quelle für neue Wörter."}
{"index": 16, "docid": 0, "segid": 16, "prediction": "Lernformen, die in der spanischen Sprache eingeführt wurden. .", "reference": "Hier finden Sie einige Beispiele für lexikalische Entlehnungen, die als neue Wörter in die spanische Sprache aufgenommen wurden."}
{"index": 17, "docid": 0, "segid": 17, "prediction": "In Bezug auf NLP #ah Borrowings sind eine Ausgabe von Wörtern aus der Sprache und in", "reference": "Beim NLP sind Entlehnungen eine häufige Quelle von Wörtern, die nicht im Wortschatz enthalten sind."}
{"index": 18, "docid": 0, "segid": 18, "prediction": "der Tat automatisch auswählen, wie die Textverarbeitung für NLP-T ests wie Sprachverknüpfungen oder Maschinendurchsetzungen verwendet werden kann.", "reference": "Die automatische Erkennung lexikalischer Entlehnungen erwies sich als nützlich für NLP und nachgelagerte Aufgaben wie Parsing, Text-zu-Sprache-Synthesen oder die maschinelle Übersetzung."}
{"index": 19, "docid": 0, "segid": 19, "prediction": "Es der Wirkung von Englisch auf andere Sprachen, besonders Ich habe mir vorgenommen, dass ich dem Englischen auskennen kann, was manchmal als \"Anglicisations\" bezeichnet wird.", "reference": "Der Einfluss des Englischen auf andere Sprachen erfährt immer stärkeres Interesse, insbesondere bei englischen lexikalischen Entlehnungen. Diese werden manchmal auch als Anglizismen bezeichnet."}
{"index": 20, "docid": 0, "segid": 20, "prediction": "Und du hast einige Beispiele für #ah-Work auf der automatischen Ausarbeitung von #ah-Bewerbungen in einigen dieser Sprachen.", "reference": "Hier sind einige Beispiele von Arbeiten zur automatischen Erkennung von Entlehnungen in einigen dieser Sprachen."}
{"index": 21, "docid": 0, "segid": 21, "prediction": "Die Aufgabe, die wir vorschlagen, ist es zu detektieren, nicht analysierte lexikalische Bogen in spanischen Nachrichten, was bedeutet, dass", "reference": "Die Aufgabe, die wir vorschlagen, besteht also darin, nicht-assimilierte lexikalische Entlehnungen in spanischen Nachrichten zu erkennen."}
{"index": 22, "docid": 0, "segid": 22, "prediction": "wir interessiert sind, in aus Die anderen Sprachen, die in spanischen Zeitungen verwendet wurden, waren nicht integriert oder in die Sprache integriert ,", "reference": "Wir sind daran interessiert, aus anderen Sprachen entlehnte Wörter zu extrahieren, die in spanischen Zeitungen verwendet werden, aber nicht in die Empfängersprache integriert oder assimiliert wurden."}
{"index": 23, "docid": 0, "segid": 23, "prediction": "also nicht in spanisch integriert.", "reference": "Sie wurden also noch nicht ins Spanische integriert."}
{"index": 24, "docid": 0, "segid": 24, "prediction": "Hier haben Sie ein Beispiel für", "reference": "Hier ist ein Beispiel."}
{"index": 25, "docid": 0, "segid": 25, "prediction": "diese Sprache.", "reference": "Dies ist ein Satz auf Spanisch: Las prendas bestsellers se estampan con motivos florales, animal print o retales tipo patchwork."}
{"index": 26, "docid": 0, "segid": 26, "prediction": "Sie sehen die drei Spannungen, die eigentlich englisch sind, English Work, Animal Printing und Packing Work.", "reference": "Wie Sie sehen können, sind hier drei Textpassagen, die eigentlich englische Wörter sind: Bestseller, Animal Print und Patchwork."}
{"index": 27, "docid": 0, "segid": 27, "prediction": "Das ist die Art Spannungen, die wir in der Erforschung und Erforschung interessieren.", "reference": "Bei diesen Passagen wollen wir extrahieren und erkennen."}
{"index": 28, "docid": 0, "segid": 28, "prediction": "Es gab vorherige Worte. auf Anglistik der Anglistik, die von der C.R.F. Model für die Anglistik der spanischen Nachrichten in der spanischen Version erstellt wurde .", "reference": "Es gab früher schon Arbeiten über die Erkennung von Anglizismen. Diese beschäftigten sich mit einem CRF-Modell für die Erkennung von Anglizismen in spanischen Nachrichten."}
{"index": 29, "docid": 0, "segid": 29, "prediction": "Dieser Model hat einen Punkt von sechsundsechzig, aber", "reference": "Dieses Modell erreichte einen F1-Score von 86."}
{"index": 30, "docid": 0, "segid": 30, "prediction": "es gibt einige Einschränkungen in der Modellierung und in der Modellierung.", "reference": "Es gab jedoch einige Einschränkungen sowohl beim Datensatz als auch beim Modellierungsansatz."}
{"index": 31, "docid": 0, "segid": 31, "prediction": "Es ist auch so, dass es in der Zeit, in der wir uns in der befinden, in der wir uns in", "reference": "Der Datensatz konzentrierte sich also ausschließlich auf eine Quelle von den Nachrichten und bestand nur aus Schlagzeilen."}
{"index": 32, "docid": 0, "segid": 32, "prediction": "der Zeit befinden, in der wir uns in der Zeit befinden.", "reference": "Außerdem gab es Überschneidungen bei den Entlehnungen, die im Trainingssatz und im Testsatz vorkommen."}
{"index": 33, "docid": 0, "segid": 33, "prediction": "die Präsentation der Modellierung der Modellierung kann eigentlich generell zu den vorherigen Bawings.", "reference": "Dadurch konnte nicht beurteilt werden, ob der Modellierungsansatz tatsächlich auf zuvor unbekannte Entlehnungen verallgemeinert werden kann."}
{"index": 34, "docid": 0, "segid": 34, "prediction": "#ah, so wir wollen es, einige dieser #ah, Limitation in der Taske zu verarbeiten.", "reference": "Unser Ziel ist es also, einige dieser Einschränkungen in der Aufgabe zu überwinden."}
{"index": 35, "docid": 0, "segid": 35, "prediction": "Also zu Beginn mit der wir eine neue Datensatz ung erzeugt haben.", "reference": "Zu Beginn haben wir also einen neuen Datensatz erstellt."}
{"index": 36, "docid": 0, "segid": 36, "prediction": "Die neue Datensatzung ist Anmerkung: mit den Textverarbeit ungen und dem Test wird erzeugt, dass es so schwierig ist, es möglich ist,", "reference": "Das Ziel war ein neuer Datensatz, der mit lexikalischen Entlehnungen annotiert wurde, und einen möglichst schwierigen Testsatz zu erstellen."}
{"index": 37, "docid": 0, "segid": 37, "prediction": "es so wenig wie möglich in den Themen zwischen dem Training und dem Test", "reference": "Es gäbe also minimale Überschneidungen bei Wörtern und Themen zwischen dem Trainingssatz und dem Testsatz."}
{"index": 38, "docid": 0, "segid": 38, "prediction": "zu finden ist, und dass die Tests aus den Quellen und den Quellen kommen. Wir sehen nicht in der Trainings-Serie,", "reference": "Das Ergebnis ist, dass der Testsatz aus Quellen und Daten stammt, die wir nicht im Trainingssatz sehen."}
{"index": 39, "docid": 0, "segid": 39, "prediction": "dass Sie nicht in der Zeit überlaufen können.", "reference": "Hier können Sie sehen, dass es keine Überschneidungen in der Zeit gibt."}
{"index": 40, "docid": 0, "segid": 40, "prediction": "Es ist auch sehr schwierig,", "reference": "Außerdem enthält der Testsatz auch sehr viele Entlehnungen."}
{"index": 41, "docid": 0, "segid": 41, "prediction": "Ihnen einige Zahlen zu geben, wenn Sie die Trainings-Serie mit sechs Boards haben. tausend Tok en, die enthält zwanzig Barings. Ich habe tausend Token.", "reference": "Um Ihnen ein paar Zahlen zu nennen: Wenn der Trainingssatz sechs Entlehnungen pro 1000 Token enthält, enthält der Testsatz 20 Entlehnungen pro 1000 Token."}
{"index": 42, "docid": 0, "segid": 42, "prediction": "Die Test stelle enthält so viele aus Vokabular gewählte Wörter wie möglich.", "reference": "Der Testsatz enthielt so viele Vokabelwörter wie möglich."}
{"index": 43, "docid": 0, "segid": 43, "prediction": "Tatsächlich waren 92 % der Borrowings in der Teststelle OOB,", "reference": "Tatsächlich sind 92 Prozent der Entlehnungen im Testsatz OOV."}
{"index": 44, "docid": 0, "segid": 44, "prediction": "also waren sie nicht während des Trainings gesehen.", "reference": "Sie waren also während des Trainings nicht bekannt."}
{"index": 45, "docid": 0, "segid": 45, "prediction": "#um der Korpus ist eigentlich eine Sammlung von Texten, die aus verschiedenen Sprach arten der spanischen Zeitungen stammen", "reference": "Der Korpus bestand im Wesentlichen aus einer Sammlung von Texten, die aus verschiedenen Quellen spanischer Zeitungen stammten."}
{"index": 46, "docid": 0, "segid": 46, "prediction": "und #um ist von Hand verwendet,", "reference": "Er wurde manuell mit zwei Tags annotiert."}
{"index": 47, "docid": 0, "segid": 47, "prediction": "einen englischen Text zu verwenden, der für die meisten Sprachveröffentlichungen in Spanien und dann für andere Sprachen verwendet wird.", "reference": "Einer für englische lexikalische Entlehnungen, die den Großteil der lexikalischen Entlehnungen im Spanischen ausmachen, und dann das andere Label für Entlehnungen aus anderen Sprachen."}
{"index": 48, "docid": 0, "segid": 48, "prediction": "Wir verwenden Konformform ate und wir verwenden BIO-Encoder. dass wir ein Single-Token-Browning, wie z.B. ein Multi-Token-Browning oder ein Machine Learning-Browning machen können.", "reference": "Wir verwenden CONLL-Formate und die BIO-Kodierung, sodass wir einfache Token-Entlehnungen wie „App“ oder mehrteilige Token-Entlehnungen wie „maschinelles Lernen“ kodieren können."}
{"index": 49, "docid": 0, "segid": 49, "prediction": "Diese Zahlen der Corpus", "reference": "Das sind die Nummern des Korpus."}
{"index": 50, "docid": 0, "segid": 50, "prediction": "können Sie sagen, dass es drei hundert undsiebzigtausend Tonnen", "reference": "Wie Sie sehen können, handelt es sich um etwa 370 000 Token."}
{"index": 51, "docid": 0, "segid": 51, "prediction": "sind und Sie haben die Nummer von dass wir als Englisch und die Sp rachsprachigen als andere Wörter und wie viele von ihnen wir haben.", "reference": "Hier sehen Sie die Reihe an Passagen, die als Englisch markiert wurden, und die Passagen, die als andere Entlehnungen markiert waren, und wie viele davon einzigartig waren."}
{"index": 52, "docid": 0, "segid": 52, "prediction": "Und hier haben wir ein paar Beispiele von der Datensatz -Satzung,", "reference": "Hier sehen Sie einige Beispiele für den Datensatz."}
{"index": 53, "docid": 0, "segid": 53, "prediction": "wie Sie sehen können, Wir haben in der ersten Example wir die Bauing. Das ist ein Multi-Wort-Borgen.", "reference": "Wie Sie zum Beispiel hier sehen können, haben wir im ersten Beispiel die Entlehnung „batch cooking“, die eine mehrteilige Wort-Entlehnung ist."}
{"index": 54, "docid": 0, "segid": 54, "prediction": "Und wir haben es mit der BIO- BIO-Anmeldung wird immer verwendet.", "reference": "Wir haben dieses Wort mit der BIO-Kodierung annotiert."}
{"index": 55, "docid": 0, "segid": 55, "prediction": "Wo ist in Spanisch so nicht für für wo wir nicht barren? Und", "reference": "BIO wurde also für Wörter im Spanischen verwendet, also nicht für Wörter, die nicht entlehnt wurden."}
{"index": 56, "docid": 0, "segid": 56, "prediction": "hier in diesem zweiten Beispiel haben wir Penching und Crash. die auch als \"Labour\" aus Englisch bezeichnet werden.", "reference": "Hier in diesem zweiten Beispiel sehen Sie „benching“ und „crash“, die ebenfalls als Entlehnungen aus dem Englischen markiert sind."}
{"index": 57, "docid": 0, "segid": 57, "prediction": "So, als wir das Datensatz hatten, haben wir mehrere Modelle für die Aufgabe der Entdeckung und Entdeckung dieser lexikalischen Ent deckungen erforscht.", "reference": "Nachdem wir also den Datensatz hatten, untersuchten wir verschiedene Modelle für die Aufgabe, bei der wir lexikalische Entlehnungen extrahieren und erkennen wollten."}
{"index": 58, "docid": 0, "segid": 58, "prediction": "Das erste, was wir versuchen, ist die Conditioning-Modell-Modell.", "reference": "Zuerst haben wir das bedingte Zufallsfeld Modell getestet."}
{"index": 59, "docid": 0, "segid": 59, "prediction": "Das ist das Modell , das in der vorherigen Arbeit verwendet wurde.", "reference": "Das war das Modell, das bei früheren Arbeiten verwendet worden war."}
{"index": 60, "docid": 0, "segid": 60, "prediction": "Und wir benutzen die gleichen Hand werk-Features von der Arbeit,", "reference": "Wir haben die gleichen manuell erstellten Funktionen wie bei dieser Arbeit verwendet."}
{"index": 61, "docid": 0, "segid": 61, "prediction": "wie man sie von der Arbeit aus sieht.", "reference": "Wie Sie sehen können, sind dies die Funktionen."}
{"index": 62, "docid": 0, "segid": 62, "prediction": "Diese Features sind die Features, die in der Welt der Apotheke sind.", "reference": "Dies sind binäre Funktionen, wie das Wort oder das Token in Großbuchstaben."}
{"index": 63, "docid": 0, "segid": 63, "prediction": "Es ist eine Quote,", "reference": "Handelt es sich um einen Titel?"}
{"index": 64, "docid": 0, "segid": 64, "prediction": "die Mark", "reference": "Ist es ein Anführungszeichen?"}
{"index": 65, "docid": 0, "segid": 65, "prediction": "so etwas wie: sind die Art von Funktionen, die man in einer benannten Editurenerkennungs-Task erwarten würde.", "reference": "Solche Dinge sind die Art von Funktionen, die man bei einer Named Entity Recognition-Aufgabe erwarten würde."}
{"index": 66, "docid": 0, "segid": 66, "prediction": "Das sind die Ergebnisse, die wir bekommen haben.", "reference": "Das sind die Ergebnisse, die wir erhalten haben."}
{"index": 67, "docid": 0, "segid": 67, "prediction": "Ich habe fünfzig fünf von F one Score erhalten, indem ich mit der C.R.F. #ahm mit Handcrafted Feature, die unterschiedlich ist.", "reference": "Wir erhalten 55 F1-Scores, wenn wir das CRF-Modell mit manuell erstellten Funktionen verwenden."}
{"index": 68, "docid": 0, "segid": 68, "prediction": "Vergleichen das mit dem berichteten F1 Score von 86 - das Ergebnis wurde gleichen CRF-Modell er mittelt, aber mit unterschiedlichen Daten, auch für die Sprachverarbeitung.", "reference": "Das ist ein großer Unterschied im Vergleich zum bereits berichteten F1-Score von 86, der ein Ergebnis desselben CRF-Modells mit derselben Funktionen war, aber auf einen anderen Datensatz angewendet wurde, auch für die Erkennung von spanischen lexikalischen Entlehnungen."}
{"index": 69, "docid": 0, "segid": 69, "prediction": "dass die Daten, die wir erstellt haben, viel schwieriger sind als die Wir brauchen , um Modelle für diese Aufgabe zu erforschen.", "reference": "Das beweist also, dass der Datensatz, den wir erstellt haben, schwieriger ist und dass wir anspruchsvollere Modelle für diese Aufgaben entwickeln müssen."}
{"index": 70, "docid": 0, "segid": 70, "prediction": "Also Wir haben das Transformator-Modell getestet,", "reference": "Wir haben also zwei Transformer-basierte Modelle getestet."}
{"index": 71, "docid": 0, "segid": 71, "prediction": "wir benutzen das Beto, das ein monolinguales Modell für Spanisch und auch für Sprach modelle ist.", "reference": "Wir haben BETO verwendet, ein einsprachiges BERT-Modell, das auf Spanisch trainiert ist, und auch ein mehrsprachiges BERT-Modell."}
{"index": 72, "docid": 0, "segid": 72, "prediction": "Wir beide Modelle, die wir durch die Transformations-Bild ung nutzen.", "reference": "Beide Modelle verwenden wir über die Transformer-Bibliothek von HuggingFace."}
{"index": 73, "docid": 0, "segid": 73, "prediction": "Das ist so, dass wir", "reference": "Das sind die Ergebnisse, die wir erhalten haben."}
{"index": 74, "docid": 0, "segid": 74, "prediction": "sagen können, dass wir besser als Beto funktionieren. Die Entwicklung ist auf der Testseite und auf Testseite und über alle Metriken.", "reference": "Wie Sie sehen können, schneidet das mehrsprachige BERT sowohl im Entwicklungssatz als auch im Testsatz und bei allen Metriken besser ab als BETO."}
{"index": 75, "docid": 0, "segid": 75, "prediction": "Das haben wir. Und ich habe die Idee, die zu vergleichen, die C.R.F. Model Ten and Eighty Two.", "reference": "Das CRF-Modell hat 82 erreicht, nur damit wir einen Vergleich ziehen können."}
{"index": 76, "docid": 0, "segid": 76, "prediction": "Das ist der C.R.F. von der Obtainer Fifty Five, der Obtainer Fifty Five von One Score, der Multilingual Bird Obtainer Eighty. die eine große Differenz sind.", "reference": "Das CRF-Modell erreichte einen F1-Score von 55, während das mehrsprachige BERT 82 erreichte, was ein großer Unterschied ist."}
{"index": 77, "docid": 0, "segid": 77, "prediction": "So, dass wir diese Ergebnisse haben, fragen wir uns selbst eine andere Frage, die lautet: Was uns? #uh Find ein Bios DMCRF-Modell, mit verschiedenen Typen von Embeddings. Es gibt verschiedene Arten von linguistischen Informationen. und so, dass die Ergebnisse von Transformationsmodellen erhalten werden. Also wir müssen es", "reference": "Nachdem wir also diese Ergebnisse hatten, stellten wir uns eine weitere Frage, nämlich: Können wir ein BiLSTM-CRF-Modell finden, verschiedene Arten von Einbettungen darin einspeisen, Einbettungen, die verschiedene Arten von sprachlichen Informationen kodieren, und die Ergebnisse von Transformer-basierten Modellen übertreffen?"}
{"index": 78, "docid": 0, "segid": 78, "prediction": "so machen, wir ran some Preliminary experiments we we. #ah, die ranne die TMCR-Modell-Mode.", "reference": "Dafür haben wir einige präliminäre Experimente durchgeführt, und zwar mit dem BiLSTM-CRF-Modell unter Verwendung von Flare Library."}
{"index": 79, "docid": 0, "segid": 79, "prediction": "Wir haben die mit verschiedenen Arten von Bäumen, wie z.B. Bäumen, aber auch mit Fassbändern und so weiter.", "reference": "Wir haben mit verschiedenen Arten von Einbettungen experimentiert, z. B. mit Transformer-basierten, aber auch mit Schnell-Text-Einbettungen und Zeichen-Einbettungen."}
{"index": 80, "docid": 0, "segid": 80, "prediction": "Was wir herausfanden, dass Transformator-basierte Embeddings besser funktionieren als nicht-kontextuelle Embeddings , dass die Kombination aus englischen und spanischen Embeddings aus Multi-Lingual-Bedings und B.P. Embeddings produziert wird.", "reference": "Wir haben herausgefunden, dass Transformer-basierte Einbettungen besser abschneiden als nicht kontextualisierte Einbettungen, dass die Kombination aus englischer BERT- und spanischer BETO-Einbettung besser ist als mehrsprachige BERT-Einbettungen."}
{"index": 81, "docid": 0, "segid": 81, "prediction": "Besser als ein und ein und ein und ein und ein.", "reference": "Auch ergeben die BPE-Einbettungen ein besseres F1 und die Zeicheneinbettungen ein besseres Recall."}
{"index": 82, "docid": 0, "segid": 82, "prediction": "Das ist besser, wie ich das hier denke.", "reference": "Vor diesem Hintergrund waren dies die besten Ergebnisse, die wir erzielen konnten."}
{"index": 83, "docid": 0, "segid": 83, "prediction": "Best performing weighs so viel. Modelle werden von der MCRF Modell.", "reference": "Beide Modelle waren BiLSTM-CRF-Modelle unter Verwendung von Flare."}
{"index": 84, "docid": 0, "segid": 84, "prediction": "#ah, wenn man ein Stück weiß, ist Beton und Betonbeton und B.P. und der andere Beton und B.P. und auch B.P. und auch und B.P. Das ist das", "reference": "Bei einem wurden BETO- und BERT-Einbettungen und BPE eingespeist, beim anderen BETO- und BERT-Einbettungen und BPE sowie Zeichen-Einbettungen."}
{"index": 85, "docid": 0, "segid": 85, "prediction": "letzte, das ist der höchste Punkt auf der Teststelle, der höchste Punkt auf der Entwicklung wird von der Entwicklung durch die", "reference": "Letzteres war dasjenige, das den höchste F1-Score beim Testsatz erzielte, obwohl der höchste Score beim Entwicklungssatz durch das Modell ohne Zeichen-Einbettungen erreicht wurde."}
{"index": 86, "docid": 0, "segid": 86, "prediction": "eine mit den B.P. Ich denke, dass wir mit dem besten Ergebnis mit einer Mütze von siebenundsechzig auf Entwicklung und zwei auf Testseite kommen.", "reference": "Vergessen Sie nicht, dass das beste Ergebnis, das wir mit mehrsprachigem BERT erzielt haben, einen F1-Wert von 76 im Entwicklungssatz und 82 im Testsatz erreichte."}
{"index": 87, "docid": 0, "segid": 87, "prediction": "Das ist eine Verbesserung, die mit diesen Ergebnissen vergleichen lässt.", "reference": "Dies ist also eine Verbesserung im Vergleich zu diesen Ergebnissen."}
{"index": 88, "docid": 0, "segid": 88, "prediction": "Wir haben uns endlich gefragt, eine andere Frage stellen können, Kann die translationsfähige Ausdehnung von der Sprachidentifizierung und der Sprachverwechslung so aussehen,", "reference": "Schließlich stellten wir uns noch eine weitere Frage: Kann die Erkennung von lexikalischen Entlehnungen als Transferlernen von Sprachidentifikation beim Code-Switching formuliert werden?"}
{"index": 89, "docid": 0, "segid": 89, "prediction": "wir das gleiche Modell haben, wir haben, aber wir verwenden nicht die verwendeten Transformations-Behälter. und aber auch in den Bäudungen. Wir verwenden Code Switching und Betten.", "reference": "Wir haben also dasselbe BiLSTM-CRF-Modell wie mit Flare verwendet, aber anstelle dieser nicht angepassten Transformer-basierten BETO- und BERT-Einbettungen haben wir Code-Switch-Einbettungen verwendet."}
{"index": 90, "docid": 0, "segid": 90, "prediction": "Was sind Code Switching und Betten?", "reference": "Was sind Code-Switch-Einbettungen?"}
{"index": 91, "docid": 0, "segid": 91, "prediction": "die in der Transform ation eingeb aut wurden, die für die Sprachidenti fizierung in der spanischen englischen Sprach abteilung der Sprachvermittlung verwendet", "reference": "Dies sind Einbettungen, die auf Transformer-basierte Einbettungen abgestimmt wurden. Diese wurden für die Sprachidentifikation im Spanisch-Englisch-Abschnitt des LinCE-Code-Switching-Datensatzes vortrainiert."}
{"index": 92, "docid": 0, "segid": 92, "prediction": "wurden , die der Sprachvermittlung in der spanischen Sprachabteilung der Sprachvermittlung angesiedelt ist.", "reference": "LinCE ist ein Datensatz vom Code-Switching, der einen Abschnitt mit Code-Switching von Spanisch und Englisch enthält."}
{"index": 93, "docid": 0, "segid": 93, "prediction": "Fett unsere Biol ast-TMCRF Switch -Bedding und Optional-Bedding und B .P.B.B.ing und so weiter.", "reference": "Wir speisten also Code-Switch-Einbettungen in unser BiLSTM-CRF ein. Optional können Zeicheneinbettungen, BPE-Einbettungen und so weiter eingefügt werden."}
{"index": 94, "docid": 0, "segid": 94, "prediction": "Das beste Ergebnis ist vierundzwanzig Punkt, den wir alle versuchen, auf der Teststelle zu testen. Das beste Ergebnis ist, dass wir einen Punkt auf", "reference": "Das beste Ergebnis, das wir erzielt haben, war 84,22. Das ist das beste Ergebnis aller Modelle, die wir mit dem Testsatz ausprobiert haben."}
{"index": 95, "docid": 0, "segid": 95, "prediction": "der Entwicklung erzielen, der siebenund zwanzig ist, und das beste Ergebnis ist, dass wir mit unadaptierten Einbettungen.", "reference": "Obwohl der beste F1-Score, den wir beim Entwicklungssatz erzielt haben, 97 war, war dieser niedriger als das beste Ergebnis vom BiLSTM-CRF, das mit unangepassten Einbettungen eingespeist war."}
{"index": 96, "docid": 0, "segid": 96, "prediction": "Also einige Schlussfolgerungen aus unserer Arbeit haben wir", "reference": "Hier sind einige Schlussfolgerungen aus unserer Arbeit."}
{"index": 97, "docid": 0, "segid": 97, "prediction": "Wir haben ein neues Datensatz von Spanish News Wired produziert. Das ist mit dem nicht-verknüpften, nicht-verknüpften Barwins,", "reference": "Wir haben einen neuen Datensatz mit spanischen Nachrichten erstellt, der mit nicht assimilierten lexikalischen Entlehnungen annotiert ist."}
{"index": 98, "docid": 0, "segid": 98, "prediction": "der ist mehr als der Barwins und", "reference": "Dieser Datensatz ist dichter an Entlehnungen und OOV-reicher als frühere Ressourcen."}
{"index": 99, "docid": 0, "segid": 99, "prediction": "wir haben vier Arten von Modellen für die Erforschung von Barwins in der Erforschung von", "reference": "Wir haben vier Arten von Modellen für die Erkennung lexikalischer Entlehnungen erforscht."}
{"index": 100, "docid": 0, "segid": 100, "prediction": "Analysen. Wir haben mit Wick Point für alle Modelle zusammengetragen.", "reference": "Also. Was die Fehleranalyse betrifft, war der Recall ein Schwachpunkt bei allen Modellen."}
{"index": 101, "docid": 0, "segid": 101, "prediction": "#um, wie ich hier auch Frequent negative Folgen beinhalten auch Abwehrmaßnahmen, die in englischer und spanischer Sprache vorhanden sind.", "reference": "Wie Sie hier sehen können, gehören zu den häufigen falsch-negativen Ergebnissen beispielsweise auch Entlehnungen in Großbuchstaben und Wörter, die es sowohl im Englischen als auch im Spanischen gibt."}
{"index": 102, "docid": 0, "segid": 102, "prediction": "#um auch interessant die B P-B ewertungen zu verbessern ein Punkt", "reference": "Interessant ist auch, dass BPE-Einbettungen den F1-Core zu verbessern scheinen."}
{"index": 103, "docid": 0, "segid": 103, "prediction": "und zu verbessern die #um zu verbessern,", "reference": "Und die Einbettung von Zeichen scheint den Recall zu verbessern."}
{"index": 104, "docid": 0, "segid": 104, "prediction": "was #um ist interessant zu finden, dass wir es auf der Zukunft arbeiten können", "reference": "Das ist eine interessante Erkenntnis, die wir vielleicht in künftigen Arbeiten untersuchen können."}
{"index": 105, "docid": 0, "segid": 105, "prediction": "und das alles haben wir alles,", "reference": "Also. Das wäre alles, was ich zu sagen habe."}
{"index": 106, "docid": 0, "segid": 106, "prediction": "danke fürs Hören.", "reference": "Vielen Dank fürs Zuhören."}
{"index": 107, "docid": 1, "segid": 0, "prediction": "Mein Name ist Antoine.", "reference": "Mein Name ist Antoine."}
{"index": 108, "docid": 1, "segid": 1, "prediction": "Ich bin Doktorand an der Universität von Massachusetts Amherst.", "reference": "Ich bin Doktorand an der University of Massachusetts Amherst."}
{"index": 109, "docid": 1, "segid": 2, "prediction": "Ich präsentiere unsere Zeitung. Kenya Morphologie, wie? Kenya Rwanda Language Model. ein sehr gutes Beispiel.", "reference": "Ich stelle unser Paper KinyaBERT vor: ein Morphologie-bewusstes Kinyarwanda-Sprachmodell."}
{"index": 110, "docid": 1, "segid": 3, "prediction": "Heute werde ich über die Motivation für diese Forschung sprechen.", "reference": "Heute werde ich über den Grund für diese Forschung sprechen."}
{"index": 111, "docid": 1, "segid": 4, "prediction": "Dann präsentiere ich Kenyabereits Modellarchitektur im Detail.", "reference": "Dann werde ich die KinyaBERT-Modell-Architektur im Detail vorstellen."}
{"index": 112, "docid": 1, "segid": 5, "prediction": "Ich werde dann über unsere experimentellen Ergebnisse reden. Dann schließen wir einigen Schlüssen.", "reference": "Ich werde dann über unsere experimentellen Ergebnisse sprechen und zum Schluss einige Schlussfolgerungen darstellen."}
{"index": 113, "docid": 1, "segid": 6, "prediction": "Wir wissen alle, dass die jüngste Fortschritte bei der Verarbeitung natürlicher Sprachen durch die Verwendung von prätrainierten Sprachmodellen wie BERT ermöglicht wurden.", "reference": "Wir alle wissen, dass die jüngsten Fortschritte bei der NLP durch die Verwendung von vortrainierten Sprachmodellen wie BERT ermöglicht wurden."}
{"index": 114, "docid": 1, "segid": 7, "prediction": "Aber es gibt eine Reihe von Limitationen.", "reference": "Allerdings gibt es immer noch eine Reihe von Einschränkungen."}
{"index": 115, "docid": 1, "segid": 8, "prediction": "aufgrund der komplexen Morphologie, die von den meisten morphologisch reichen Sprachen ausgedrückt wird? Die Ubiquitas byte. Die al gorithmischen Tokenisierungsalgorithmen, die verwendet werden, können nicht die exakten Unter wortlexikaleinheiten extrahieren, die für die Effektivität der Morpheme benötigt werden.", "reference": "Aufgrund der komplexen Morphologie, die von den meisten morphologisch reichen Sprachen ausgedrückt wird, kann der allgegenwärtige byte pair encoding-Tokenisierungsalgorithmus, den ich verwendet habe, nicht die genauen lexikalischen Unterwort-Einheiten extrahieren, d. h. die Morpheme, die für eine effektive Repräsentation benötigt werden."}
{"index": 116, "docid": 1, "segid": 9, "prediction": "Repräsentation Zum Beispiel haben wir hier drei kinya-rwanda-Wörter. Das hat mehr Morphine in ihm. aber die B P Algorithmen können sie nicht extrahieren.", "reference": "Hier haben wir zum Beispiel drei Kinyarwanda-Wörter, die mehrere Morpheme enthalten, aber die BPE-Algorithmen können sie nicht extrahieren."}
{"index": 117, "docid": 1, "segid": 10, "prediction": "Das ist, weil es morphologische Regeln gibt. produziert verschiedene Oberflächenformen, die die genauen lexikalischen Informationen verbergen. und E, die sich auf der Oberfläche der befindet, hat keinen Zugang zu diesem lexikalischen Modell.", "reference": "Das liegt daran, dass einige morphologische Regeln verschiedene Oberflächenformen erzeugen, die die genauen lexikalischen Informationen verbergen. BPE stützt sich aber nur auf die Oberflächenformen und hat so keinen Zugang zu diesem lexikalischen Modell."}
{"index": 118, "docid": 1, "segid": 11, "prediction": "Die zweite Herausforderung ist, dass auch wenn man Zugang zu einem morphologischen Analyzer hat, die BPE-Token mit Morphemen zu ersetzen, um die morphologische Kompositionalität auszudrücken.", "reference": "Die zweite Herausforderung besteht darin, dass, selbst wenn man Zugang zu einem morphologischen Analysator von Oracle hätte, würde es nicht ausreichen, das BPE-Token durch Morpheme zu ersetzen, um die morphologische Kompositionalität auszudrücken."}
{"index": 119, "docid": 1, "segid": 12, "prediction": "Ein dritter Gap in der Forschung ist, dass Neue , prät rained Language-Modelle sind meistens auf High-Resource-Sprachen evaluiert.", "reference": "Eine dritte Lücke in der Forschung besteht darin, dass neue vortrainierte Sprachmodelle meist bei ressourcenintensiven Sprachen evaluiert werden."}
{"index": 120, "docid": 1, "segid": 13, "prediction": "und wir müssen ihre Anwendbarkeit auf Rohstoffe bewerten. und diverse Languages as well.", "reference": "Wir müssen ihre Anwendbarkeit auch bei geringen Ressourcen und in verschiedenen Sprachen bewerten."}
{"index": 121, "docid": 1, "segid": 14, "prediction": "Deshalb präsentieren wir Kenya Bird. die eine einfache, aber wirksame Anpassung der B ERT-Architektur ist, die morphologisch reiche Sprachen effektiver handhaben soll.", "reference": "Daher präsentieren wir KinyaBERT. Das ist eine einfache, aber effektive Anpassung der BERT-Architektur, die für den effektiveren Umgang mit morphologisch reichen Sprachen gedacht ist."}
{"index": 122, "docid": 1, "segid": 15, "prediction": "Wir evaluieren Kenya B ells in Kenia Rwanda, eine Low -Resource morphologisch-reiche Sprache, die von mehr als zwölf Millionen Menschen in East und Central Africa gesprochen wird.", "reference": "Wir evaluieren KinyaBERT mit Kinyarwanda, einer ressourcenarmen und morphologisch reichen Sprache, die von mehr als zwölf Millionen Menschen in Ost- und Zentralafrika gesprochen wird."}
{"index": 123, "docid": 1, "segid": 16, "prediction": "Das der Eingang zu dem Modell. ist auch eine Satz oder ein Dokument.", "reference": "Die Eingabe für das Modell ist entweder ein Satz oder ein Dokument."}
{"index": 124, "docid": 1, "segid": 17, "prediction": "Hier haben wir zum Beispiel John , du warst ja treuer Mensch, das bedeutet, wir waren überrascht, John da zu finden.", "reference": "Zum Beispiel haben wir hier den Satz: „John twarahamubonye biradutangaza“. Das bedeutet: „Wir waren überrascht, John dort anzutreffen.“"}
{"index": 125, "docid": 1, "segid": 18, "prediction": "Wie Sie sehen können, enthalten die kenianischen Wörter mehrere Morph eme, die unterschiedliche Informationen enthalten.", "reference": "Wie Sie sehen können, enthaltenWörter im Kinyarwanda mehrere Morpheme, die unterschiedliche Informationen enthalten."}
{"index": 126, "docid": 1, "segid": 19, "prediction": "Deshalb. In unserem Modell geben diesen Satz oder ein Dokument einem morphologischen Analyzer vor.", "reference": "Daher übergeben wir in unserem Modell diesen Satz oder ein Dokument an einen morphologischen Analysator."}
{"index": 127, "docid": 1, "segid": 20, "prediction": "Die dann Morpheme generiert, die in jedem der Wörter enthalten sind.", "reference": "Dieser erzeugt dann Morpheme, die in allen Wörtern enthalten sind."}
{"index": 128, "docid": 1, "segid": 21, "prediction": "Die Morpheme sind normalerweise made of a stem and zero or more affixes.", "reference": "Die Morpheme setzen sich in der Regel aus dem Stamm und null oder mehr Affixen zusammen."}
{"index": 129, "docid": 1, "segid": 22, "prediction": "Die Aff ixe können Z ehnter, Aspekt, Subjekt oder Objekt in Verbs und mehr oft in Bezug zu den Band-Namen angeben. Klasse für Subjekte und Objekte.", "reference": "Die Affixe können Zeitform, Aspekt, Subjekt oder Objekt in den Verben anzeigen und beziehen sich häufiger auf die Substantivklasse der Bantu für Subjekte und Objekte."}
{"index": 130, "docid": 1, "segid": 23, "prediction": "Der morphologische Analyzer produziert auch ein Part-of-Speech-Tag für jedes der Wörter.", "reference": "Der morphologische Analysator erzeugt auch einen Teil eines Sprach-Tags für jedes der Wörter."}
{"index": 131, "docid": 1, "segid": 24, "prediction": "Nach diesem Schritt Wir machen Embeddings für die Spee the for the part speech tags.", "reference": "Nach diesem Schritt erstellen wir Einbettungen für den Teil der Sprach-Tags."}
{"index": 132, "docid": 1, "segid": 25, "prediction": "Imbedings für die Affixes.", "reference": "Einbettungen für die Affixe."}
{"index": 133, "docid": 1, "segid": 26, "prediction": "Und ist für den Stamm.", "reference": "Einbettungen für den Stamm."}
{"index": 134, "docid": 1, "segid": 27, "prediction": "Das sind die mehr fragilisierbaren. Das ist Morphologie-Ebene.", "reference": "Dies sind die Einbettungen auf Morphologie-Ebene."}
{"index": 135, "docid": 1, "segid": 28, "prediction": "Imbedings Wir übertragen diese Embeddings dann einen Morphologie-Enkoder, der ein kleiner Transformator-Enkoder ist , der unabhängig auf jede Welt angewendet wird.", "reference": "Anschließend durchlaufen diese Einbettungen einen Morphologie-Encoder, der ein kleiner Transformer-Encoder ist, der auf jedes Wort unabhängig angewendet wird."}
{"index": 136, "docid": 1, "segid": 29, "prediction": "Die gaben sind die Vektoren, die mit der morphologischen Information an jeder Seite kontextualisiert sind.", "reference": "Ausgegeben werden die Vektoren, die mit den morphologischen Informationen bei jedem Wort kontextualisiert werden."}
{"index": 137, "docid": 1, "segid": 30, "prediction": "Jetzt führen wir Kompositionen durch. wo die morphologischen Embeddings, die einem Teil der Sprache und dem Stamm entsprechen, zusammenkorrelieren.", "reference": "Nun führen wir eine Komposition durch, bei der die morphologischen Einbettungen, die der Sprache und dem Wortstamm entsprechen, miteinander verkettet werden."}
{"index": 138, "docid": 1, "segid": 31, "prediction": "Wir werden sie weiter mit einem anderen Stamm in der Satzstufe kombinieren.", "reference": "Wir verketten sie mit einer weiteren Einbettung des Stammes auf der Ebene des Satzes."}
{"index": 139, "docid": 1, "segid": 32, "prediction": "dann bilden wir einen Input zu dem Main-Satz- oder Dokumentencoder.", "reference": "Dann erstellen wir eine Eingabe für den Hauptsatz oder den Dokument-Encoder."}
{"index": 140, "docid": 1, "segid": 33, "prediction": "Die End-Ausgabe sind kontextualisierte Embeddings, die für Downstream-NLP-Aufgaben verwendet werden können.", "reference": "Die Endausgabe sind kontextualisierte Einbettungen, die für nachgelagerte NLP-Aufgaben verwendet werden können."}
{"index": 141, "docid": 1, "segid": 34, "prediction": "für einen morphologischen Analysator. Wir verwenden endliche , zweistufige Morphologie-Prinzipien mit einer benutzerdefinierten Implementierung, die der kenia-rwandischen Sprache zugeschrieben ist.", "reference": "Für einen morphologischen Analysator verwenden wir Morphologie-Prinzipien mit endlichen Automaten auf zwei Ebenen und mit einer maßgeschneiderten Implementierung, die auf die Sprache Kinyarwanda zugeschnitten ist."}
{"index": 142, "docid": 1, "segid": 35, "prediction": "Wir modellieren die Morphologie aller kinya-rwandischen Wörter, einschließlich der Verbal, Nomen, Demonstrative und Possessive Pronomen, Numerals und andere.", "reference": "Wir modellieren die Morphologie aller Wörter auf Kinyarwanda, einschließlich der Verben, Substantive, Demonstrativ- und Possessivpronomen, Numerale und andere."}
{"index": 143, "docid": 1, "segid": 36, "prediction": "Wir benutzen unüberwachte part of Speech tagging Algorithm.", "reference": "Wir verwenden einen nicht überwachten Teil eines Sprach-Tagging-Algorithmus."}
{"index": 144, "docid": 1, "segid": 37, "prediction": "Ein First-Told-Up-Faktor-Modell wird verwendet, um Morphologie-Wahrscheinlichkeiten zu berücksichtigen. #ah, die Wahrscheinlichkeit, die ist von der morphologischen Analyzer.", "reference": "Ein faktorisiertes Modell erster Ordnung wird verwendet, um die Morphologie-Wahrscheinlichkeit zu berücksichtigen, d. h. die Wahrscheinlichkeit, die vom morphologischen Analysator zugewiesen wird."}
{"index": 145, "docid": 1, "segid": 38, "prediction": "Wir werden auch berücksichtigen, Die Part of Speech tags pre wie auch die syntactic agreements, die vorhanden sind. in den Input-Wörtern.", "reference": "Wir berücksichtigen auch den Vorrang der Sprach-Tags sowie die syntaktischen Vereinbarungen, die in den eingegebenen Wörtern vorhanden sind."}
{"index": 146, "docid": 1, "segid": 39, "prediction": "Der Part of Speech Tagger verwendet eine bidirektionale Inferenz. die die häufigsten VTB- Algorithmen für das Dekodieren verbessert.", "reference": "Der Teil des Sprach-Taggers verwendet eine bidirektionale Inferenz, die den häufiger verwendeten Viterbi-Algorithmus für die Dekodierung verbessert."}
{"index": 147, "docid": 1, "segid": 40, "prediction": "Ein paar Bemerkungen hier für Positionalschlüsselung.", "reference": "Hier noch ein paar Anmerkungen zur Positionskodierung."}
{"index": 148, "docid": 1, "segid": 41, "prediction": "Der Morphologie-Enkoder verwendet keine Positionalschlüsselung", "reference": "Erstens verwendet der Morphologie-Encoder keine Positionskodierung."}
{"index": 149, "docid": 1, "segid": 42, "prediction": ", da jeder der Morpheme einen eigenen Slot im morphologischen Modell hat,", "reference": "Das liegt daran, dass jedes der Morpheme einen bekannten Platz im morphologischen Modell einnimmt."}
{"index": 150, "docid": 1, "segid": 43, "prediction": "daher ist die Position alschlüsselung inhärent, wenn die Morpheme gegeben werden.", "reference": "Daher ist die positionelle Information inhärent, wenn die Morpheme gegeben sind."}
{"index": 151, "docid": 1, "segid": 44, "prediction": "Zweite Der Satzencoder verwendet die sogenannten untiefen relativen Positionalschlüsselungen, die kürzlich in der I con Conference veröffentlicht wurden.", "reference": "Zweitens verwendet der Satz-Encoder die so genannten ungebundenen, relativ positionellen Einbettungen, die kürzlich auf der ICLR-Konferenz veröffentlicht wurden."}
{"index": 152, "docid": 1, "segid": 45, "prediction": "Diese Positionalschlüsselungen ent ziehen die positionalen Korrelationen von Token-zu-Token-Aufmerksamkeit-Computation.", "reference": "Diese positionellen Einbettungen entkoppeln im Wesentlichen positionelle Korrelationen von Token hin zu einer Token-Aufmerksamkeitsberechnung."}
{"index": 153, "docid": 1, "segid": 46, "prediction": "Ähnlich wie bei Bert verwenden wir ein maskiertes Sprachmodell für das Vor-Training-Ziel.", "reference": "Ähnlich wie BERT verwenden wir ein maskiertes Sprachmodell als Vortrainingsziel."}
{"index": 154, "docid": 1, "segid": 47, "prediction": "Wir müssen sowohl den Stamm als auch die Affixe vorhersagen , die mit den Wörtern assoziiert sind.", "reference": "Im Wesentlichen müssen wir sowohl den Stamm als auch die Affixe, aus denen die Wörter bestehen, vorhersagen."}
{"index": 155, "docid": 1, "segid": 48, "prediction": "während des Pretraining Fünfzehn Prozent aller Wörter werden für die Vorhersage in Betracht gezogen. von denen 80 % maskiert sind, 10 % mit zufälligen Worten geschickt und 10 % unverändert bleiben.", "reference": "Während des Vortrainings werden fünfzehn Prozent aller Wörter für die Vorhersage berücksichtigt, von denen achtzig Prozent maskiert, zehn Prozent mit zufälligen Wörtern ausgetauscht und zehn Prozent unverändert gelassen werden."}
{"index": 156, "docid": 1, "segid": 49, "prediction": "Für eine Affix-Vorhersage Wir haben ein Multi-Label-Klassifikationsproblem.", "reference": "Für die Vorhersage der Affixe stehen wir vor einem Multi-Label-Klassifikationsproblem."}
{"index": 157, "docid": 1, "segid": 50, "prediction": "für this, we either group together affixes into a fixed number of sets and predict the set as class label.", "reference": "Daher fassen wir entweder die Affixe zu einer festen Reihe von Gruppen zusammen und sagen die Gruppe als Klassenlabel voraus."}
{"index": 158, "docid": 1, "segid": 51, "prediction": "Die andere Option ist, die affixed Probabilitätsvektoren zu vorhersagen.", "reference": "Die andere Möglichkeit ist die Vorhersage der Affixe durch einen Wahrscheinlichkeitsvektor."}
{"index": 159, "docid": 1, "segid": 52, "prediction": "Wir evaluieren both of these approaches in our experiments.", "reference": "Wir bewerten beide Ansätze in unseren Experimenten."}
{"index": 160, "docid": 1, "segid": 53, "prediction": "Wir haben Kenya Beret auf etwa 2,5 Gigabyte von Kinyarwanda Text und compare it to three baseline models.", "reference": "Wir trainieren KinyaBERT mit etwa zweieinhalb Gigabyte an Text in Kinyarwanda vor und vergleichen es mit drei Modellen der Baseline."}
{"index": 161, "docid": 1, "segid": 54, "prediction": "Einer ist ein multilinguales Modell, das als Excel bezeichnet wird. M R. Das ist ein Trained on a large text corpora. Das ist aus mehreren Sprachen gemacht.", "reference": "Eines davon ist ein mehrsprachiges Modell namens XLM-R, das mit großen Textkorpora trainiert wird, die aus mehreren Sprachen bestehen."}
{"index": 162, "docid": 1, "segid": 55, "prediction": "Die anderen beiden Bas eline sind auf demselben Text aus Kenia Rwanda vorbereitet, wobei entweder ein BitPay-Encoder-Algorithmus oder oder morph ologische Analysen verwenden, ohne die Transformator-Architektur zu verwenden.", "reference": "Die beiden anderen Baselines werden mit demselben Text auf Kinyarwanda vortrainiert, wobei entweder der byte pair encoding-Algorithmus oder die morphologische Analyse ohne die zweistufige Transformer-Encoder-Architektur verwendet wird."}
{"index": 163, "docid": 1, "segid": 56, "prediction": "Alle Modelle sind konfiguriert. in der B Architektur, die zwischen hundert und hundert und zehn Millionen Parameter mit Kenya Rwanda mit Kenya Bite verwendet, die geringste Anzahl von Parameter.", "reference": "Alle Modelle sind in der Basisarchitektur konfiguriert, die etwa hundert bis hundertzehn Millionen Parameter umfasst, wobei Kinyarwanda mit KinyaBERT die geringste Anzahl von Parametern verwendet."}
{"index": 164, "docid": 1, "segid": 57, "prediction": "Alle Modelle , außer der mehrsprach ige, sind für 32000 Gradient-Updates vorbereitet. mit der Batch-Size von zwei fünfhundert und Sequenzen. in jeder Batch.", "reference": "Alle außer dem mehrsprachigen Modell werden für zweiunddreißigtausend Gradient-Updates vortrainiert, mit einer Batchgröße von zweitausendfünfhundertsechzig Sequenzen in jedem Batch."}
{"index": 165, "docid": 1, "segid": 58, "prediction": "Wir bewerten die prätrainierten Modelle auf drei Set of tasks,", "reference": "Wir bewerten die vortrainierten Modelle anhand von drei Gruppen von Aufgaben."}
{"index": 166, "docid": 1, "segid": 59, "prediction": "one is the group benchmark, die für die Auswertung der Effektivität von Pre-trained Language Models verwendet wurde.", "reference": "Eine davon ist die GLUE-Benchmark, die häufig zur Bewertung der Effektivität von vortrainierten Sprachmodellen verwendet wird."}
{"index": 167, "docid": 1, "segid": 60, "prediction": "Wir haben unsere Blue Benchmark erhalten. Daten , indem wir die ursprünglichen Benchmark-Daten in Kenia, Rwanda, using Google Translate, übersetzen.", "reference": "Wir erhalten unsere GLUE-Benchmark-Daten, indem wir die originalen Benchmark-Daten mit Google Translate ins Kinyarwanda übersetzen."}
{"index": 168, "docid": 1, "segid": 61, "prediction": "Die zweite Aufgabe ist Kenya Rwanda Named Identity Recognition Benchmark. Das ist ein hochwertiger Datensatz, der von trainierten Native Speakers annotiert wurde.", "reference": "Die zweite Aufgabe ist die named entity recognition-Benchmark von Kinyarwanda, ein qualitativ hochwertiger Datensatz, der von trainierten Muttersprachlern annotiert wurde."}
{"index": 169, "docid": 1, "segid": 62, "prediction": "Der dritte... ist die News Kategorierung task. wo wir Newsartikel von mehreren Websites ziehen und ihre Kategor isierungstags korrigieren, die von den Autoren unterschrieben wurden , im Wesentlichen versuchen, das die gleichen Kategorien.", "reference": "Bei der dritten Aufgabe handelt es sich um eine Kategorisierung von Nachrichten. Hier rufen wir Nachrichtenartikel von verschiedenen Websites ab und sammeln ihre Kategorisierungstags, die von den Autoren zugewiesen wurden. Im Wesentlichen versuchen wir, die gleichen Kategorien vorherzusagen."}
{"index": 170, "docid": 1, "segid": 63, "prediction": "Und gehen wir zu den Ergebnissen.", "reference": "Sehen wir uns jetzt die Ergebnisse an."}
{"index": 171, "docid": 1, "segid": 64, "prediction": "für die Gluebenchmark Wir finden, dass Kenya Belt konsequent Baseline-Modelle übertrifft.", "reference": "Bei der GLUE-Benchmark haben wir festgestellt, dass KinyaBERT durchweg besser abschneidet als die Baseline-Modelle."}
{"index": 172, "docid": 1, "segid": 65, "prediction": "Hier sehen wir die durchschnitt liche Leistung für ten fine tuning runs.", "reference": "Hier zeigen wir die durchschnittliche Leistung von zehn Durchläufen zur Feinabstimmung."}
{"index": 173, "docid": 1, "segid": 66, "prediction": "Wir führen auch eine User-E valuation der Übersetzungen durch Google Translate durch.", "reference": "Wir führen auch eine Benutzerevaluation der Übersetzungen durch, die von Google Translate erstellt werden."}
{"index": 174, "docid": 1, "segid": 67, "prediction": "Es ist nicht so, dass die Nutzer etwa sechstausend Beispiele bewertet haben, indem Scores auf einer Skala von 1 bis 4 zugewiesen haben. die Qualität der Transaktionen.", "reference": "Im Wesentlichen bewerteten die Benutzer etwa sechstausend Beispiele und vergaben Noten auf einer Skala von eins bis vier, um die Qualität der Übersetzungen zu bewerten."}
{"index": 175, "docid": 1, "segid": 68, "prediction": "Das Ergebnis ist, dass viele Translationen laut sind.", "reference": "Das Ergebnis war, dass viele Übersetzungen qualitativ schlecht waren."}
{"index": 176, "docid": 1, "segid": 69, "prediction": "Aber alle Mod elle mussten mit dem selben Translation Noise coppiert werden, und die relative Performance zwischen den Modellen ist immer noch wichtig zu bemerken.", "reference": "Aber alle Modelle mussten mit der gleichen schlechten Qualität der Übersetzung umgehen, und die relative Leistung zwischen den Modellen kann immer noch bedeutend festgestellt werden."}
{"index": 177, "docid": 1, "segid": 70, "prediction": "für die Namensdentizitätserkennung. Wir finden auch, dass König Berth mit der Bestleistung , mit der Best leistung der Affinity Distribution Regulation sind", "reference": "Bei der Aufgabe Named Entity Recognition stellten wir außerdem fest, dass KinyaBERT die beste Leistung erbringt, wobei die Variante mit der Verteilungsregression der Affixe am besten abschneidet."}
{"index": 178, "docid": 1, "segid": 71, "prediction": "diese Ergebnisse auch Durchschnittswerte von zehn Fin tuning runs.", "reference": "Diese Ergebnisse sind auch Mittelwerte von zehn Durchläufen zur Feinabstimmung."}
{"index": 179, "docid": 1, "segid": 72, "prediction": "für die News-Kategorifizierung Wir finden gemischte Ergebnisse ,", "reference": "Bei der Aufgabe zur Kategorisierung der Nachrichten bekamen wir gemischte Ergebnisse."}
{"index": 180, "docid": 1, "segid": 73, "prediction": "vorherige War-Kontext-Klassifikationen für Kenia Rwanda haben ergeben, dass die simple Keyword-Detektion ist enough for solving this specific task, therefore", "reference": "Frühere Arbeiten zur Textklassifizierung für Kinyarwanda hatten herausgefunden, dass eine einfache Schlüsselworterkennung meistens ausreicht, um diese spezifische Aufgabe zu lösen."}
{"index": 181, "docid": 1, "segid": 74, "prediction": "there is Leise von using Pretraying Language Models.", "reference": "Daher ist die Verwendung von vortrainierten Sprachmodellen weniger erfolgsversprechend."}
{"index": 182, "docid": 1, "segid": 75, "prediction": "auf dieser besonderen Aufgabe der Kategoriisierung von Nachrichten.", "reference": "Jetzt zu dieser besonderen Aufgabe der Kategorisierung von Nachrichten."}
{"index": 183, "docid": 1, "segid": 76, "prediction": "Wir haben auch eine Ablationstheorie durchgeführt, um zu sehen, ob es alternative Strukturen gibt, die die Leistung verbessern.", "reference": "Wir haben auch eine Ablationsstudie durchgeführt, um zu sehen, ob es alternative Strukturen gibt, die die Leistung verbessern."}
{"index": 184, "docid": 1, "segid": 77, "prediction": "für die Group Benchmark finden wir, dass wir verwenden Affix-Sets. Konsequenz volles Performance besser, wenn wir die Probabilität der Erstellung der Bestleistung der Besten Performance der Besten Recognition. Oder so.", "reference": "Bei der GLUE-Benchmark haben wir festgestellt, dass die Verwendung von Affix-Sätzen durchweg besser abschneidet, während das Ziel der Affix-Wahrscheinlichkeitsregression die beste Leistung bei der Named Entity Recognition erbringt."}
{"index": 185, "docid": 1, "segid": 78, "prediction": "Durch das Aussehen der Lose Curves für die Feintuning finden wir, dass Knyabert eine in den meisten Fällen.", "reference": "Auch wenn man die niedrigen Werte bei der Feinabstimmung betrachtet, stellt man fest, dass KinyaBERT in den meisten Fällen eine bessere Konvergenz aufweist."}
{"index": 186, "docid": 1, "segid": 79, "prediction": "So zu dem Schluss, dass diese Arbeit die Wirksamkeit der expliziten Verwendung morphologischer Informationen in prä- trainierten Sprachmodellen demonstriert hat.", "reference": "Abschließend lässt sich sagen, dass diese Arbeit die Effektivität der expliziten Verwendung von morphologischen Informationen in vortrainierten Sprachmodellen bewiesen hat."}
{"index": 187, "docid": 1, "segid": 80, "prediction": "Ermöglicht die morphologische Erfassung von C. die morphologische Kompositionalität erfasst, was ein wichtiger Aspekt von morphologisch reichen Sprachen ist.", "reference": "Die vorgeschlagene zweistufige Transformer-Encoder-Architektur ermöglicht die Erfassung von morphologischer Komplexität und morphologischer Kompositionalität, die ein wichtiger Aspekt von morphologisch reichen Sprachen ist."}
{"index": 188, "docid": 1, "segid": 81, "prediction": "Diese Ergebnisse sollten weitere Forschungsinitiativen fördern. In Morphologie haben wir Language Pre-Traded Language Models.", "reference": "Diese Ergebnisse sollten zu weiteren Forschungen über morphologie-bewusste, vortrainierte Sprachmodelle motivieren."}
{"index": 189, "docid": 2, "segid": 0, "prediction": "Hallo, mein Name ist Miho Petrushka und es ist mir eine Freude, Ihnen das Papier mit dem Titel Sparsifying Transformers Models with Trainable Representation Pooling vorzustellen.", "reference": "Hallo, mein Name ist Michał Pietruszka und es ist mir eine Freude, Ihnen das Paper mit dem Titel Sparsifying Transformer Models with Trainable Representation Pooling vorzustellen."}
{"index": 190, "docid": 2, "segid": 1, "prediction": "Ich hab's nicht so gut geschafft.", "reference": "Die Arbeit wurde bei Applica KI in Zusammenarbeit mit Lukasz Borchmann und Lukasz Garncarek durchgeführt."}
{"index": 191, "docid": 2, "segid": 2, "prediction": "Lass mich mit den Problemen beginnen, unsere Arbeitsziele.", "reference": "Lassen Sie mich mit den Problemen beginnen, die in unserer Arbeit abgehandelt werden."}
{"index": 192, "docid": 2, "segid": 3, "prediction": "Unsere Methode funktioniert gut für Fälle, in denen lange Eingänge berücksichtigt werden.", "reference": "Unsere Methode funktioniert gut für die Fälle, in denen lange Eingaben berücksichtigt werden."}
{"index": 193, "docid": 2, "segid": 4, "prediction": "Es ist für das Quarter gedacht, es ein Input von über 2000 Token und die Ziele sind kürzer. Dann haben sie Inputs geliefert.", "reference": "Grob gesagt ist sie für Aufgaben und Eingaben von über zweitausend Token gedacht, wo die Zieltexte kürzer als die vorgegebenen Eingaben sind."}
{"index": 194, "docid": 2, "segid": 5, "prediction": "Das hat einige spezifische Anwendungen in NLP.", "reference": "Dies führt zu einigen spezifischen Anwendungen in der NLP."}
{"index": 195, "docid": 2, "segid": 6, "prediction": "Zum Beispiel kann man sich vorstellen, dass es aufgrund eines langen Dokuments eine Notwendigkeit gibt, das zusammenfassen und zu klassifizieren. die Frage, Extrak-Informationen. Ich habe ein paar Schlüsselphrasen.", "reference": "Man kann sich zum Beispiel vorstellen, dass ein langes Dokument zusammengefasst, klassifiziert und eine Frage darüber beantwortet werden muss und Informationen oder einige Schlüsselsätze extrahiert werden müssen."}
{"index": 196, "docid": 2, "segid": 7, "prediction": "Lass mich an den Vanilla-Transformer erinnern, der eine Frage der Aufmerksamkeit Komplexität hat, die vom Quadrat des Eingangsläufers abhängt.", "reference": "Erinnern wir uns an den Vanilla-Transformer und sein Problem mit der Aufmerksamkeitskomplexität, die vom Quadrat der Eingabezeile abhängt."}
{"index": 197, "docid": 2, "segid": 8, "prediction": "In der Vanilla-Transformer Mit voller Aufmerksamkeit auf die Konnektivität, die Beziehungen von jedem Token every other token have be calculated.", "reference": "Im Vanilla-Transformer müssen bei voller Aufmerksamkeitskonnektivität die Relationen jedes Token zu jedem anderen Token berechnet werden."}
{"index": 198, "docid": 2, "segid": 9, "prediction": "Die Komplexität der Aufmerksamkeit hängt von der Anzahl der Layer ab. Sequenzlänge an Eine weitere Sequenz. und die Dimensionalität von Repräsentationen.", "reference": "Die rechnerische Komplexität der Aufmerksamkeit hängt von der Reihe der Schichten l, der Sequenzlänge n, einer weiteren Sequenzlänge und der Dimensionalität der Repräsentationen ab."}
{"index": 199, "docid": 2, "segid": 10, "prediction": "In ähnlicher Weise scharft der Dekatur Aufmerksamkeit auf dieses Bild auf der rechten Seite. Der einzige Unterschied hier ist, dass die Target Tokens die Input Tokens betreffen. In diesem Fall.", "reference": "Ähnlich verhält es sich mit der Cross-Attention des Decoders zu diesem Bild auf der rechten Seite, wobei der einzige Unterschied darin besteht, dass die Ziel-Token in diesem Fall auf die Eingabe-Token gerichtet sind."}
{"index": 200, "docid": 2, "segid": 11, "prediction": "Das ist auch in dieser Formel zu sehen.", "reference": "Das sieht man in dieser Formel."}
{"index": 201, "docid": 2, "segid": 12, "prediction": "Die blaue Punktzahl stellt Beziehungen dar, die berechnet werden müssen.", "reference": "Der BLEU-Score stellt Relationen dar, die berechnet werden müssen."}
{"index": 202, "docid": 2, "segid": 13, "prediction": "Im Falle der vollen Aufmerksamkeit müssen wir alle Beziehungen innerhalb der Eingangssequenz berechnen.", "reference": "Im Falle der vollständigen Aufmerksamkeit müssen wir jede Relation innerhalb der Eingabe-Sequenz berechnen."}
{"index": 203, "docid": 2, "segid": 14, "prediction": "Jetzt sehen wir, was passiert, wenn wir einen Block-Wise-Enkoder haben, der durch die Begrenzung der Token-Konnektivität funktioniert. So dass sie nur andere Token in der Nähe sehen können.", "reference": "Jetzt sehen wir, was passiert, wenn wir einen blockweisen Encoder haben, der die Konnektivität der Token so einschränkt, dass sie nur andere Token in der Nähe sehen können."}
{"index": 204, "docid": 2, "segid": 15, "prediction": "Der Text wird in Stücken gelesen, was die Anzahl der Berechnungen auf der Encoder-Seite drastisch reduzieren kann. aber es verbessert nicht die Decoders. Cross Attention , da jeder Input-Token sowieso an den Decoder übergeben wird.", "reference": "Der Text wird in Blöcken gelesen, was die Reihe der Berechnungen auf der Encoder-Seite drastisch reduzieren kann. Aber die Cross-Attention des Decoders wird so nicht verbessert, da jedes Eingabe-Token ohnehin an den Decoder weitergegeben wird."}
{"index": 205, "docid": 2, "segid": 16, "prediction": "Diese Methode wird oft als Fusion Indikator bezeichnet.", "reference": "Diese Methode wird oft als Fusion im Decoder bezeichnet."}
{"index": 206, "docid": 2, "segid": 17, "prediction": "Die Verbesserung hier kann als Veränderung interpretiert werden. Einer der Abhängigkeiten von n zu einer anderen Konstante m. die Blockgröße repräsentiert.", "reference": "Die Verbesserung hier kann so interpretiert werden, dass eine der Abhängigkeiten von n durch eine andere Konstante m ersetzt wird, die die Blockgröße darstellt."}
{"index": 207, "docid": 2, "segid": 18, "prediction": "Unsere wichtigste Beobachtung ist, dass die meisten Tokens irrelevant sind. für eine Vielzahl von Aufgaben und kann fast vollständig vernachlässigt werden. Das ist auf der Schieberliste veranschaulicht.", "reference": "Unsere wichtigste Beobachtung ist, dass die meisten Token für eine Vielzahl von Aufgaben irrelevant sind und fast vollständig vernachlässigt werden können. Dies ist beispielhaft auf der Folie dargestellt."}
{"index": 208, "docid": 2, "segid": 19, "prediction": "wo nur Teile der Eingänge relevant sind. zu the desired output.", "reference": "Nur ein Teil der Eingaben ist für die gewünschte Ausgabe relevant."}
{"index": 209, "docid": 2, "segid": 20, "prediction": "zum Beispiel", "reference": "Zum Beispiel."}
{"index": 210, "docid": 2, "segid": 21, "prediction": "Man kann einen Artikel einmal lesen. die wichtigsten Teile mit einem High lighter zu markieren und dann eine Zusammenfassung basierend auf diesen Teilen aus der mittleren Stufe zu erstellen.", "reference": "Man kann einen Artikel einmal lesen, die wichtigsten Teile mit einem Textmarker markieren und dann eine Zusammenfassung erstellen, die nur auf diesem Teil aus der mittleren Phase basiert."}
{"index": 211, "docid": 2, "segid": 22, "prediction": "Die Kosten für die Auszeichnung und Entscheidung, ob die aktuellen Token für die Herstellung der Zusammenfassung unerlässlich sind, sind also billig und hängen nur von der Repräsentation der Token ab.", "reference": "Die Kosten für die Hervorhebung und die Entscheidung, ob das aktuelle Token für die Erstellung der Zusammenfassung wesentlich ist, sind somit gering und hängen nur von der Repräsentation des Token ab."}
{"index": 212, "docid": 2, "segid": 23, "prediction": "Das Pulling der hervorgehobenen Tokens ist möglich.", "reference": "Das Pooling der hervorgehobenen Token ist möglich."}
{"index": 213, "docid": 2, "segid": 24, "prediction": "Danke an unseren Top K Operator. Und die Kosten sind unerheblich.", "reference": "Das ist unserem Top-k-Operator zu verdanken und die Kosten sind vernachlässigbar."}
{"index": 214, "docid": 2, "segid": 25, "prediction": "Die Kosten für die Zusammenfassung aus einem kurzfristigen Input. Es ist auch viel niedriger als im Van ille-Modell, wenn der gesamte Eingang berücksichtigt wird.", "reference": "Die Kosten für die Erstellung einer Zusammenfassung aus einer gekürzten Eingabe sind ebenfalls viel niedriger als beim Vanilla-Modell, wenn die gesamte Eingabe berücksichtigt wird."}
{"index": 215, "docid": 2, "segid": 26, "prediction": "Aber hier ist die Frage:", "reference": "Aber hier stellt sich eine Frage."}
{"index": 216, "docid": 2, "segid": 27, "prediction": "wie man wichtige Tokens auswählt und wieder Gradienten zu dieser Selektion.", "reference": "Wie kann man wichtige Token auswählen und Gradienten zu dieser Auswahl zurückverfolgen?"}
{"index": 217, "docid": 2, "segid": 28, "prediction": "Das ist das Problem, das wir lösen, den trainierbaren Selektionsmechanismus vorzuschlagen.", "reference": "Das zugrundeliegende wesentliche Problem, das wir lösen, besteht darin, einen trainierbaren Auswahlmechanismus vorzuschlagen."}
{"index": 218, "docid": 2, "segid": 29, "prediction": "Einer, der es ermöglicht, dass Gradienten während des Trainings zurückverbreitet werden, so dass das Netzwerk lernen kann, die wichtigsten Token zu wählen.", "reference": "Dieser ermöglicht es, dass der Gradient während des Trainings rückverfolgt werden kann, sodass das Netzwerk lernen kann, die wichtigsten Token auszuwählen."}
{"index": 219, "docid": 2, "segid": 30, "prediction": "Genauer gesagt:", "reference": "Präziser ausgedrückt:"}
{"index": 220, "docid": 2, "segid": 31, "prediction": "Geben Sie mir einige Embeddings und Schnürsenker aus einer einfachen linearen Schicht gewonnen Die Aufgabe ist es, die höchsten Punkte zurückzugeben. Zuerst ist die Sequenz permutirt. und Paars werden so vorbereitet, dass der höhere Scoring-Vektor mit dem niedrigeren Scoring genommen wird.", "reference": "Angesichts einiger Unterwert-Einbettungen, die aus einer einfachen linearen Schicht stammen, besteht die Aufgabe darin, die Einbettungen mit dem höchsten Score zu ermitteln. Zunächst wird die Sequenz permutiert, und es werden Paare gebildet, sodass der höher bewertete Vektor mit dem niedriger bewerteten zusammengebracht wird."}
{"index": 221, "docid": 2, "segid": 32, "prediction": "Als nächstes werden die Gewichte mit Boosted Softmax über die Scores berechnet.", "reference": "Anschließend werden die Gewichtungen mithilfe von einer verstärkten Softmax über die Scores berechnet."}
{"index": 222, "docid": 2, "segid": 33, "prediction": "Nach jeder Turnierrunde Neue Vektoren und die Scores sind als eine lineare Kombination aus diesen Parts zusammengesetzt. Und da haben wir das gewählt.", "reference": "Nach jeder Turnierrunde werden neue Vektoren und Scores als lineare Kombination dieser Paare mit den erhaltenen Gewichtungen zusammengestellt."}
{"index": 223, "docid": 2, "segid": 34, "prediction": "Kurz gesagt, wir kombinieren sie linear. durch Performing Softmax Over The Das ist natürlich... Und", "reference": "Kurz gesagt kombinieren wir sie linear, indem wir eine Softmax über ihre Scores durchführen."}
{"index": 224, "docid": 2, "segid": 35, "prediction": "während wir kombinieren, Zwei Tokens, ein bisschen Noise. die man produzieren kann.", "reference": "Während man zwei Token kombiniert, kann auch schlechte Qualität erzeugt werden."}
{"index": 225, "docid": 2, "segid": 36, "prediction": "produziert, aber es erlaubt auch, die Gradienten zu propagieren, um alle Eingaben zu embonieren.", "reference": "Aber auch die Übertragung der Gradienten auf alle eingegebenen Einbettungen wird ermöglicht."}
{"index": 226, "docid": 2, "segid": 37, "prediction": "Kurz gesagt, ein trainierbares Wir haben einen Antrag basiert auf der Durchführung eines Turniers wie Soft Selection.", "reference": "Kurz gesagt wollen wir ein trainierbares Top-k vorschlagen, das eine turnierähnliche Soft-Selection bei jedem Schritt ausführt."}
{"index": 227, "docid": 2, "segid": 38, "prediction": "Und aus einer anderen Perspektive. Die Repräsentationspoling folgt der Encoder-Schicht.", "reference": "Aus einem anderen Blickwinkel betrachtet, folgt das Repräsentationspooling auf die Encoder-Schicht."}
{"index": 228, "docid": 2, "segid": 39, "prediction": "Zuerst wird jede Repräsentation bewertet und dann nur die mit den höchsten Punkten. zu den nächsten Layer.", "reference": "Zunächst wird jede Repräsentation bewertet. Dann werden nur jene mit den höchsten Scores an die nächste Ebene weitergegeben."}
{"index": 229, "docid": 2, "segid": 40, "prediction": "Die Kodierung kann wie in der Standard-Transformer-Architektur auf der vollständigen Eingabe durchgeführt werden.", "reference": "Die Kodierung kann wie in der Standard-Transformer-Architektur auf die volle Länge der Eingabe durchgeführt werden."}
{"index": 230, "docid": 2, "segid": 41, "prediction": "Es ist jedoch möglich, Texte in Blöcken von Pixel zu verarbeiten. fixed length and globally select the best representation.", "reference": "Es ist jedoch möglich, Text in Blöcken mit fester Länge zu verarbeiten und global die beste Repräsentation zu wählen."}
{"index": 231, "docid": 2, "segid": 42, "prediction": "Hier ist ein Beispiel für das das nach dem Encoder eingeführt wurde.", "reference": "Hier ist ein Beispiel für das nach dem Encoder eingeführte Repräsentationspooling."}
{"index": 232, "docid": 2, "segid": 43, "prediction": "Das beeinflusst direkt die Kosten für die enge Aufmerksamkeit, Nicht auf der Input-Link-End. Aber die Konstante \"K\". Ich repräsentiere das Pool Play.", "reference": "Dies hatte einen direkten Einfluss auf die Ursache der Cross-Attention. Diese hängt nicht von der Länge der Eingabe N ab, sondern von der Konstante K, welche die gepoolte Länge darstellt."}
{"index": 233, "docid": 2, "segid": 44, "prediction": "Das ist konstant informiert, wie viele Repräsentanten ausgewählt wurden. und dann an den Dekoder.", "reference": "Diese Konstante gibt an, wie viele Repräsentationen ausgewählt und an den Decoder übergeben werden."}
{"index": 234, "docid": 2, "segid": 45, "prediction": "Produzieren eine Zusammenfassung aus einem kürzeren Text ist &nbsp; bedeutend billiger als vorherige Lösung.", "reference": "Die Erstellung einer Zusammenfassung aus einem kürzeren Text ist wesentlich billiger als die frühere Lösung."}
{"index": 235, "docid": 2, "segid": 46, "prediction": "Da die Sequenzlänge durch einen großen Faktor verkürzt werden", "reference": "Die Sequenzlänge kann um einen großen Faktor verkürzt werden."}
{"index": 236, "docid": 2, "segid": 47, "prediction": "kann, Zum Beispiel haben wir erfolgreich K verwendet. oder sogar 60 Mal. oder sogar 64 mal kleiner als der Wert von n in unserem Experiment.", "reference": "Wir haben beispielsweise in unseren Experimenten k erfolgreich sechzehnmal oder sogar vierundsechzigmal kleiner als den Wert von n verwendet."}
{"index": 237, "docid": 2, "segid": 48, "prediction": "Bitte nicht. dass die positive Wirkung von Blockwise-Enkodierung und Selbstbetrachtung ist sustained.", "reference": "Bitte beachten Sie, dass die positive Wirkung von blockweiser Kodierung und selbständiger Aufmerksamkeit anhaltend ist."}
{"index": 238, "docid": 2, "segid": 49, "prediction": "Ich habe immer gedacht, dass die Rechnungskosten der Aufmerksamkeit vom Quadrat der Eingangslänge abhängen.", "reference": "Vergessen Sie nicht, dass die Rechenkosten der Aufmerksamkeit vom Quadrat der eingegebenen Länge abhängen."}
{"index": 239, "docid": 2, "segid": 50, "prediction": "Das ein Kind. Die früheren Eingänge während des Kodierungsprozesses können die Kosten deutlich senken.", "reference": "Die Reduzierung der Eingabe zu einem früheren Zeitpunkt während des Kodierungsprozesses kann die Kosten erheblich senken."}
{"index": 240, "docid": 2, "segid": 51, "prediction": "Für das pyramidische Modell schränken wir die Größe der Repräsentation auf der Ausgabe von jeder gewählten Schicht, was zu einer exponentiellen Reduktion die Kosten für die Berechnung, während die Kodierung voranschreitet.", "reference": "Für das Pyramidion-Modell haben wir die Größe der Repräsentation bei der Ausgabe jeder ausgewählten Schicht eingegrenzt, was zu einer exponentiellen Verringerung der Rechenkosten führt, wenn die Kodierung fortschreitet."}
{"index": 241, "docid": 2, "segid": 52, "prediction": "Wie Sie sehen können, die Gesamtrechnung skosten eines vollständigen Encoders sind weniger als doppelt so hoch wie die Kosten für die Vollgroß erste Schicht.", "reference": "Wie Sie sehen können, sind die gesamten Rechenkosten eines vollständigen Encoders hier weniger als doppelt so hoch als die Kosten der ersten Schicht in voller Größe."}
{"index": 242, "docid": 2, "segid": 53, "prediction": "Wenn das Pulling früher eingeführt wurde, Die Summe aller lila Quadrate ist also auf eine Konstante. nicht abhängig von der Anzahl der Layer L.", "reference": "Wenn das Pooling früher eingeführt wird, wird die Summe aller lila Quadrate auf eine Konstante begrenzt, die nicht von der Anzahl der Schichten abhängt."}
{"index": 243, "docid": 2, "segid": 54, "prediction": "Was die Konstanz zu sagen die durch die Platzierung der Pulling-Layer innerhalb des Netzwerks beeinflusst werden können.", "reference": "Die Konstante c kann durch die Anordnung der Pooling-Schichten innerhalb des Netzwerks beeinflusst werden."}
{"index": 244, "docid": 2, "segid": 55, "prediction": "Unsere Verbesserungen wurden auf achttausend Token langen verglichen.", "reference": "Unsere Verbesserungen wurden mit Eingaben in der Länge von achttausend Token verglichen."}
{"index": 245, "docid": 2, "segid": 56, "prediction": "Und die Figur zeigt, dass beim die beste Skalierbarkeit für die Netzwerkstiefe erreicht wird.", "reference": "Die Abbildung zeigt, dass die beste Skalierbarkeit für die Tiefe des Netzwerks erreicht wird, wenn das Pooling aktiviert ist."}
{"index": 246, "docid": 2, "segid": 57, "prediction": "hier kann man noten Das ist billiger , als einen zwei-Schicht-Vanille-Transformer mit so langem Eingang zu trainieren.", "reference": "Hier kann man feststellen, dass bei solchen langen Eingaben das Training des Pyramidions mit 24 Schichten billiger sein kann als das Training eines zweischichtigen Vanilla-Transformers."}
{"index": 247, "docid": 2, "segid": 58, "prediction": "Nicht zu erwähnen, wie leicht Vanilla Transformers Geh dem Gedächtnis für so einen langen Input.", "reference": "Ganz zu schweigen davon, wie schnell ein Vanilla-Transformer bei einer so langen Eingabe ungenügend Speicher haben kann."}
{"index": 248, "docid": 2, "segid": 59, "prediction": "Der **. Qualitative Vergleichung unserer Die andere Baseline ist die Perform on the long document summary task, die die Gesamtheit eines Artikels aus Archive oder Papmat erzeugt.", "reference": "Der qualitative Vergleich unseres Trend-Pyramidions mit anderen Baselines wird anhand der langen Aufgabe mit der Zusammenfassung des Dokuments durchgeführt. Alternativ kann der Hauptteil eines Artikels von arXiv oder PubMed herangezogen werden, wo die Aufgabe darin besteht, eine Zusammenfassung zu erstellen."}
{"index": 249, "docid": 2, "segid": 60, "prediction": "Wie man sehen kann. Blockwise, das ist unsere Baseline. performs state of the art models. Während die Pyramidion die Leistung dieser wettbewerbsfähigen Baseline beibehält oder verbessert,", "reference": "Man kann also sehen, dass der blockweise Ansatz, der unsere Baseline darstellt, auf modernsten Modellen basiert, während das Pyramidion die Leistung dieser konkurrenzfähigen Baseline beibehält oder verbessert."}
{"index": 250, "docid": 2, "segid": 61, "prediction": "Unser Modell ist 80. Per zent schneller zu trainieren und über 450 Prozent schneller bei der In ferenz im Vergleich zur Blockweite-Basislinie.", "reference": "Gleichzeitig ist unser Modell zu achtzig Prozent schneller zu trainieren und zu über vierhundertfünfzig Prozent schneller bei der Inferenz im Vergleich zur blockweisen Baseline."}
{"index": 251, "docid": 2, "segid": 62, "prediction": "Beide Modelle haben eine viel niedrigere Parameterzahl und wurden von Anfang an an den gewählten Aufgaben ausgebildet.", "reference": "Beide Modelle haben viel weniger Parameter und wurden von Grund auf für die ausgewählten Aufgaben trainiert."}
{"index": 252, "docid": 2, "segid": 63, "prediction": "Vorherige Ansätze um zu erreichen. Eine ähnliche Leistung musste mehr Parameter verwenden. Und die Leverage- Pretrained-Foundation, die Fundamentals. und zusätzliche Sprachen Training. Ziel: Ähnliche Leistung zu erzielen.", "reference": "Frühere Ansätze zur Erzielung einer ähnlichen Leistung mussten mehr Parameter verwenden sowie vortrainierte grundlegende Modelle und zusätzliche Vortrainingsziele bei der Sprache nutzen."}
{"index": 253, "docid": 2, "segid": 64, "prediction": "Wir laden Sie ein, unser vollständiges Papier zu lesen und unseren GitHub-Code zu verwenden.", "reference": "Wir laden Sie ein, unser vollständiges Paper zu lesen und unseren GitHub-Code zu verwenden."}
{"index": 254, "docid": 2, "segid": 65, "prediction": "Danke, dass du es gesehen hast.", "reference": "Vielen Dank fürs Zuschauen."}
{"index": 255, "docid": 3, "segid": 0, "prediction": "Hallo, das ist Yoweri Joe von der Harvard University.", "reference": "Hallo, ich bin Jiawei Zhou von der Harvard University."}
{"index": 256, "docid": 3, "segid": 1, "prediction": "Ich bin sehr froh, ihre Arbeit zur Online-Semantik-Parsing für Latenzreduktion in taskorientiertem Dialog vorzustellen.", "reference": "Ich freue mich sehr, unsere Arbeit zum semantischenOnline-Parsing für die Latenzreduktion bei einem aufgabenorientierten Dialog vorstellen zu können."}
{"index": 257, "docid": 3, "segid": 2, "prediction": "Das ist ein Joint Work mit Jason, Michael, Anthony und Sam von Microsoft Cementing Machines.", "reference": "Dies ist eine gemeinsame Arbeit mit Jason, Michael, Anthony und Sam von Microsoft Semantic Machines."}
{"index": 258, "docid": 3, "segid": 3, "prediction": "in task-oriented dialog Ein Benutzer interagiert mit einem System, das Anfragen von Benutzervertretern, normalerweise im Sprechen, bearbeitet.", "reference": "Beim aufgabenorientierten Dialog interagiert ein Benutzer mit dem System, das Anfragen von Benutzeräußerungen, in der Regel in gesprochener Form, bearbeitet."}
{"index": 259, "docid": 3, "segid": 4, "prediction": "von der Endung der Benutzeransage bis zur Systemantwort Es gibt oft unmerkliche Verzögerungen.", "reference": "Vom Ende der Benutzeräußerung bis zur Antwort des Systems gibt es oft eine spürbare Verzögerung."}
{"index": 260, "docid": 3, "segid": 5, "prediction": "Unter der Hood wird das Benutzer-Audience in ein ausführbares Programm übersetzt.", "reference": "Im Detail wird die Benutzeräußerung in ein ausführbares Programm übersetzt."}
{"index": 261, "docid": 3, "segid": 6, "prediction": "und dann ausgeführt , damit das System richtig reagieren kann.", "reference": "Dieses wird dann ausgeführt, damit das System richtig reagieren kann."}
{"index": 262, "docid": 3, "segid": 7, "prediction": "Hier ist das Programm als semant ischer Graph dargestellt, der die Berechnung skizziert. wo eine Note eine Funktion inruft und seine Kinder die Argumente sind", "reference": "Denn das Programm wird als semantischer Graph dargestellt, der die Berechnung skizziert, wobei ein Knoten einen Funktionsaufruf darstellt und seine Kindknoten die Argumente sind."}
{"index": 263, "docid": 3, "segid": 8, "prediction": "Die großen N odes markieren augenblickliche Operationen , aber die anderen sind langsam auszuführen.", "reference": "Die großen Knoten markieren sofortige Operationen, aber die anderen sind langsam in der Ausführung."}
{"index": 264, "docid": 3, "segid": 9, "prediction": "bemerken, dass, unlike the simple example here we show, Diese Programme können oft kompliziertere Graphen sein. Jenseits der Baumstrukturen.", "reference": "Das einfache Beispiel hier zeigt, dass diese Programme oft kompliziertere Graphen sein können als die Baumstrukturen."}
{"index": 265, "docid": 3, "segid": 10, "prediction": "in diesem talk wir stellten die frage können wir generieren das Programm und die Ausführung, bevor der Benutzer die Änderung fertigstellt, damit das System eine schnellere Reaktion erzielen kann.", "reference": "In diesem Vortrag stellen wir die Frage, ob wir mit der Generierung des Programms und seiner Ausführung beginnen können, bevor der Benutzer überhaupt die Äußerung beendet hat, sodass das System schneller reagieren kann."}
{"index": 266, "docid": 3, "segid": 11, "prediction": "Das ist eine Online- Vorhersage und Entscheidung Problem.", "reference": "Dies ist das Problem der Online-Vorhersage und Entscheidung."}
{"index": 267, "docid": 3, "segid": 12, "prediction": "Es gibt viele andere in diesem Raum.", "reference": "Es gibt viele andere in diesem REALM."}
{"index": 268, "docid": 3, "segid": 13, "prediction": "Beispiele hierfür sind die gleichzeitige Übersetzung Wo ein Live-Interpreter eine Sprache in in eine andere übersetzt. Smart Text -Autokompilierung, um die Benutzerabsicht zu ermitteln. und über den Pool, wo die Fahrer geschickt werden, wo sie gebraucht basierend auf der vorhergesagten Nachfrage.", "reference": "Beispiele sind Simultanübersetzungen, bei denen ein Dolmetscher live eine Sprache in eine andere in Echtzeit übersetzt; die intelligente automatische Vervollständigung von Text, um die Absicht des Benutzers zu erraten; und der Uber Pool, wo Fahrer dorthin geschickt werden, wo sie basierend auf der prognostizierten Nachfrage möglicherweise benötigt werden."}
{"index": 269, "docid": 3, "segid": 14, "prediction": "Alle diese Szen arien haben eine Gemeinsamkeit:", "reference": "All diese Szenarien haben eines gemeinsam."}
{"index": 270, "docid": 3, "segid": 15, "prediction": "Es ist vorteilhaft, Entscheidungen zu treffen, bevor man andere Einträge sieht.", "reference": "Es ist vorteilhaft, Entscheidungen zu treffen, bevor man alle Eingaben sieht."}
{"index": 271, "docid": 3, "segid": 16, "prediction": "In unserem Fall werden wir mit Online-Semantik parsen. was erwartet werden könnte, dass es schwierig ist, da wir wissen, was der Benutzer sagen könnte,", "reference": "In unserem Fall werden wir uns mit dem semantischen Online-Parsing befassen, was eine Herausforderung sein könnte, da wir erraten müssen, was der Benutzer sagen könnte."}
{"index": 272, "docid": 3, "segid": 17, "prediction": "es ist auch untersucht , ohne formelle Bewertungsmetrik.", "reference": "Dieses Gebiet ist auch wenig erforscht und hat keine formale Evaluationsmetrik."}
{"index": 273, "docid": 3, "segid": 18, "prediction": "Zuerst schauen wir uns an, wie ein normales System funktioniert.", "reference": "Schauen wir uns zunächst an, wie ein gewöhnliches System funktioniert."}
{"index": 274, "docid": 3, "segid": 19, "prediction": "Es ist offline betrieben, indem es nur am Ende der Benutzeranweisung zum Programm parst.", "reference": "Dieses funktioniert offline durch Parsing zum Programm erst am Ende der Benutzeräußerung."}
{"index": 275, "docid": 3, "segid": 20, "prediction": "Hier wird der Kriptograph vorhergesagt, nachdem er alle Informationen gesehen hat.", "reference": "Hier wird das Zeichen Graph vorhergesagt, nachdem alle Information gesehen wurden."}
{"index": 276, "docid": 3, "segid": 21, "prediction": "Im Gegensatz dazu schlagen wir ein Online-System vor, das bei jedem Alternativ-Präfix parsert.", "reference": "Im Gegensatz dazu schlagen wir ein Online-System vor, das bei jedem Äußerung das Präfix vergleicht."}
{"index": 277, "docid": 3, "segid": 22, "prediction": "Zum Jedes Mal, wenn wir einen neuen Token sehen, prognostizieren wir einen neuen Graph.", "reference": "Jedes Mal, wenn wir beispielsweise ein neues Token sehen, prognostizieren wir einen neuen Graphen."}
{"index": 278, "docid": 3, "segid": 23, "prediction": "nicht gesagt da könnte er", "reference": "Beachten Sie, dass Fehler auftreten können."}
{"index": 279, "docid": 3, "segid": 24, "prediction": "irres an der position von an der pool party mit barack obama Wir haben einen Grafik mit den rechten Noten. auf die Person und das Ereignis-Subjekt, aber das ist die falsche Zeitinformation.", "reference": "Bei der Position von der Poolparty mit Barack Obama haben wir einen Graphen mit den richtigen Knoten über die Person und das Thema des Ereignisses, aber wahrscheinlich die falschen Informationen zu den Zeiten."}
{"index": 280, "docid": 3, "segid": 25, "prediction": "Dieser Prozess geht weiter. bis wir die vollen Benutzeranweisungen erhalten haben.", "reference": "Dieser Prozess geht weiter, bis wir die vollständige Benutzeräußerung erhalten."}
{"index": 281, "docid": 3, "segid": 26, "prediction": "Wie würde sich das auswirken? Die Exekution ist in der Offline-Systeme Wir haben", "reference": "Wie würde sich dies auf die Ausführungszeiten im Offline-System auswirken?"}
{"index": 282, "docid": 3, "segid": 27, "prediction": "am Ende den Programmgrafen. So dass das System an diesem Punkt mit der Ausführung beginnen kann.", "reference": "Am Ende erhalten wir den Programm-Graphen, damit das System an dieser Stelle mit der Ausführung beginnen kann."}
{"index": 283, "docid": 3, "segid": 28, "prediction": "Denken Sie daran, dass die großen Knoten schnell funktionieren , also betrachten wir nur die Ausführungszeitlinie der farbigen langsamen Funktionen.", "reference": "Vergessen Sie nicht, dass die großen Knoten schnelle Operationen sind. Daher betrachten wir nur die Ausführungszeiten der farbigen langsamen Funktionen."}
{"index": 284, "docid": 3, "segid": 29, "prediction": "Er stens können diese zwei Personenfunktionen parallel ausgeführt werden. und sie haben keine Abhängigkeit von anderen Funktionen.", "reference": "Erstens können diese beiden Personensuchfunktionen parallel ausgeführt werden, da sie keine Abhängigkeit von anderen Funktionen haben. Sie sind weiß hinterlegt im rosa Kästchen."}
{"index": 285, "docid": 3, "segid": 30, "prediction": "Als nächstes kann das No-C rit-Event dann ausgeführt werden, nachdem Ergebnisse von niedrigeren Noten erhalten wurden, dann die Topfunktion yellt, so dass das ganze Programm fertig ist.", "reference": "Als Nächstes kann der Knoten „Ereignis erstellen“ ausgeführt werden, nachdem er Ergebnisse von Knoten bekommen hat. Mit der Top-Ertragsfunktion wird das gesamte Programm beendet."}
{"index": 286, "docid": 3, "segid": 31, "prediction": "Der Ausführungsprozess ist auf die Programmabhängigkeitsstruktur beschränkt. wo manche Operationen nicht parallelisiert werden können , was eine bemerkenswerte Verzögerung verursacht.", "reference": "Der Ausführungsprozess ist streng und beschränkt sich auf die Abhängigkeitsstruktur des Programms, bei der einige Operationen nicht parallelisiert werden können. Das führt zu einer spürbaren Verzögerung."}
{"index": 287, "docid": 3, "segid": 32, "prediction": "in unserem Online-System, wo wir vorhersagen, wie gehen. Die Ausführung des früher beginnen.", "reference": "In unserem Online-System, wo wir im Laufe der Zeit vorhersagen, kann die Programmausführung früher beginnen."}
{"index": 288, "docid": 3, "segid": 33, "prediction": "Hier ist das Präfix nach Obama. Wir können Sicherheit vorhersagen, dass die F inder-Person-Funktion in einem Programm sein sollte , aber der Rest kann Fehler enthalten, da sie ausgegraben sind.", "reference": "Hier, beim Präfix nach Obama, sagen wir zuversichtlich voraus, dass die Funktion „Person finden“ im Programm sein muss, aber dass der Rest Fehler enthalten kann, da sie ausgegraut sind."}
{"index": 289, "docid": 3, "segid": 34, "prediction": "Die Ausführung des Knoten punktes kann sofort in diesem Schritt gestartet werden.", "reference": "Die Ausführung des Knotens kann sofort als Schritt gestartet werden."}
{"index": 290, "docid": 3, "segid": 35, "prediction": "Dann , mit mehr Tok en, prognostizieren wir einen völlig neuen Graph , aber ein Teil davon wird bereits ausgeführt ,", "reference": "Dann, mit mehr Token, prognostizieren wir einen völlig neuen Graphen, aber ein Teil davon wird bereits ausgeführt."}
{"index": 291, "docid": 3, "segid": 36, "prediction": "also müssen wir nur den Rest der Nodes, von denen wir uns auch sicher sind,", "reference": "Wir müssen also nur den Rest der Knoten betrachten, bei denen wir uns auch sicher sind."}
{"index": 292, "docid": 3, "segid": 37, "prediction": "Hier kann eine andere gute Person parallel hingerichtet werden.", "reference": "Hier kann wieder „Finde Person“ parallel ausgeführt werden."}
{"index": 293, "docid": 3, "segid": 38, "prediction": "Wiederum, wir können falsche Vorhersagen haben.", "reference": "Auch hier könnten wir falsche Vorhersagen haben."}
{"index": 294, "docid": 3, "segid": 39, "prediction": "mit mehr Text. Wir haben mehr Fähigkeit, es richtig", "reference": "Mit mehr Text steigt die Chance, dass wir richtig liegen."}
{"index": 295, "docid": 3, "segid": 40, "prediction": "zu machen , wie z.B. die Ereigniszeit hier, wo am auch richtig erwartet wird.", "reference": "Zum Beispiel bei der Zeit des Ereignisses, bei der AM auch richtig vorhergesehen wird."}
{"index": 296, "docid": 3, "segid": 41, "prediction": "Dann können wir den Rest ausführen , nach der Programmabhängigkeitsstruktur.", "reference": "Dann können wir mit der Ausführung des Rests beginnen, indem wir der Abhängigkeitsstruktur des Programms folgen."}
{"index": 297, "docid": 3, "segid": 42, "prediction": "Durch die Überlappung der Ausführungszeitlinie mit der Zeitlinie der Änderungen sparen wir eine große Zeit.", "reference": "Indem wir die Ausführungszeiten mit den Zeiten der Äußerungen überlappen, sparen wir viel Zeit ein."}
{"index": 298, "docid": 3, "segid": 43, "prediction": "Wir haben also die Aufgabe vorgeschlagen, Online-Sementizparsing zu machen.", "reference": "Also haben wir die Aufgabe mit dem semantischen Online-Parsing vorgeschlagen."}
{"index": 299, "docid": 3, "segid": 44, "prediction": "Eine zugrunde liegende Annahme ist, dass die Ausführungszeit die Modell vorhersagezeit dominiert,", "reference": "Eine zugrunde liegende Annahme ist, dass die Ausführungszeit die Vorhersagezeit des Modells dominiert."}
{"index": 300, "docid": 3, "segid": 45, "prediction": "so dass wir nur Zeit gewinnen konnten, indem wir früher vorhersagen.", "reference": "Also konnten wir nur Zeit gewinnen, indem wir früher voraussagten."}
{"index": 301, "docid": 3, "segid": 46, "prediction": "Eine andere Annahme ist, dass die Vorhersage und die Ausführung den Hintergrund haben, der nicht sichtbar ist für Benutzer", "reference": "Eine weitere Annahme ist, dass, wenn die Vorhersage und die Ausführung im Hintergrund stattfinden, sie für Benutzer nicht sichtbar sind."}
{"index": 302, "docid": 3, "segid": 47, "prediction": "es ist nicht notwendig, eine Konsistenz zu halten Wir analysieren die Geschichte,", "reference": "Es ist nicht notwendig, eine konsistente Parsing-Historie aufzubewahren."}
{"index": 303, "docid": 3, "segid": 48, "prediction": "also analysieren wir nach jedem Token von Anfang bis Ende.", "reference": "Also parsen wir nach jedem Token von Grund auf neu."}
{"index": 304, "docid": 3, "segid": 49, "prediction": "In besonderem Fall schlagen wir einen Zwei-Stufen-Ansatz vor.", "reference": "Insbesondere wollen wir einen zweistufigen Ansatz vorschlagen."}
{"index": 305, "docid": 3, "segid": 50, "prediction": "ein vorgeschlagener Schritt, der einen Graphen mit vollständiger Struktur vorhersagt. Und ein Select- Schritt, der die Noten auswählt, die es wert sind, in ausgeführt zu werden.", "reference": "Wir schlagen einen Schritt vor, bei dem ein Graph mit vollständiger Struktur vorhergesagt wird und einen Schritt, bei dem die Knoten ausgewählt werden, die es zu diesem Zeitpunkt wert sind, ausgeführt zu werden."}
{"index": 306, "docid": 3, "segid": 51, "prediction": "Wir haben zwei Varianten der vorgeschlagenen Methode.", "reference": "Wir hatten zwei Varianten der vorgeschlagenen Methode."}
{"index": 307, "docid": 3, "segid": 52, "prediction": "Der erste Ansatz kombiniert eine Sprachmodellvollendung mit vollständiger Ausdrucksfähigkeit zur Graph-Parsing.", "reference": "Der erste Ansatz kombiniert eine Sprachmodell-Vervollständigung mit einer vollständigen Äußerung zum Graph-Parsing."}
{"index": 308, "docid": 3, "segid": 53, "prediction": "In der Regel ist das Präfix abt Obama erst durch ein Feintun-Bart-Sprachmodell abgeschlossen und dann in ein Programm mit vollem Offline-Parser übersetzt.", "reference": "Insbesondere wird das Präfix nach Obama zunächst durch ein fein abgestimmtes BART-Sprachmodell vervollständigt und dann in ein Programm mit vollständigem Offline-Parser übersetzt."}
{"index": 309, "docid": 3, "segid": 54, "prediction": "Der zweite Ansatz sagt das Programm direkt von YouTube aus voraus. Benutzer -Other-Prefix.", "reference": "Der zweite Ansatz sagt das Programm direkt aus den Präfixen der Benutzeräußerung voraus."}
{"index": 310, "docid": 3, "segid": 55, "prediction": "Das wird erreicht, indem ein einzelner Online-Parser angepasst wird, um den Gold graph von jedem Präfix zu übersetzen.", "reference": "Dies wird durch Training von einem einzigen Online-Parser erreicht, der aus jedem Präfix in den Zielgraphen übersetzen soll."}
{"index": 311, "docid": 3, "segid": 56, "prediction": "Dies erleichtert dem Modell, die richtige Ansicht zu lernen.", "reference": "Dies erleichtert dem Modell, die richtige Erwartung zu erlernen."}
{"index": 312, "docid": 3, "segid": 57, "prediction": "Ein detaillierter. Wie generieren wir diese Grafiken?", "reference": "Detaillierter gesagt: Wie können wir diese Graphen generieren?"}
{"index": 313, "docid": 3, "segid": 58, "prediction": "Wir formulieren das Problem, indem wir eine serielle Version des Graphen generieren.", "reference": "Wir formulieren das Problem, indem wir eine serielle Version des Graphen generieren."}
{"index": 314, "docid": 3, "segid": 59, "prediction": "Jede Note oder Kante wird durch eine Aktion dargestellt.", "reference": "Jeder Knoten oder jede Kante wird durch eine Aktion dargestellt."}
{"index": 315, "docid": 3, "segid": 60, "prediction": "Hier beginnen wir mit dem ersten Knoten.", "reference": "Hier beginnen wir mit dem ersten Knoten."}
{"index": 316, "docid": 3, "segid": 61, "prediction": "Die Nummer unten zeichnet die absolute Indexierung der Aktionsgeschichte auf.", "reference": "Die Reihe unten zeichnet den absoluten Index im Aktionsverlauf auf."}
{"index": 317, "docid": 3, "segid": 62, "prediction": "Dann haben wir die zweite Note.", "reference": "Dann haben wir den zweiten Knoten."}
{"index": 318, "docid": 3, "segid": 63, "prediction": "Als nächstes ist die Kante zwischen ihnen.", "reference": "Als Nächstes kommt die Kante zwischen ihnen."}
{"index": 319, "docid": 3, "segid": 64, "prediction": "Sie enthält den Zeiger zum Index der vorherigen Knoten. Und das Randlabel : Zero hier", "reference": "Dort ist der Zeiger auf den Index des früheren Knotens und das Kantenlabel enthalten."}
{"index": 320, "docid": 3, "segid": 65, "prediction": "bedeutet, die neueste Knotenstelle zu verbinden. mit einem Nullpunkt, der durch die Nullaktion erzeugt und Next Note, Next Edge.", "reference": "Null bedeutet hier, dass der neueste Knoten mit dem Knoten verbunden wird, der durch die Null-Aktion und die Kante des nächsten Knotens generiert wurde."}
{"index": 321, "docid": 3, "segid": 66, "prediction": "Dieser Prozess geht weiter , bis wir den vollen Graph generieren.", "reference": "Dieser Prozess geht weiter, bis wir den vollständigen Graph generieren."}
{"index": 322, "docid": 3, "segid": 67, "prediction": "Das zugrunde liegende Modell basiert auf einem Transformator mit einem selbstgesteuerten Mechanismus, ähnlich wie ein früherer übergangsbasierter Parser.", "reference": "Das zugrunde liegende Modell basiert auf dem Transformator mit Selbstausrichtungsmechanismus, ähnlich einem früheren übergangsbasierten Parser."}
{"index": 323, "docid": 3, "segid": 68, "prediction": "Nach der Erstellung eines vollständigen Graphen Wir haben die Wahrscheinlichkeiten der Aktionsstufe, die verschiedenen Teilen des Graphen entsprechen.", "reference": "Nach Generierung eines vollständigen Graphen haben wir die Wahrscheinlichkeiten der Aktionsebene erhalten, die verschiedenen Teilen des Graphen entsprechen."}
{"index": 324, "docid": 3, "segid": 69, "prediction": "mit leichten, zuversichtlichen Subgraphen basierend auf der Schwellenheuristik, die ausgeführt werden soll.", "reference": "Wir wählen Konfidenzteilgraphen auf der Grundlage der auszuführenden Schwellwerte heuristisch aus."}
{"index": 325, "docid": 3, "segid": 70, "prediction": "Später werden wir den Schwellenwert variieren, um unterschiedliche Kompromisse zwischen der Latenzreduktion und den Ausführungskosten zu erzielen.", "reference": "Später werden wir den Schwellenwert variieren, um unterschiedliche Kompromisse zwischen der Latenzreduzierung und den Ausführungskosten zu erzielen."}
{"index": 326, "docid": 3, "segid": 71, "prediction": "Eine formelle Bewertung der Online-Methoden Wir schlagen eine endgültige Latenzreduktion vor. Oh, FLR-Mänschchen.", "reference": "Für die formale Evaluation der Online-Methoden wollen wir eine endgültige Latenzreduzierung oder FLR-Metrik vorschlagen."}
{"index": 327, "docid": 3, "segid": 72, "prediction": "Hier ist ein Zusammenfassung , wie ein Offline-System die Ausführungszeitlinie beendet.", "reference": "Hier ist eine Zusammenfassung, wie ein Offline-System die Ausführungszeiten beendet."}
{"index": 328, "docid": 3, "segid": 73, "prediction": "in Online-Systemen Die überschneidet sich mit der Zeitlinie der Abwechslung, also endet sie früher.", "reference": "In Online-Systemen überschneidet sich die Ausführung mit den Zeiten der Äußerung. Sie endet also früher."}
{"index": 329, "docid": 3, "segid": 74, "prediction": "FLR ist definiert als die Reduzierungszeit im Vergleich zum Offline-System, die am Ende der Ausführung markiert wird.", "reference": "FLR ist als die Reduktionszeit im Vergleich zum Offline-System definiert und durch das Ende der Ausführung markiert."}
{"index": 330, "docid": 3, "segid": 75, "prediction": "Wir führen Experimente an zwei großen, konservierten, zementierten Parsing-Datensätzen durch. Kaflo und Ch. D.", "reference": "Wir führen Experimente an zwei großen Datensätzen von Konversationen mit semantischem Parsing durch: SMCalFlow und TreeDST."}
{"index": 331, "docid": 3, "segid": 76, "prediction": "S. T. graph-based parser. Wenn man offline arbeitet, kann neuesten Leistungen bei der Parsing erzielen. Auf beiden Datensätzen erreicht", "reference": "Unser auf dem Graphen basierte Parser erreicht, wenn er offline betrieben wird, beste Leistungen beim Parsing für beide Datensätze."}
{"index": 332, "docid": 3, "segid": 77, "prediction": "das Outline Complete Modell auch Nontrivial. bluegane vergleicht mit der einfachen bislinie der note completion Jetzt", "reference": "Das LM-Complete-Modell erzielt auch eine nicht triviale BLEU -Verstärkung im Vergleich zur einfachen Basislinie der Knotenvervollständigung."}
{"index": 333, "docid": 3, "segid": 78, "prediction": "schauen wir uns die Vorhersage Genauigkeit unseres Grafikparser-Präfixes an.", "reference": "Schauen wir uns nun die Vorhersagegenauigkeit unseres Präfixes für den Graph-Parser an."}
{"index": 334, "docid": 3, "segid": 79, "prediction": "Wir testen die Übereinstimmung von Grafen zwei Pools zwischen der Generation und der Gergraph-Invalidationsdaten. in Y-Achsen , für jede Präfixlänge in X-Achsen , repräsentiert durch Prozentsätze.", "reference": "Wir testen den F1-Score der Graph-Tupel zwischen der Generierung und dem Go-Graph in den Validierungsdaten auf der y-Achse für jede Präfixlänge in der x-Achse, dargestellt durch Prozentsätze."}
{"index": 335, "docid": 3, "segid": 80, "prediction": "Jede dieser Kurven repräsentiert ein anderes Modell. mit dem Unterschied in den Trainingsdaten.", "reference": "Jede dieser Kurven stellt ein anderes Modell mit dem einzigen Unterschied in den Trainingsdaten dar."}
{"index": 336, "docid": 3, "segid": 81, "prediction": "Die Bottom Curve ist der Offline-Parser. und wir mischen in den unterschiedliche Linien. um das Modell zu einem Online-Parser zu transformieren.", "reference": "Die untere Kurve ist der Offline-Parser. Wir mischen Präfix- Daten in verschiedenen Längen hinein, um das Modell in einen Online-Parser zu überführen."}
{"index": 337, "docid": 3, "segid": 82, "prediction": "Zum Beispiel das Legend-Präfix 80 % plus. Das bedeutet, dass das Modell mit den Präfixdaten gekennzeichnet ist. einer Länge von Perfekten, die größer ist als 80 % der Länge der vollständigen Ausdrucksform.", "reference": "Zum Beispiel bedeutet das Legendenpräfix „80 Prozent plus“, dass das Modell mit Präfix-Daten trainiert wird, wobei die Präfixlänge größer als 80 Prozent der vollen Äußerungslänge ist."}
{"index": 338, "docid": 3, "segid": 83, "prediction": "Die obere linke Ecke ist ein gewünschter Bereich.", "reference": "Die obere linke Ecke ist der gewünschte Bereich."}
{"index": 339, "docid": 3, "segid": 84, "prediction": "wie wir sehen können , der Offline-Parser in Black Curve ist nicht gut auf den Präfixdaten,", "reference": "Wie wir sehen können, funktioniert der Offline-Parser in der schwarzen Kurve der Präfix-Daten nicht gut."}
{"index": 340, "docid": 3, "segid": 85, "prediction": "als wir mehr Präfixe in mischen, die Kurve ist liftend oben und links , besser auf alle die Präfixlänge.", "reference": "Da wir im Training mehr Präfixe mischen, hebt sich die Kurve nach oben und links und schneidet bei allen Präfixlängen besser ab."}
{"index": 341, "docid": 3, "segid": 86, "prediction": "Allerdings die voll ordnungsgemäße Parsingleistung in der oberen rechten Punkt nicht beeinflusst.", "reference": "Die volle Leistung des Äußerung-Parsings wird jedoch im oberen rechten Punkt nicht beeinflusst."}
{"index": 342, "docid": 3, "segid": 87, "prediction": "basierend auf diesen starken Ergebnissen Wie viel Latenz reduzieren wir?", "reference": "Basierend auf diesen starken Ergebnissen, stellt sich die Frage, wie viel Latenz reduziert wird."}
{"index": 343, "docid": 3, "segid": 88, "prediction": "Wir messen die Zeit an der Anzahl der Quelltokens und simulieren verschiedene Funktionsausführungszeiten.", "reference": "Wir messen die Zeit anhand der Reihe von Quelltoken und simulieren verschiedene Funktionsausführungszeiten."}
{"index": 344, "docid": 3, "segid": 89, "prediction": "Die Kur ven zeigen den Kompromiss zwischen der FLR-Metrik und den Ausführungskosten, gemessen an der Anzahl der übermäßigen Funktionskosten, die nicht korrekt sind.", "reference": "Die Kurven zeigen den Kompromiss zwischen der FLR-Metrik und den Ausführungskosten, gemessen an der Reihe übermäßiger Funktionskosten, die nicht korrekt sind."}
{"index": 345, "docid": 3, "segid": 90, "prediction": "Das wird erreicht, indem man die Subgraph-Selektionsschwelle variiert.", "reference": "Dies wird durch Variation der Teilgraphenauswahlschwelle erreicht."}
{"index": 346, "docid": 3, "segid": 91, "prediction": "Eine höhere Schwelle wählt weniger Funktionen und aus , aber erhält eine kleinere FLR. Während die untere Schwelle aggressiver Programme auswählt und ausführt.", "reference": "Eine höhere Schwelle wählt weniger Fehlfunktionen aus, erhält aber eine kleinere FLR, während die niedrigere Schwelle aggressiver Programme auswählt und ausführt."}
{"index": 347, "docid": 3, "segid": 92, "prediction": "Wir vergleichen die beiden Ansätze, die wir vorschlagen, in der Basislinie. Das tut nichts, als direkt den Offline-Parser anzuwenden. für den Online-Gebrauch.", "reference": "Wir vergleichen die beiden Ansätze, die wir vorschlagen, mit einer Baseline, die nichts anderes tut, als den Offline-Parser für den Online-Einsatz anzuwenden."}
{"index": 348, "docid": 3, "segid": 93, "prediction": "Regen ist das beste FLR und kostet Trade-off.", "reference": "Der obere linke Bereich hat den besten FLR und Kostenvorteil."}
{"index": 349, "docid": 3, "segid": 94, "prediction": "Wir sehen , dass beide Methoden die Basislinie mit großer Marge schlagen und dass sie auf GDST ähnlicher funktionieren.", "reference": "Wir sehen, dass beide unserer Methoden die Baseline um eine große Differenz übertreffen und sie bei TreeDST ähnlicher abschneiden."}
{"index": 350, "docid": 3, "segid": 95, "prediction": "Wenn die einzelne Funktion schneller ausgeführt wird, gibt es tendenziell mehr Run-Ausführungen und weniger Latenzreduktionsraum.", "reference": "Während die Ausführung einzelner Funktionen schneller ist, gibt es tendenziell mehr Ausführungsvorgänge und weniger Raum für die Latenzreduzierung."}
{"index": 351, "docid": 3, "segid": 96, "prediction": "wenn die Ausführung der einzelnen Funktionen langsamer ist Es gibt mehr Raum für FLR-Bevorderung.", "reference": "Wenn die Ausführung einzelner Funktionen langsamer ist, gibt es mehr Möglichkeiten für FLR-Verbesserungen."}
{"index": 352, "docid": 3, "segid": 97, "prediction": "Unsere beiden Ansätze erzielen bessere Leistungen in verschiedenen Kostregionen.", "reference": "Unsere beiden Ansätze erreichen eine bessere Leistung in verschiedenen Kostenbereichen."}
{"index": 353, "docid": 3, "segid": 98, "prediction": "Insgesamt haben wir 30 bis 63 % relative Latenzreduktion erreicht. abhängig von der Ausführungszeit. und erlaubt Kosten.", "reference": "Insgesamt erreichen wir je nach Ausführungszeit und zulässigen Kosten eine Reduzierung der relativen Latenz um 63 bis 60 Prozent."}
{"index": 354, "docid": 3, "segid": 99, "prediction": "Schließlich haben wir einen Abbruch der durchschnittlichen Latenz reduktion in Tokens für jede Art der Funktionsknoten. Die erlaubte Ursache sind drei falsche Hinrichtungen.", "reference": "Schließlich haben wir eine Aufschlüsselung der durchschnittlichen Latenzreduktion in Token für jeden Typ des Funktionsknotens, wenn die zulässigen Kosten drei Ausführungsvorgänge betragen."}
{"index": 355, "docid": 3, "segid": 100, "prediction": "wie wir sehen können, sind sie gegen alle der borde", "reference": "Wie wir sehen können, gibt es in jedem Bereich positive Ergebnisse."}
{"index": 356, "docid": 3, "segid": 101, "prediction": "Es gibt auch einige Funktionen, auf denen wir gewinnen. Impress ive Latenzreduktion, wo die rote Stange viel länger ist , wie z.B. bei Manager und Empfänger.", "reference": "Es gibt auch einige Funktionen, bei denen wir eine beeindruckende Latenzreduzierung erzielen und der rote Balken viel länger ist, wie z. B. Find Manager und Empfänger."}
{"index": 357, "docid": 3, "segid": 102, "prediction": "Das sind Low-Level-Funktionen , die nicht viel Abhängigkeit haben. und andere.", "reference": "Dies sind Low-Level-Funktionen, die keine große Abhängigkeit von anderen haben."}
{"index": 358, "docid": 3, "segid": 103, "prediction": "In der Schlussfolgerung Wir schlagen Online- Sem ester-Parsing als neue Aufgabe vor, die mit einer strengen Latenzreduktionsmetrik erforscht werden muss.", "reference": "Abschließend haben wir das semantische Online-Parsing als neue Aufgabe vorgeschlagen, das wir mit der strengen Latenzreduktionsmetrik untersuchen."}
{"index": 359, "docid": 3, "segid": 104, "prediction": "mit einem starken grafischen Zementparser Wir haben eine relativ gute Latenzreduktion erreicht. entweder durch eine Pipeline -Ansatz mit L-Line-Erfüllung und einem vollen Parser oder direkt durch einen gelernten Parser auf den Präfixen.", "reference": "Mit einem starken graph-basierten, semantischen Parser erreichen wir eine relativ gute Latenzreduktion, entweder durch unseren Pipeline-Ansatz mit LM-Abschluss und einem vollständigen Parser, oder direkt durch einen gelernten Parser mit Fokus auf die Präfixe."}
{"index": 360, "docid": 3, "segid": 105, "prediction": "Unser Ansatz kann ein allgemeiner Rahmen sein und auf andere ausführbare semantische Repräsentationen in verschiedenen Bereichen angewendet werden.", "reference": "Darüber hinaus kann unser Ansatz ein allgemeiner Rahmen sein und auf andere ausführbare semantische Darstellungen in verschiedenen Domänen angewendet werden."}
{"index": 361, "docid": 3, "segid": 106, "prediction": "Zukunftswerke könnten eine schlauer Vorhersage- und Ausführungsintegrationsmethode erforschen.", "reference": "Zukünftige Arbeiten könnten eine intelligentere Vorhersage und die Methode der Ausführungsintegration erforschen."}
{"index": 362, "docid": 3, "segid": 107, "prediction": "Danke für das Hören.", "reference": "Danke fürs Zuhören."}
{"index": 363, "docid": 4, "segid": 0, "prediction": "Hallo,", "reference": "Hi."}
{"index": 364, "docid": 4, "segid": 1, "prediction": "ich werde unsere Arbeit über die Erzeugung von Retrieval-Augmented- Kontrar-Factuals für Fragen-Antwort-Aufgaben diskutieren.", "reference": "Ich werde nun unsere Arbeit am generierenden Retrieval und den erweiterten Kontrafakten für Aufgaben der Fragenbeantwortung vorstellen."}
{"index": 365, "docid": 4, "segid": 2, "prediction": "Das ist eine Arbeit, die ich während meines Praktikums bei Google Research gemacht habe, wo ich von Matthew Lam und Ian Tenney betreut wurde.", "reference": "Dies ist die Arbeit, die ich während meines Praktikums bei Google Research gemacht habe, wo ich von Matthew Lamm und Ian Tenney betreut wurde."}
{"index": 366, "docid": 4, "segid": 3, "prediction": "Um die Aufgabe zu motivieren, las se mich mit der Definition eines Vertrags beginnen.", "reference": "Um die Aufgabe vorzustellen, möchte ich damit beginnen, das Wort kontrafaktisch zu definieren."}
{"index": 367, "docid": 4, "segid": 4, "prediction": "In diesem Werk definieren wir ein Counterfactual als eine Pertribution des Inputs. Text, der sich in irgendeiner bedeutungsvollen, kontrollierten Weise von dem Originaltext unterscheidet", "reference": "In dieser Arbeit definieren wir kontrafaktisch als eine Störung des eingegebenen Textes, der sich in irgendeiner bedeutungsvollen kontrollierten Weise vom ursprünglichen Text unterscheidet."}
{"index": 368, "docid": 4, "segid": 5, "prediction": "und uns erlaubt, über die Veränderungen im Ausgang oder das Tasklabel zu reden.", "reference": "Damit können wir über die Änderungen des Ergebnisses oder des Labels der Aufgabe schlussfolgern."}
{"index": 369, "docid": 4, "segid": 6, "prediction": "Zum wenn man die Worte faszinierend zu faszinierend ändert. Und das expected to mind ändert das Gefühl diese Filmrevision.", "reference": "Wenn man beispielsweise die Wörter „faszinierend“ zu „fesselnd“ oder „erwartet“ zu „todlangweilig“ ändert, ändert das die Stimmung der Filmrezension."}
{"index": 370, "docid": 4, "segid": 7, "prediction": "Ähnlich er den Qualifikator Womens zu der Frage hinzu. Ändert die Antwort auf die Frage im Beispiel unten.", "reference": "Wird die nähere Bestimmung „Damen“ zur Frage hinzugefügt, ändert sich die Antwort auf die Frage wie im Beispiel unten dargestellt."}
{"index": 371, "docid": 4, "segid": 8, "prediction": "Menschen sind im Vergleich zu NLP-Modellen, die auf der Aufgabe ausgebildet sind, typischerweise robust gegen solche Störungen.", "reference": "Menschen sind in der Regel robust gegenüber solchen Störungen im Vergleich zu NLP-Modellen, die für die Aufgabe trainiert wurden."}
{"index": 372, "docid": 4, "segid": 9, "prediction": "Warum ist das ?", "reference": "Warum?"}
{"index": 373, "docid": 4, "segid": 10, "prediction": "Der Datensatz kann mit Systemax samplt werden. Das ist eine schlichtere Entscheidung, die von Gegenwirkung durch die Grenzen durchführt wird.", "reference": "Der Datensatz kann mit systematischen Bias gesampelt werden. Das führt zu einer einfachen Entscheidungsgrenze, die kontrafaktisch übertreten wird."}
{"index": 374, "docid": 4, "segid": 11, "prediction": "wie in diesem Studiendeklassifikationsproblem gezeigt.", "reference": "Das zeigt sich in diesem 2D-Klassifizierungsproblem."}
{"index": 375, "docid": 4, "segid": 12, "prediction": "Priorwerk hat festgestellt, dass das Hinzufügen von Konterfaktusbeispielen zu den Tradingdaten kann das Modell robust zu solchen Perdurationen machen.", "reference": "Mit meiner Arbeit habe ich herausgefunden, dass das Hinzufügen von kontrafaktischen Beispielen zu den Trainingsdaten das Modell robust gegen solche Störungen machen kann."}
{"index": 376, "docid": 4, "segid": 13, "prediction": "Also, wenn Fälschungsregeln wertvoll sind. Wie können wir sie generieren?", "reference": "Wenn also Kontrafakten wertvoll sind, wie können wir sie dann generieren?"}
{"index": 377, "docid": 4, "segid": 14, "prediction": "Diese Aufgabe ist besonders schwer für NLP. Weil hier sind drei Beispiele aus drei verschiedenen NLP-Aufgaben.", "reference": "Diese Aufgabe ist besonders schwierig für NLP, denn hier sind drei Beispiele aus drei verschiedenen NLP-Aufgaben."}
{"index": 378, "docid": 4, "segid": 15, "prediction": "Wie Sie sehen können, gibt es Beispiele, die die Entscheidungsschwelle verletzen. Zwischen den Ergebnissen muss man sehr sorgfältig gearbeitet werden, indem man einige Attribute des Textes, die hier unterstrichen sind, perterpiniert.", "reference": "Wie Sie sehen können, müssen Beispiele, die die Entscheidungsgrenze zwischen den Ergebnissen verletzen, sehr sorgfältig erstellt werden, indem einige Attribute des Textes, die hier unterstrichen werden, gestört werden."}
{"index": 379, "docid": 4, "segid": 16, "prediction": "Das könnte man mit menschlicher Anmerkung machen, aber das ist teuer und voreingenommen.", "reference": "Dies könnte durch menschliche Annotation geschehen, aber dies ist teuer und voreingenommen."}
{"index": 380, "docid": 4, "segid": 17, "prediction": "Einige frühere Arbeiten konzentrierten sich auf die Verwendung von Syntax bäumen oder der semantischen Rollenlabelung.", "reference": "Einige frühere Arbeiten konzentrierten sich auf die Verwendung von Syntax-Bäumen oder einer semantischen Rollenbezeichnung."}
{"index": 381, "docid": 4, "segid": 18, "prediction": "Aber die Menge der durch diese Technik erzeugten Störungen ist durch den semantischen Rahmen begrenzt.", "reference": "Aber die Reihe von Störungen, die durch diese Techniken generiert werden, sind durch den semantischen Rahmen begrenzt."}
{"index": 382, "docid": 4, "segid": 19, "prediction": "In jüngeren Arbeiten wurden Massensprachmodelle verwendet, um Füllen Masken-Partsionen des Textes aus. zu", "reference": "Neuere Arbeiten haben maskierte Sprachmodelle verwendet, um maskierte Teile des Textes auszufüllen, um Labels zu ändern."}
{"index": 383, "docid": 4, "segid": 20, "prediction": "finden, welche Teile des Textes zu stören sind.", "reference": "Aber herauszufinden, welche Teile des Textes zu stören sind, kann eine Herausforderung sein."}
{"index": 384, "docid": 4, "segid": 21, "prediction": "Es gibt mehr Challenges zu Generating Contracts für Fragen beantworten, speziell.", "reference": "Es gibt mehr Herausforderungen für die Generierung von Kontrafakten als für die spezifische Beantwortung der Frage."}
{"index": 385, "docid": 4, "segid": 22, "prediction": "Diese Aufgabe erfordert Hintergrundwissen.", "reference": "Diese Aufgabe erfordert Hintergrundwissen."}
{"index": 386, "docid": 4, "segid": 23, "prediction": "Zum Beispiel, um die ursprüngliche Frage zu stellen : Ist Indiana Jones Temple of Doom ein Prequel?", "reference": "Um beispielsweise die ursprüngliche Frage zu stören: „Ist Indiana Jones und der Tempel des Todes ein Prequel?“"}
{"index": 387, "docid": 4, "segid": 24, "prediction": "Wir müssen uns der anderen Filme in der Franchise bewusst sein. um eine Frage zu bekommen , wie ist Indiana Jones Raiders of the Lost Ark?", "reference": "Wir müssen die anderen Filme im Franchise kennen, um die Frage stellen zu können: „Ist Indiana Jones – Jäger des verlorenen Schatzes ein Prequel?“"}
{"index": 388, "docid": 4, "segid": 25, "prediction": "Darüber hinaus können zufällige Störungen zu Fragen führen, die mit den verfügbaren Beweisen nicht beantwortet werden können. oder falsche Prämissen haben.", "reference": "Darüber hinaus können zufällige Störungen zu Fragen führen, die mit den verfügbaren Beweisen nicht beantwortet werden können oder falsche Voraussetzungen haben."}
{"index": 389, "docid": 4, "segid": 26, "prediction": "Darüber hinaus können einige Fragen perturbationen zu einer signifikanten semantischen Drift vom ursprünglichen Input führen.", "reference": "Darüber hinaus können einige Frage-Störungen zu einer signifikanten semantischen Abweichung von der ursprünglichen Eingabe führen."}
{"index": 390, "docid": 4, "segid": 27, "prediction": "Zum Beispiel ist diese Frage indiana jones praktizierender kindesklaver in temple of doom", "reference": "Zum Beispiel ist diese Frage hier: „Praktiziert Indiana Jones Kindersklaverei im Tempel des Todes?“"}
{"index": 391, "docid": 4, "segid": 28, "prediction": "Wir schlagen eine sehr einfache und dennoch effektive Technik vor , die Retrieve-Generate-Filter oder RGF genannt wird. um gegen wirkende Perturbationen von Fragen zu bekämpfen und auch alle anderen erwähnten Herausforderungen zu bekämpfen.", "reference": "Wir wollen eine sehr einfache, aber effektive Technik mit dem Namen „Retrieve Generate Filter“ oder RGF vorschlagen, um kontrafaktische Störungen von Fragen in Angriff zu nehmen. Sie zielt auch darauf ab, alle anderen oben genannten Herausforderungen zu bewältigen."}
{"index": 392, "docid": 4, "segid": 29, "prediction": "Die Kernintuition hinter RTF ist , dass die notwendige Hintergrundinformation, die benötigt wird, um Störungen zu erzeugen, in den nahen Missen des Fragestellmodells vorhanden sein kann.", "reference": "Die Kernintuition hinter RGF ist, dass die notwendigen Hintergrundinformationen, die erforderlich sind, um Störungen zu genieren, in den Near-Misses vorhanden sein können, die von einem Frage-Antwort-Modell erstellt werden."}
{"index": 393, "docid": 4, "segid": 30, "prediction": "Zum Beispiel produziert das State-of-the-Art-Modell Realm die folgenden Top-K-Antworten auf die Frage, wer der Kapitän des Richmond Football Clubs ist.", "reference": "Zum Beispiel liefert das hochmoderne Modell REALM die folgenden Top-k-Antworten auf die Frage, wer der Kapitän des Richmond Football Club ist."}
{"index": 394, "docid": 4, "segid": 31, "prediction": "Nun, es hat den ursprünglichen Referenzpassage -Anwalt Trent Kotkin als Topmost-Choice", "reference": "Es holt die ursprüngliche Referenzpassage und die Antwort „Trent Cotchin“ als erste Wahl ein."}
{"index": 395, "docid": 4, "segid": 32, "prediction": "zurückgewonnen, es hat auch zusätzliche Passagen und Antworten zurückgewonnen, die verwendet werden können, um For instance,", "reference": "Zusätzlich werden auch zusätzliche Passagen und Antworten abgerufen, die verwendet werden können, um Störungen der Frage zu steuern."}
{"index": 396, "docid": 4, "segid": 33, "prediction": "es enthält zwei weitere Antworten, die den Kapitänen der Reserve- und der Frauenmannschaft des gleichen Clubs entsprechen, das kann zu interessanten Edits führen.", "reference": "Zum Beispiel holt es zwei weitere Antworten ein, passend zu den Kapitänen der Reservemannschaft und der Damenmannschaft des gleichen Vereins. Dies kann zu interessanten Änderungen führen."}
{"index": 397, "docid": 4, "segid": 34, "prediction": "Zusammenfassend: RGF erstmals holt Top-K-relevanteste Antworten und Kontexte ab, die nicht mit der Referenzantwort und dem Kontext übereinstimmen.", "reference": "Zusammenfassend lässt sich sagen, dass RGF zuerst die wichtigsten Top-k-Antworten und Kontexte abruft, die nicht mit der Referenz Antwort in Kontext übereinstimmen."}
{"index": 398, "docid": 4, "segid": 35, "prediction": "Nach diesem Schritt , dem werden die Bedingungen für diese alternativen Antworten, eine Frage zu generieren,", "reference": "Im Anschluss an diesen Schritt konditioniert dieses Fragengenerierungsmodell diese alternativen Antworten, um eine ihnen entsprechende Frage zu generieren."}
{"index": 399, "docid": 4, "segid": 36, "prediction": "Und schließlich können wir die generierten Fragen filtern, die auf Minimalität basieren oder auf der Art der semantischen Pertribution basieren, die wir einführen möchten.", "reference": "Schließlich können wir die generierten Fragen nach Minimalität oder nach der Art der semantischen Störung, die wir einführen möchten, filtern."}
{"index": 400, "docid": 4, "segid": 37, "prediction": "Über jeden Schritt geht man im Detail. Für das Retrieval verwenden wir ein Retrieve- und Read-Modell wie Realm, das die Originalfrage als Input und ein großes Korpus wie Wikipedia.", "reference": "Wenn wir beim Retrieval jeden Schritt genauer durchgehen, dann sehen wir, dass wir einen Abruf verwenden. Dann lesen wir ein Modell wie REALM, das als Eingabe die ursprüngliche Frage und einen großen Korpus wie etwa Wikipedia hernimmt."}
{"index": 401, "docid": 4, "segid": 38, "prediction": "Es besteht aus zwei Modulen.", "reference": "Es besteht aus zwei Modulen."}
{"index": 402, "docid": 4, "segid": 39, "prediction": "Das Retriever-Modul führt eine Ähnlichkeitsüberprüfung über einen dichten Index von Passagen durch, um die topk-relevantesten Passagen der Frage zu finden,", "reference": "Das Retrieval-Modul führt eine Ähnlichkeitssuche über einen dichten Index von Passagen durch, um die wichtigsten Top-k-Passagen zur Frage abzurufen."}
{"index": 403, "docid": 4, "segid": 40, "prediction": "Reader-Modul, das Extrakt aus jedem Passage. als eine potenzielle Antwort.", "reference": "Das Lesemodul extrahiert dann aus jeder Passage einen Bereich als potenzielle Antwort."}
{"index": 404, "docid": 4, "segid": 41, "prediction": "Realm retrieves the gold. Passage and Answer in most cases.", "reference": "REALM ruft die Goldpassage und in den meisten Fällen die Antwort ab."}
{"index": 405, "docid": 4, "segid": 42, "prediction": "In diesem Werk sind wir jedoch mehr interessiert in die Antworten und Kontexte, die es weiter hinunter die Linie retrieves.", "reference": "In dieser Arbeit sind wir jedoch mehr an den Antworten und am Kontext interessiert, der später abgerufen wird."}
{"index": 406, "docid": 4, "segid": 43, "prediction": "Im nächsten Schritt Wir verwenden diese alternativen Antworten und Kontexte, um neue Fragen zu erzeugen , die diesen Alternativen entsprechen.", "reference": "Im nächsten Schritt der Fragengenerierung verwenden wir diese alternativen Antworten und Kontexte, um neue Fragen zu generieren, die diesen Alternativen entsprechen."}
{"index": 407, "docid": 4, "segid": 44, "prediction": "Das das Modell der Generation Question. Pre-Trained Text-to-Text-Transformer, das ist auf die NQ-Daten ausgelegt. um eine Frage für eine Antwort zu generieren, die im Kontext markiert ist.", "reference": "Das Modell der Fragengenerierung ist ein vortrainierter Text-to-Text-Transformer, der auf die NQ-Daten abgestimmt ist, um eine Frage für eine Antwort zu generieren, die für den Kontext markiert ist."}
{"index": 408, "docid": 4, "segid": 45, "prediction": "Während der Inferenz liefern wir das Fragestellungsmodell, die alternative Antwort und Kontexte, die wir in der vorherigen Folge zurückgewonnen haben.", "reference": "Während der Interferenz liefern wir das Fragengenerierungsmodell, die alternative Antwort und den Kontext, die wir im früheren Schritt abgerufen haben."}
{"index": 409, "docid": 4, "segid": 46, "prediction": "Zum Beispiel für das Query. Wer ist der Kapitän des Richmond Football Clubs? Das Frauenteam des Clubs, kapitän von Jess Kennedy, Und das Generation-Modell der Frage: Der Question, der Kapitän des Richmond Football Clubs,", "reference": "Zum Beispiel für die Anfrage: „Wer ist der Kapitän des Richmond Football Clubs?“ REALM ruft Passagen über die Damenmannschaft des Clubs ab, die von Jess Kennedy angeführt wird. Das fragengenerierende Modell generiert die Anfrage: „Wer war Kapitänin der ersten Damenmannschaft des Richmond Football Clubs?“"}
{"index": 410, "docid": 4, "segid": 47, "prediction": "der erste weibliche Team des Jahres. die eine spezifische somatische Perturbation hat.", "reference": "Hier gibt es eine spezifische semantische Störung."}
{"index": 411, "docid": 4, "segid": 48, "prediction": "In ähnlicher Weise bekommen wir auch Qu eries wie: Wer ist Kapitän Richmond s VFL-Reserv eteam", "reference": "In einer ähnlichen Art und Weise erhalten wir auch Abfragen, wie etwa: „Wer war Kapitän der Richmond's VFL-Reservemannschaft?“"}
{"index": 412, "docid": 4, "segid": 49, "prediction": "oder wer hat Graham negiert im Grand Final letztes Jahr?", "reference": "Oder: „Wen hat Graham letztes Jahr im großen Finale geschlagen?“"}
{"index": 413, "docid": 4, "segid": 50, "prediction": "Schließlich filtern wir eine Teilmenge der generierten Anfragen auf der gewünschter Merkmale aus.", "reference": "Schließlich filtern wir eine Teilmenge der generierten Abfragen basierend auf gewünschten Eigenschaften aus."}
{"index": 414, "docid": 4, "segid": 51, "prediction": "Wie bereits erwähnt, möchten wir sicherstellen, dass die neue Frage dem Original semantisch noch nahe steht.", "reference": "Wie vorhin begründet, möchten wir sicherstellen, dass die neue Frage immer noch semantisch nah am Original ist."}
{"index": 415, "docid": 4, "segid": 52, "prediction": "Für eine Filtertechnik, die keine zusätzliche Aufsicht erfordert, behalten wir einfach neue Fragen dass sie eine kleine, auf Token-Ebene angelegte Distanz von der ursprünglichen Frage haben.", "reference": "Bei den Filtertechniken, die keine zusätzliche Überwachung erfordern, speichern wir einfach neue Fragen, die einen kleinen Token-Label und einen Bearbeitungsabstand von der ursprünglichen Frage haben."}
{"index": 416, "docid": 4, "segid": 53, "prediction": "Zum Wir entfernen die Frage , wer Graham in der Grand Final letztes Jahr negierte, weil", "reference": "Wir entfernen zum Beispiel die Frage: „Wen hat Graham letztes Jahr im großen Finale geschlagen?“"}
{"index": 417, "docid": 4, "segid": 54, "prediction": "es eine lange Längere Distanz von der ursprünglichen Frage.", "reference": "Denn diese hat einen längeren Bearbeitungsabstand zur ursprünglichen Frage."}
{"index": 418, "docid": 4, "segid": 55, "prediction": "In unseren Experimenten demonstrieren wir, dass diese einfache Heuristik kann zu Augment and Q-Training-Daten verwendet werden.", "reference": "In unseren Experimenten zeigen wir, dass diese einfache Heuristik verwendet werden kann, um Trainingsdaten zu erweitern und in die Warteschlange zu stellen."}
{"index": 419, "docid": 4, "segid": 56, "prediction": "Wir experimentieren auch mit einer Filterstrategie, die auf der Art der semantischen Perturbation basiert.", "reference": "Wir experimentieren auch mit einer Filterstrategie, die auf der Art der semantischen Störung basiert."}
{"index": 420, "docid": 4, "segid": 57, "prediction": "Zu diesem Zweck verwenden wir ein allgemeines Queriedekompositionen-Framework, das QED genannt wird.", "reference": "Zu diesem Zweck verwenden wir einen allgemeinen Zerlegungsrahmen für die Anfrage mit dem Namen QED."}
{"index": 421, "docid": 4, "segid": 58, "prediction": "QED identifiziert zwei Teile der Frage , ein Predikat und eine Referenz.", "reference": "QED identifiziert zwei Teile der Frage: ein Prädikat und eine Referenz."}
{"index": 422, "docid": 4, "segid": 59, "prediction": "Referenzen sind nun Phrasen in der Frage, die Entitäten im Kontext entsprechen.", "reference": "Referenzen sind Substantivgruppen in der Frage, die Entitäten im Kontext entsprechen."}
{"index": 423, "docid": 4, "segid": 60, "prediction": "Ein Predikat ist im Grunde der verbleibende Teil der Frage.", "reference": "Ein Prädikat ist im Grunde der verbleibende Teil der Frage."}
{"index": 424, "docid": 4, "segid": 61, "prediction": "Zum Beispiel sind wir in der Lage, die Query zu dekomposieren. Das war die erste Frauenmannschaft von Captain Richmond. In zwei Referenzen : Richmond Football Club, Frauenmannschaft Und das Prädikat: \"Wer hat Kapitän X?\"", "reference": "Zum Beispiel sind wir in der Lage, die Abfrage zu zerlegen: „Wer war Kapitänin der ersten Damenmannschaft des Richmond Football Clubs?“ Wir können die Frage in zwei Referenzen zerlegen: das Damenteam vom Richmond Football Club und das Prädikat X (wer war Kapitänin?)."}
{"index": 425, "docid": 4, "segid": 62, "prediction": "Ein Modell, das auf Referenzpredikaten-Anmerkungen für Und Q gibt uns diese Frage der Dekomposition.", "reference": "Ein Modell, das auf Referenzen der Prädikatannotationen für NQ trainiert wurde, erlaubt uns diese Zerlegung der Frage."}
{"index": 426, "docid": 4, "segid": 63, "prediction": "Das dekomposieren der ursprünglichen und der generierten Fragen basierend auf Q&amp;E. Er erlaubt uns, unsere Generatoren zu kategorisieren.", "reference": "Die Zerlegung sowohl des Originals als auch der generierten Frage basierend auf QED ermöglicht es uns, unsere generierten Kontrafakten für die Bewertung zu kategorisieren."}
{"index": 427, "docid": 4, "segid": 64, "prediction": "Wir haben zwei Gruppen von Fragen erhalten.", "reference": "Konkret erhalten wir zwei Gruppen von Fragen."}
{"index": 428, "docid": 4, "segid": 65, "prediction": "Das ist der Referenzwechsel, während wir die Prädikate behalten. Und die, die einen Prädikatwechsel durchlaufen, und optionale Referenzen", "reference": "Es gibt Fragen, bei denen sich die Referenz ändert, aber die Prädikate gleichbleiben, und Fragen, bei denen sich die Prädikate ändern und optional Referenzen hinzugefügt werden."}
{"index": 429, "docid": 4, "segid": 66, "prediction": "für For instance. Wer Captain Richmonds VFL-Reserve-Team", "reference": "Hier ist ein Beispiel für eine Änderung der Referenz: „Wer war Kapitän der Richmond's VFL-Reservemannschaft?“"}
{"index": 430, "docid": 4, "segid": 67, "prediction": "ist eine Referenzänderung. Während wer trägt Nummer neun für Der Club ist ein Prädikatwechsel.", "reference": "Das ist eine Veränderung des Prädikats: „Wer trägt die Nummer neun beim Club?“"}
{"index": 431, "docid": 4, "segid": 68, "prediction": "Wir bewerten jetzt die Wirksamkeit von RGF-Perturbationen , wenn sie auf Trainingsdaten ergänzt werden.", "reference": "Wir bewerten nun die Effektivität von RGF-Störungen, wenn diese um die Trainingsdaten ergänzt werden."}
{"index": 432, "docid": 4, "segid": 69, "prediction": "um die Wirksamkeit der gegenwahrscheinlichen Erweiterung besonders effektiv zu bewerten. Wir experimentieren mit zwei starken Daten-Augmentations-Basislinien.", "reference": "Um insbesondere die Effektivität des kontrafaktischen Aufbaus effektiv bewerten zu können, experimentieren wir mit zwei starken Baselines des Datenaufbaus."}
{"index": 433, "docid": 4, "segid": 70, "prediction": "Die erste Baseline , die als Random Answer and Question Generation bezeichnet wird, Das hat keine Beziehung zu der ursprünglichen Frage.", "reference": "Die erste Baseline, die als zufällige Antwort- und Fragengenerierung bezeichnet wird, fügt Daten hinzu, die keine Relation zur ursprünglichen Frage haben."}
{"index": 434, "docid": 4, "segid": 71, "prediction": "Das heißt, Passagen und Antworten sind einfach zufällig aus Wikipedia samplt.", "reference": "Das heißt, dass Passagen und Antworten einfach zufällig aus Wikipedia entnommen werden."}
{"index": 435, "docid": 4, "segid": 72, "prediction": "Diese Baseline fügt im Grunde mehr Daten hinzu, die wie NQ aussehen.", "reference": "Diese Baseline fügt im Grunde mehr Daten hinzu, die wie NQ aussehen."}
{"index": 436, "docid": 4, "segid": 73, "prediction": "mit der zweiten Wäsche, Gold, Antwort und Frage, Generation. Wir werden speziell den Retrieval-Teil unserer Methode ablösen.", "reference": "Mit der zweiten Baseline, der Goldantwort und der Fragengeneration, aktualisieren wir speziell den Retrieval bei unserer Methode."}
{"index": 437, "docid": 4, "segid": 74, "prediction": "Hier werden nur alternative Antworten aus demselben Abschnitt ausgewählt, der die goldene Antwort enthielt.", "reference": "Hier werden alternative Antworten nur aus der gleichen Passage ausgewählt, welche die Goldantwort enthält."}
{"index": 438, "docid": 4, "segid": 75, "prediction": "Wie gehen die Basislinien und RJF? # Augmentation Auf Read-Comprehension, wo das Modell Zugang zu Frage und Kontext hat.", "reference": "Welche Leistung erbringen die Baselines, RGF und der Aufbau beim Leseverständnis, wo das Modell Zugriff auf Frage und Kontext hat?"}
{"index": 439, "docid": 4, "segid": 76, "prediction": "Wir experimentieren mit sechs Out-of-Domain-Datensätzen und präsentieren hier die Ergebnisse. Wo die Training-Daten in Augmentation verdoppelt werden.", "reference": "Wir experimentieren mit sechs von den Datensätzen der Domäne und präsentieren hier die Ergebnisse, wobei es bei den Daten um die Trainingsdaten geht und beim Aufbau verdoppelt werden."}
{"index": 440, "docid": 4, "segid": 77, "prediction": "Wir finden das beide. Daten-Augmentation-Basislinien sind nicht in der Lage, die Domain-Generalisierung zu verbessern.", "reference": "Wir stellten fest, dass beide Baselines des Datenaufbaus nicht in der Lage sind, unsere Verallgemeinerung der Domäne zu verbessern."}
{"index": 441, "docid": 4, "segid": 78, "prediction": "Ein Ensemble von sechs Modellen, trainiert auf den Originaldaten. Es scheint die konkurrenzfähigste Basis zu sein.", "reference": "Tatsächlich scheint ein Ensemble von sechs Modellen, die mit den ursprünglichen Daten trainiert wurden, die wettbewerbsfähigste Baseline zu sein."}
{"index": 442, "docid": 4, "segid": 79, "prediction": "Verglichen mit der Basislinie finden wir, dass RGF-Counterfactuals die in der Lage sind, die Performance außerhalb der Domain zu verbessern, während sie die Performance in der Domain beibehalten.", "reference": "Im Vergleich zu dieser Baseline stellten wir fest, dass RGF-Kontrafakten in der Lage sind, die Leistung außerhalb der Domäne zu verbessern, während die Leistung innerhalb der Domäne beibehalten wird."}
{"index": 443, "docid": 4, "segid": 80, "prediction": "Das suggeriert, dass die Reasoning Gaps des Modells Die Quanti fakt-Augmentation ist effektiver als das Hinzufügen von Daten aus der Training-Distribution.", "reference": "Dies deutet darauf hin, dass das Füllen der Argumentationslücken beim Modell über einen kontrafaktischen Aufbau effektiver ist als mehr Daten aus der Training-Verteilung hinzuzufügen."}
{"index": 444, "docid": 4, "segid": 81, "prediction": "Darüber hinaus sind wir auch sehr zufrieden mit der neuen Technologie. , dass wir mit Retrieval zu samplenden alternativen Ausgängen oder Antworten ist wichtig für eine effektive CDA.", "reference": "Darüber hinaus fanden wir heraus, dass die Verwendung von Retrievals zur Erprobung alternativer Ergebnisse oder Antworten für effektive CDA wichtig ist."}
{"index": 445, "docid": 4, "segid": 82, "prediction": "Wir experimentieren auch mit Open-Domain-Q A-Einstellungen, bei denen das Modell nur die Frage sieht. Und wir bewerten vier Out-of-Domain-Datensätze.", "reference": "Wir experimentieren auch mit einer offenen Domäne-QA -Einstellung, bei der das Modell nur die Frage sieht. Wir bewerten wieder vier von den Datensätzen der Domäne."}
{"index": 446, "docid": 4, "segid": 83, "prediction": "Wir finden , dass Baseline-Modelle nicht so effektiv sind für die Aus-der-Domain-Vergänzung.", "reference": "Wir stellten fest, dass die Baseline-Modelle nicht so effektiv für die Verallgemeinerung der Domäne sind."}
{"index": 447, "docid": 4, "segid": 84, "prediction": "Die Datenvergrößerung mit R GF zeigt signifikante Verbesserungen.", "reference": "Allerdings zeigt der Datenaufbau mit RGF signifikantere Verbesserungen."}
{"index": 448, "docid": 4, "segid": 85, "prediction": "Wir haben sogar in der Indomäne-NQ-Datensatz verbessert.", "reference": "Wir verbessern uns sogar in der Domäne NQ-Datensatz."}
{"index": 449, "docid": 4, "segid": 86, "prediction": "Wir haben die Hypothese aufgestellt, dass die kontrafaktischen Daten Augmentation hilft dem Modell. In Learning Better Query Encoding für sehr ähnliche Queries.", "reference": "Wir haben angenommen, dass der kontrafaktische Datenaufbau das Modell beim Lernen von besseren Abfragekodierungen für sehr ähnliche Abfragen unterstützt."}
{"index": 450, "docid": 4, "segid": 87, "prediction": "Schließlich bewerten wir auch die Fähigkeit des Modells, die Konsistenz in der lokalen Nachbarschaft der ursprünglichen Frage zu verbessern.", "reference": "Schließlich bewerten wir auch die Fähigkeit des Modells, die Einheitlichkeit in der lokalen Nachbarschaft der ursprünglichen Frage zu verbessern."}
{"index": 451, "docid": 4, "segid": 88, "prediction": "Die Konsistenz misst den Anteil der von dem Modell korrekt beantworteten Fragen, bei denen sowohl die ursprüngliche als auch die kontrafaktuelle Frage richtig beantwortet sind.", "reference": "Die Einheitlichkeit misst den Anteil der vom Modell korrekt beantworteten Fragen, bei denen sowohl das Original als auch die kontrafaktische Abfrage korrekt beantwortet werden."}
{"index": 452, "docid": 4, "segid": 89, "prediction": "Das hilft uns, die Robustheit des Modells zu messen. zu kleinen Perturbationen in der Nachbarschaft der Original-Input", "reference": "Dies hilft uns explizit, die Robustheit des Modells bei kleinen Störungen in der Nähe der ursprünglichen Eingabe zu messen."}
{"index": 453, "docid": 4, "segid": 90, "prediction": "Wir experimentieren mit fünf Datensätzen , die Paare von Fragen enthalten, die sich semantisch nahe stehen.", "reference": "Wir experimentieren mit fünf Datensätzen, die Paare von Fragen enthalten, die semantisch nahe beieinander liegen."}
{"index": 454, "docid": 4, "segid": 91, "prediction": "Abgesehen von den drei Datensätzen EQA, Ambiguous und CoreF kontrastieren sie, die bereits verfügbar sind. bewerten auch auf RGF -Counterfactuals, die mit Original- und Q-Fragen gepaart sind, basierend auf dem, ob sie eine Prädikatsänderung oder einen Referenzwechsel durchlaufen haben.", "reference": "Abgesehen von den drei Datensätzen AQA, AmbigQA und den QUOREF-Kontrastsätzen, die bereits verfügbar sind, bewerten wir auch RGF-Kontrafakten. Diese sind mit ursprünglichen NQ-Fragen gepaart, basierend darauf, ob sie von einer Prädikat- oder Referenzänderung betroffen waren."}
{"index": 455, "docid": 4, "segid": 92, "prediction": "Diese Untersätze wurden im Haus annottiert, um Lärm zu eliminieren. und werden als Ressource zur Verfügung gestellt.", "reference": "Diese Teilmengen wurden intern annotiert, um Qualitätsmängel zu eliminieren. Sie werden als Ressource bereitgestellt."}
{"index": 456, "docid": 4, "segid": 93, "prediction": "Alle Bas islinien sind nicht in der Lage, die Konsistenz mit dem Ensemble-Modell&nbsp;&nbsp;", "reference": "Alle Baselines können die Einheitlichkeit signifikant nicht verbessern. Das Ensemble der Modelle kann die Einheitlichkeit geringfügig verbessern."}
{"index": 457, "docid": 4, "segid": 94, "prediction": "Consistency via Small Margin Allerdings hat RDF-Konterfaktualvergrößerung beeindruckende Gewinne und Konsistenz. Sowohl auf vorherigen Datensätzen als auch auf den beiden Subsätzen werden wir kuratiert. für Referenz - und Predikate-Motivationen.", "reference": "Der kontrafaktische Aufbau der RGF kann jedoch eine beeindruckende Steigerung der Einheitlichkeit sowohl bei früheren Datensätzen als auch bei den beiden Teilmengen, die wir für Referenz- und Prädikat-Störungen ausgewählt haben, aufweisen."}
{"index": 458, "docid": 4, "segid": 95, "prediction": "Beachten Sie, dass die erweiterte Argie der Daten nicht durch Motivationstypen verzerrt ist, nur die Evaluierungssätze sind.", "reference": "Beachten Sie, dass die erweiterten RGF-Daten nicht durch den Störungstyp verfälscht werden, sondern nur durch die Evaluationssätze."}
{"index": 459, "docid": 4, "segid": 96, "prediction": "In Tat wurde eine qualitative Inspektion der Arten von Counterfeits erstellt. dass die generierten Fragen mehrere verschiedene Störungen enthalten.", "reference": "Tatsächlich zeigt eine qualitative Überprüfung der verschiedenen Arten von Kontrafaktoren, dass die generierten Fragen mehrere unterschiedliche Störungen enthalten."}
{"index": 460, "docid": 4, "segid": 97, "prediction": "Zum Beispiel diese ursprüngliche Frage über die Bevölkerung von Walnut Grove, Minnesota, ist auf unterschiedliche Dimensionen wie Stadt, Staat, Land Land verwirrt. und verschiedene Prädikate wie Standort, Poverty, Anzahl der Schulen.", "reference": "Zum Beispiel ist diese ursprüngliche Frage über die Bevölkerung von Walnut Grove in Minnesota gestört. Diese Störung betrifft verschiedene Dimensionen wie Stadt, Bundesland, Land und verschiedene Prädikate wie Lage, Armut, Anzahl von Schulen."}
{"index": 461, "docid": 4, "segid": 98, "prediction": "R GF-Perturbationen sind kontextspezifisch.", "reference": "Das Audio von Störungen ist kontextspezifisch."}
{"index": 462, "docid": 4, "segid": 99, "prediction": "Zum Beispiel für diese andere Frage über den Das Einzel-Turnier ist eine lange, Typ of Game, Typ of Tournament oder das Game-Outcome.", "reference": "Bei dieser anderen Frage über das Einzelturnier in Wimbledon handelt die Störung von der Art des Spiels, der Art des Turniers oder des Spielergebnisses."}
{"index": 463, "docid": 4, "segid": 100, "prediction": "Wir lösen die Aufgabe, die Counterfactual-Daten zu and Perturbations for Informationen suchen, Queries suchen und Wir haben uns den einzigartigen Herausforderungen der Reversal the Generation-Ansatz befasst. Übergen erieren mit Nearsight-Misses des Modells und filtern nach Störungsart oder Minimalität.", "reference": "Abschließende Erkenntnisse: Wir befassen uns mit der Aufgabe des kontrafaktischen Datenaufbaus und Störungen bei der Information. Wir suchen nach Abfragen und bewältigen deren einzigartige Herausforderungen durch eine Umkehrung des Generierungsansatzes. Wir übergenerieren mithilfe von Near-Misses des Modells und filtern basierend auf dem Störungstyp oder der Minimalität."}
{"index": 464, "docid": 4, "segid": 101, "prediction": "Wir finden , dass diese Technik keine zusätzliche Aufsicht erfordert und die Beispiele sind für Augmentation gekennzeichnet.", "reference": "Wir stellten fest, dass diese Technik keiner zusätzlichen Überwachung bedarf und die Beispiele für den Aufbau markiert sind."}
{"index": 465, "docid": 4, "segid": 102, "prediction": "Augmentation verbessert die Ausdehnung der Domäne und die Nachbarschaftskonsistenz.", "reference": "Der Aufbau verbessert sich dank der Domäneverallgemeinerung und der Konsistenz der Nachbarschaft."}
{"index": 466, "docid": 4, "segid": 103, "prediction": "Und wir finden , dass RGF-Kontrast-Fakturiere semantisch divers sind, ohne zu introduzieren. Buyers during Augmentation. (Bei", "reference": "Zudem stellten wir fest, dass die RGF-Kontrafakten semantisch divers sind, ohne dass Verzerrungen während des Aufbaus eingeführt wurden."}
{"index": 467, "docid": 4, "segid": 104, "prediction": "Augmentation) Danke fürs Gespräch.", "reference": "Vielen Dank!"}
